{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Overview\n",
    "\n",
    "## Goals\n",
    "In this unit, you will learn about the following:\n",
    "- What RAG is and why it is important. \n",
    "- How RAG is commonly designed.\n",
    "- What stages are involved in implementing RAG for our Ray QA engine.\n",
    "\n",
    "## What is RAG ?\n",
    "\n",
    "Retrieval augmented generation (RAG) is a system design that combines the strengths of LLMs and information retrieval systems. It was first introduced by Lewis et al. in the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401). It has since been implemented in popular frameworks such as [LlamaIndex](https://www.llamaindex.ai/) and [LangChain](https://langchain.com/). For a more general overview of RAG, we recommend you to take a look at our [Introduction to Retrieval Augmented Generation](https://learn.ray.io/llm-applications-and-ray-serve/intro-to-llm-applications-and-ray-serve/introduction-to-retrieval-augmented-generation.html) module.\n",
    "\n",
    "## Why RAG ?\n",
    "RAG systems are designed to address shortcomings of LLMs. You can think of RAG as providing LLMs with a \"contextual memory\" - Almost like how a human would use a search engine to look up information to provide context to a question.\n",
    "\n",
    "More specifically, RAG systems will enable us to:\n",
    "\n",
    "- Reduce LLM hallucinations by providing context relevant to a user prompt.\n",
    "- Provide clear attribution as to the source of the information used as context.\n",
    "- Control the subset of information that a user has access to when using an LLM.\n",
    "- Address inherent knowledge boundaries of LLMs by providing up-to-date information that is open to revision. \n",
    "\n",
    "## What does a RAG system design look like ?\n",
    "\n",
    "Without RAG, we start out with the following:\n",
    "- The user query.\n",
    "- A prompt that is tuned for the given model and domain.\n",
    "- A model to generate a response.\n",
    "- The generated output. \n",
    "\n",
    "See the diagram below for a visual representation of the system design without RAG.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-bootcamp-mar-2024/without_rag.svg\" alt=\"Without RAG\" width=\"50%\"/>\n",
    "\n",
    "With RAG, we now have:\n",
    "\n",
    "- The user query.\n",
    "- A query encoder of the user query.\n",
    "- A document encoder that encodes documents.\n",
    "- A retriever that takes the encoded query and fetches relevant documents from a store.\n",
    "- Augmented prompt with the retrieved context.\n",
    "- A model to generate a response.\n",
    "- The generated output.\n",
    "\n",
    "See the diagram below for a visual representation of the system design with RAG.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-bootcamp-mar-2024/with_rag.svg\" alt=\"With RAG\" width=\"80%\"/>\n",
    "\n",
    "\n",
    "Therefore to build a basic RAG system, we require introducing the following steps:\n",
    "\n",
    "1. Encoding our documents, commonly referred to as generating embeddings of our documents.\n",
    "2. Storing the generated embeddings in a vector store.\n",
    "3. Encoding our user query.\n",
    "4. Retrieving relevant documents from our vector store given the encoded user query.\n",
    "5. Augmenting the user prompt with the retrieved context.\n",
    "\n",
    "Below is the same diagram as above, but with the RAG components highlighted.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-bootcamp-mar-2024/with_rag_highlighted.svg\" alt=\"With RAG Highlights\" width=\"80%\"/>\n",
    "\n",
    "\n",
    "## What are the key stages in implementing RAG for our QA engine ?\n",
    "\n",
    "One way to break down RAG is to divide the implementation into three key stages:\n",
    "\n",
    "- Stage 1: Indexing\n",
    "    1. Loading the documents from a source like a website, API, or database.\n",
    "    2. Processing the documents into \"embeddable\" document chunks.\n",
    "    3. Encoding the documents chunks into embedding vectors.\n",
    "    4. Storing the document embedding vectors in a vector store.\n",
    "- Stage 2: Retrieval\n",
    "    1. Encoding the user query.\n",
    "    2. Retrieving the most similar documents from the vector store given the encoded user query.\n",
    "- Stage 3: Generation\n",
    "    1. Augmenting the prompt with the provided context.\n",
    "    2. Generating a response from the augmented prompt.\n",
    " \n",
    "Stages 1 is a setup stage that needs to be performed only when new data is available. Stages 2 and 3 encompass the system in its operational state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canopy Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Canopy is an open-source RAG framework and context engine built on top of the Pinecone vector database. Canopy enables you to quickly and easily experiment with and build applications using RAG. Start chatting with your documents or text data with a few simple commands.\n",
    "\n",
    "Canopy takes on the heavy lifting for building RAG applications: from chunking and embedding your text data to chat history management, query optimization, context retrieval (including prompt engineering), and augmented generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we'll set up some enviornment variables:\n",
    "1. The Pinecone API key\n",
    "2. The Anyscale base URL - this will be the endpoint hosting model we'll use to generate embeddings and completions\n",
    "3. The Anyscale API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.environ.get('PINECONE_API_KEY') or '9386359a-0227-4d5b-80d9-b1bb7600dd08'\n",
    "os.environ[\"ANYSCALE_BASE_URL\"] = 'https://api.endpoints.anyscale.com/v1'\n",
    "os.environ[\"ANYSCALE_API_KEY\"] = os.environ.get('ANYSCALE_API_KEY') or 'esecret_f6dz2g16nnrai635si83z8upk8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use Pandas to read a parquet file into which we previously downloaded and saved the Pinecone documentation website. We'll retrieve the parquet file and sample it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import psutil\n",
    "import ray\n",
    "import torch\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec, PodSpec\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"/mnt/cluster_storage/\")\n",
    "shutil.copytree(Path(\"../data/\"), DATA_DIR, dirs_exist_ok=True)\n",
    "\n",
    "# Initialize an empty list to store the JSON objects\n",
    "data = []\n",
    "\n",
    "dest_dir = DATA_DIR / \"simplest_pipeline\"\n",
    "\n",
    "# Initialize an empty list to store the documents\n",
    "dataset = []\n",
    "\n",
    "# Specify the path to your JSONL file\n",
    "file_path = dest_dir / \"air.jsonl\"\n",
    "\n",
    "# Open the JSONL file and parse it line by line\n",
    "try:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Convert each line to a JSON object\n",
    "            json_obj = json.loads(line)\n",
    "            print(line)\n",
    "            \n",
    "            # Extract the desired field (e.g., 'text') and append it to the dataset list\n",
    "            # Replace 'text' with the actual field name you're interested in\n",
    "            if 'text' in json_obj:  # Make sure the field exists\n",
    "                dataset.append(json_obj['text'])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Display the first few items in the dataset\n",
    "dataset[:2]\n",
    "\n",
    "def chunk_fn(doc):\n",
    "    return doc.split(\" \")\n",
    "\n",
    "chunks = []\n",
    "for doc in dataset:\n",
    "    chunks.extend(chunk_fn(doc))\n",
    "chunks\n",
    "\n",
    "word_to_vec = {\n",
    "    \"this\": [0.1, 0.2],\n",
    "    \"is\": [0.3, 0.4],\n",
    "    \"a\": [0.5, 0.6],\n",
    "    \"document\": [0.7, 0.8],\n",
    "    \"another\": [0.9, 1.0],\n",
    "}\n",
    "word_to_vec[\"<UNK>\"] = [0.0, 0.0]\n",
    "\n",
    "\n",
    "def embed_model(word):\n",
    "    return word_to_vec.get(word, word_to_vec[\"<UNK>\"])\n",
    "\n",
    "embeddings = [embed_model(chunk) for chunk in chunks]\n",
    "embeddings\n",
    "\n",
    "\n",
    "\n",
    "#embeddings[:2]\n",
    "\n",
    "\n",
    "#data1 = pd.read_json(\"C:\\\\Users\\\\shank\\\\Downloads\\\\canopy\\\\examples\\\\ai_arxiv.jsonl\", lines=True)\n",
    "\n",
    "#data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Tokenizer\n",
    "Many of Canopy's components are using _tokenization_, which is a process that splits text into tokens - basic units of text (like word or sub-words) that are used for processing. Therefore, Canopy uses a singleton Tokenizer object which needs to be initialized once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.tokenizer import Tokenizer\n",
    "Tokenizer.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this tokenizer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' world', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.tokenize(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a KnowledgBase to store our data for search\n",
    "The `KnowledgeBase` object is responsible for storing and indexing textual documents.\n",
    "\n",
    "Once documents are indexed, the `KnowledgeBase` can be queried with a new unseen text passage, for which the most relevant document chunks are retrieved.\n",
    "\n",
    "The `KnowledgeBase` holds a connection to a Pinecone index and provides a simple API to insert, delete and search textual documents.\n",
    "\n",
    "The `KnowledgeBase`'s `upsert()` operation is used to index new documents, or update already stored documents. The upsert process splits each document's text into smaller chunks, transforms these chunks to vector embeddings, then upserts those vectors to the underlying Pinecone index. At Query time, the KnowledgeBase transforms the textual query text to a vector in a similar manner, then queries the underlying Pinecone index to retrieve the `top-k` most closely matched document chunks.\n",
    "\n",
    "To Make the `KnowledgeBase` work with the Anyscale endpoint, we'll have to first define an `AnyscaleRecordEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base.record_encoder import AnyscaleRecordEncoder\n",
    "\n",
    "anyscale_record_encoder = AnyscaleRecordEncoder(\n",
    "    api_key=os.environ[\"ANYSCALE_API_KEY\"],\n",
    "    base_url=os.environ[\"ANYSCALE_BASE_URL\"],\n",
    "    batch_size=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a `KnowledgeBase` with our desired index name (make sure you are using some unique string like your name):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base import KnowledgeBase\n",
    "\n",
    "INDEX_NAME = \"shanker-index\" # Set the index name here\n",
    "\n",
    "kb = KnowledgeBase(index_name=INDEX_NAME, record_encoder=anyscale_record_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first one-time setup of a new Canopy service, an underlying Pinecone index needs to be created. If you have created a Canopy-enabled Pinecone index before - you can skip this step.\n",
    "\n",
    "Note: Since Canopy uses a dedicated data schema, it is not recommended to use a pre-existing Pinecone index that wasn't created by Canopy's `create_canopy_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base import list_canopy_indexes\n",
    "if not any(name.endswith(INDEX_NAME) for name in list_canopy_indexes()):\n",
    "    kb.create_canopy_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the index created in Pinecone's [console](https://app.pinecone.io/).\n",
    "\n",
    "Next, we'll connect to the create `KnowledgeBase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert data to our KnowledgBase\n",
    "First, we need to convert our dataset to list of `Document` objects\n",
    "\n",
    "Each document object can hold `id`, `text`, `source` and `metadata`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.models.data_models import Document\n",
    "\n",
    "example_docs = [Document(id=\"1\",\n",
    "                      text=\"This is text for example\",\n",
    "                      source=\"https://url.com\"),\n",
    "                Document(id=\"2\",\n",
    "                        text=\"this is another text\",\n",
    "                        source=\"https://another-url.com\",\n",
    "                        metadata={\"my-key\": \"my-value\"})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in our example dataset is already provided in this schema, so we can simply iterate over it and instantiate Document objects:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m documents \u001b[38;5;241m=\u001b[39m [Document(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrow) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[0;32m----> 3\u001b[0m documents1 \u001b[38;5;241m=\u001b[39m [Document(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrow) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata1\u001b[49m\u001b[38;5;241m.\u001b[39miterrows()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data1' is not defined"
     ]
    }
   ],
   "source": [
    "documents = [Document(**row) for _, row in data.iterrows()]\n",
    "\n",
    "documents1 = [Document(**row) for _, row in data1.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to upsert our data, with only a single command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6894eb1be74648aabb2920ef59973b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "kb.upsert(documents, batch_size=batch_size, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the `KnowledgeBase` handles all the processing needed to Index the documents. Each document's text is chunked to smaller pieces and encoded to vector embeddings that can be then upserted directly to Pinecone. Later in this notebook we'll learn how to tune and customize this process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the KnowledgeBase\n",
    "Now we can query the knowledge base. The KnowledgeBase will use its default parameters like top_k to execute the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_query_results(results):\n",
    "    for query_results in results:\n",
    "        print('query: ' + query_results.query + '\\n')\n",
    "        for document in query_results.documents:\n",
    "            print('document: ' + document.text.replace(\"\\n\", \"\\\\n\"))\n",
    "            print(\"title: \" + document.metadata[\"title\"])\n",
    "            print('source: ' + document.source)\n",
    "            print(f\"score: {document.score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to query the term `\"p1 pod capacity\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: p1 pod capacity\n",
      "\n",
      "document: ### s1 pods\\n\\n\\nThese storage-optimized pods provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements.\\n\\n\\nEach s1 pod has enough capacity for around 5M vectors of 768 dimensions.\\n\\n\\n### p1 pods\\n\\n\\nThese performance-optimized pods provide very low query latencies, but hold fewer vectors per pod than s1 pods. They are ideal for applications with low latency requirements (<100ms).\\n\\n\\nEach p1 pod has enough capacity for around 1M vectors of 768 dimensions.\n",
      "title: indexes\n",
      "source: https://docs.pinecone.io/docs/indexes\n",
      "score: 0.91904277\n",
      "\n",
      "document: ## Pod storage capacity\\n\\n\\nEach **p1** pod has enough capacity for 1M vectors with 768 dimensions.\\n\\n\\nEach **s1** pod has enough capacity for 5M vectors with 768 dimensions.\\n\\n\\n## Metadata\\n\\n\\nMax metadata size per vector is 40 KB.\\n\\n\\nNull metadata values are not supported. Instead of setting a key to hold a null value, we recommend you remove that key from the metadata payload.\\n\\n\\nMetadata with high cardinality, such as a unique value for every vector in a large index, uses more memory than expected and can cause the pods to become full.\n",
      "title: limits\n",
      "source: https://docs.pinecone.io/docs/limits\n",
      "score: 0.9050136\n",
      "\n",
      "document: ## Number of vectors\\n\\n\\nThe most important consideration in sizing is the [number of vectors](/docs/insert-data/) you plan on working with. As a rule of thumb, a single p1 pod can store approximately 1M vectors, while a s1 pod can store 5M vectors. However, this can be affected by other factors, such as dimensionality and metadata, which are explained below.\n",
      "title: choosing-index-type-and-size\n",
      "source: https://docs.pinecone.io/docs/choosing-index-type-and-size\n",
      "score: 0.9037753\n",
      "\n",
      "document: | Pod type | Dimensions | Estimated max vectors per pod |\\n| --- | --- | --- |\\n| p1 | 512 | 1,250,000 |\\n|  | 768 | 1,000,000 |\\n|  | 1024 | 675,000 |\\n| p2 | 512 | 1,250,000 |\\n|  | 768 | 1,100,000 |\\n|  | 1024 | 1,000,000 |\\n| s1 | 512 | 8,000,000 |\\n|  | 768 | 5,000,000 |\\n|  | 1024 | 4,000,000 |\\n\\n\\nPinecone does not support fractional pod deployments, so always round up to the next nearest whole number when choosing your pods. \\n\\n\\n# Queries per second (QPS)\n",
      "title: choosing-index-type-and-size\n",
      "source: https://docs.pinecone.io/docs/choosing-index-type-and-size\n",
      "score: 0.8905951\n",
      "\n",
      "document: ---\\n\\n* [Table of Contents](#)\\n* + [Overview](#overview)\\n\t+ [Pods, pod types, and pod sizes](#pods-pod-types-and-pod-sizes)\\n\t\t- [Starter plan](#starter-plan)\\n\t\t- [s1 pods](#s1-pods)\\n\t\t- [p1 pods](#p1-pods)\\n\t\t- [p2 pods](#p2-pods)\\n\t\t- [Pod size and performance](#pod-size-and-performance)\\n\t\t- [Distance metrics](#distance-metrics)\n",
      "title: indexes\n",
      "source: https://docs.pinecone.io/docs/indexes\n",
      "score: 0.8899492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from canopy.models.data_models import Query\n",
    "results = kb.query([Query(text=\"p1 pod capacity\")])\n",
    "\n",
    "print_query_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's limit the source by using a `metadata_filter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: p1 pod capacity\n",
      "\n",
      "document: ## Pod storage capacity\\n\\n\\nEach **p1** pod has enough capacity for 1M vectors with 768 dimensions.\\n\\n\\nEach **s1** pod has enough capacity for 5M vectors with 768 dimensions.\\n\\n\\n## Metadata\\n\\n\\nMax metadata size per vector is 40 KB.\\n\\n\\nNull metadata values are not supported. Instead of setting a key to hold a null value, we recommend you remove that key from the metadata payload.\\n\\n\\nMetadata with high cardinality, such as a unique value for every vector in a large index, uses more memory than expected and can cause the pods to become full.\n",
      "title: limits\n",
      "source: https://docs.pinecone.io/docs/limits\n",
      "score: 0.9050136\n",
      "\n",
      "document: # Limits\\n\\n[Suggest Edits](/edit/limits)This is a summary of current Pinecone limitations. For many of these, there is a workaround or we're working on increasing the limits.\\n\\n\\n## Upserts\\n\\n\\nMax vector dimensionality is 20,000.\\n\\n\\nMax size for an upsert request is 2MB. Recommended upsert limit is 100 vectors per request.\\n\\n\\nVectors may not be visible to queries immediately after upserting. You can check if the vectors were indexed by looking at the total with `describe_index_stats()`, although this method may not work if the index has multiple replicas. Pinecone is eventually consistent.\\n\\n\\nPinecone supports sparse vector values of sizes up to 1000 non-zero values.\\n\\n\\n## Queries\\n\\n\\nMax value for `top_k`, the number of results to return, is 10,000. Max value for `top_k` for queries with `include_metadata=True` or `include_data=True` is 1,000.\\n\\n\\n## Fetch and Delete\\n\\n\\nMax vectors per fetch or delete request is 1,000.\\n\\n\\n## Namespaces\\n\\n\\nThere is no limit to the number of <namespaces> per index.\n",
      "title: limits\n",
      "source: https://docs.pinecone.io/docs/limits\n",
      "score: 0.7975223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from canopy.models.data_models import Query\n",
    "results = kb.query([Query(text=\"p1 pod capacity\",\n",
    "                          metadata_filter={\"source\": \"https://docs.pinecone.io/docs/limits\"},\n",
    "                          top_k=2)])\n",
    "\n",
    "print_query_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Query the Context Engine\n",
    "`ContextEngine` is an object responsible for retrieving the most relevant context for a given query and token budget.\n",
    "\n",
    "While `KnowledgeBase` retrieves the full `top-k` structured documents for each query including all the metadata related to them, the context engine in charge of transforming this information to a \"prompt ready\" context that can later feeded to an LLM. To achieve this the context engine holds a `ContextBuilder` object that takes query results from the knowledge base and returns a `Context` object. The `ContextEngine`'s default behavior is to use a `StuffingContextBuilder`, which simply stacks retrieved document chunks in a JSON-like manner, hard limiting by the number of chunks that fit the `max_context_tokens` budget. More complex behaviors can be achieved by providing a custom `ContextBuilder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.context_engine import ContextEngine\n",
    "context_engine = ContextEngine(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"query\": \"capacity of p1 pods\",\n",
      "    \"snippets\": [\n",
      "      {\n",
      "        \"source\": \"https://docs.pinecone.io/docs/indexes\",\n",
      "        \"text\": \"### s1 pods\\n\\n\\nThese storage-optimized pods provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements.\\n\\n\\nEach s1 pod has enough capacity for around 5M vectors of 768 dimensions.\\n\\n\\n### p1 pods\\n\\n\\nThese performance-optimized pods provide very low query latencies, but hold fewer vectors per pod than s1 pods. They are ideal for applications with low latency requirements (<100ms).\\n\\n\\nEach p1 pod has enough capacity for around 1M vectors of 768 dimensions.\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"https://docs.pinecone.io/docs/limits\",\n",
      "        \"text\": \"## Pod storage capacity\\n\\n\\nEach **p1** pod has enough capacity for 1M vectors with 768 dimensions.\\n\\n\\nEach **s1** pod has enough capacity for 5M vectors with 768 dimensions.\\n\\n\\n## Metadata\\n\\n\\nMax metadata size per vector is 40 KB.\\n\\n\\nNull metadata values are not supported. Instead of setting a key to hold a null value, we recommend you remove that key from the metadata payload.\\n\\n\\nMetadata with high cardinality, such as a unique value for every vector in a large index, uses more memory than expected and can cause the pods to become full.\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"https://docs.pinecone.io/docs/choosing-index-type-and-size\",\n",
      "        \"text\": \"## Number of vectors\\n\\n\\nThe most important consideration in sizing is the [number of vectors](/docs/insert-data/) you plan on working with. As a rule of thumb, a single p1 pod can store approximately 1M vectors, while a s1 pod can store 5M vectors. However, this can be affected by other factors, such as dimensionality and metadata, which are explained below.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n",
      "# tokens in context returned: 434\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "result = context_engine.query([Query(text=\"capacity of p1 pods\", top_k=5)], max_context_tokens=512)\n",
    "\n",
    "print(result.to_text(indent=2))\n",
    "print(f\"\\n# tokens in context returned: {result.num_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, although we set `top_k=5`, context engine retreived only 3 results in order to satisfy the 512 tokens limit. Also, the documents in the context contain only the text and source and not all the metadata that is not necessarily needed by the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledgeable chat engine\n",
    "Now we are ready to start chatting with our data!\n",
    "\n",
    "Canopy's `ChatEngine` is a one-stop-shop RAG-infused Chatbot. The `ChatEngine` wraps an underlying LLM such as OpenAI's GPT-4, enhancing it by providing relevant context from the user's knowledge base. It also automatically phrases search queries out of the chat history and send them to the knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to allow the `ChatEngine` to work with the `AnyScaleLLM`, we'll have to initilize it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.chat_engine import ChatEngine\n",
    "from canopy.llm.anyscale import AnyscaleLLM\n",
    "from canopy.chat_engine.query_generator import InstructionQueryGenerator\n",
    "\n",
    "anyscale_llm = AnyscaleLLM(\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    api_key=os.environ[\"ANYSCALE_API_KEY\"],\n",
    "    base_url=os.environ[\"ANYSCALE_BASE_URL\"],\n",
    ")\n",
    "\n",
    "chat_engine = ChatEngine(\n",
    "    context_engine,\n",
    "    query_builder=InstructionQueryGenerator(\n",
    "        llm=anyscale_llm,\n",
    "    ),\n",
    "    llm=anyscale_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a `chat` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from canopy.models.data_models import Messages, UserMessage, AssistantMessage\n",
    "\n",
    "def chat(new_message: str, history: Messages) -> Tuple[str, Messages]:\n",
    "    messages = history + [UserMessage(content=new_message)]\n",
    "    response = chat_engine.chat(messages)\n",
    "    assistant_response = response.choices[0].message.content\n",
    "    return assistant_response, messages + [AssistantMessage(content=assistant_response)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the chat out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Each p1 pod has enough capacity for around 1 million vectors of 768 dimensions.\n",
       "\n",
       "Source: https://docs.pinecone.io/docs/limits"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "history = []\n",
    "response, history = chat(\"What is the capacity of p1 pods?\", history)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the chat's ability to look at the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " I apologize for the repeated question. To answer your query, P1 pods fit applications with low latency requirements, specifically less than 100ms.\n",
       "\n",
       "Source: <https://docs.pinecone.io/docs/indexes>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response, history = chat(\"And for what latency requirements does it fit?\", history)\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-dev-bootcamp-39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
