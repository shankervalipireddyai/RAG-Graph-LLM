{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import psutil\n",
    "import ray\n",
    "import torch\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec, PodSpec\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/cluster_storage')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path(\"/mnt/cluster_storage/\")\n",
    "DATA_DIR\n",
    "shutil.copytree(Path(\"../02_embeddings-generation/\"), DATA_DIR, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_json(DATA_DIR / \"data1\")\n",
    "\n",
    "#ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36356261075542da8daa87c6ba9c4c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Read progress 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.has_mps:\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 2\n",
    "num_cpus = psutil.cpu_count()\n",
    "\n",
    "class EmbedBatch:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(\"thenlper/gte-large\", device=device)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        text = batch[\"text\"].tolist()\n",
    "        embeddings = self.model.encode(text, batch_size=len(text))\n",
    "        batch[\"embeddings\"] = embeddings.tolist()\n",
    "        return batch\n",
    "\n",
    "ds = ds.map_batches(\n",
    "    EmbedBatch,\n",
    "    # Maximum number of actors to launch.\n",
    "    concurrency=num_gpus if device == \"cuda\" else num_cpus,\n",
    "    # Size of batches passed to embeddings actor.\n",
    "    batch_size=100,\n",
    "    # 1 GPU for each actor.\n",
    "    num_gpus=1 if device == \"cuda\" else 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 22:14:03,157\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=64 for operator ReadJSON to satisfy parallelism at least twice the available number of CPUs (32).\n",
      "2024-03-09 22:14:03,158\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 64, each read task output is split into 64 smaller blocks.\n",
      "2024-03-09 22:14:03,159\tINFO streaming_executor.py:110 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON] -> ActorPoolMapOperator[MapBatches(EmbedBatch)] -> TaskPoolMapOperator[Write]\n",
      "2024-03-09 22:14:03,159\tINFO streaming_executor.py:111 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-03-09 22:14:03,160\tINFO streaming_executor.py:113 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2024-03-09 22:14:03,181\tINFO actor_pool_map_operator.py:126 -- MapBatches(EmbedBatch): Waiting for 2 pool actors to start...\n",
      ".gitattributes: 100%|██████████| 1.52k/1.52k [00:00<00:00, 16.2MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 191/191 [00:00<00:00, 2.08MB/s]\n",
      "README.md: 100%|██████████| 67.9k/67.9k [00:00<00:00, 968kB/s]\n",
      "config.json: 100%|██████████| 619/619 [00:00<00:00, 6.71MB/s]\n",
      "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]\n",
      "model.safetensors:   3%|▎         | 21.0M/670M [00:00<00:03, 168MB/s]\n",
      "model.safetensors:   8%|▊         | 52.4M/670M [00:00<00:03, 197MB/s]\n",
      "model.safetensors:  13%|█▎        | 83.9M/670M [00:00<00:02, 207MB/s]\n",
      "model.safetensors:  17%|█▋        | 115M/670M [00:00<00:02, 211MB/s] \n",
      "model.safetensors:  22%|██▏       | 147M/670M [00:00<00:02, 214MB/s]\n",
      "model.safetensors:  27%|██▋       | 178M/670M [00:00<00:02, 215MB/s]\n",
      "model.safetensors:  31%|███▏      | 210M/670M [00:00<00:02, 215MB/s]\n",
      "model.safetensors:  36%|███▌      | 241M/670M [00:01<00:01, 215MB/s]\n",
      "model.safetensors:  41%|████      | 273M/670M [00:01<00:01, 216MB/s]\n",
      "model.safetensors:  45%|████▌     | 304M/670M [00:01<00:01, 215MB/s]\n",
      "model.safetensors:  50%|█████     | 336M/670M [00:01<00:01, 216MB/s]\n",
      "model.safetensors:  55%|█████▍    | 367M/670M [00:01<00:01, 215MB/s]\n",
      "model.safetensors:  59%|█████▉    | 398M/670M [00:01<00:01, 216MB/s]\n",
      "model.safetensors:  64%|██████▍   | 430M/670M [00:02<00:01, 219MB/s]\n",
      "model.safetensors:  69%|██████▉   | 461M/670M [00:02<00:00, 218MB/s]\n",
      "model.safetensors:  74%|███████▎  | 493M/670M [00:02<00:00, 219MB/s]\n",
      "model.safetensors:  92%|█████████▏| 619M/670M [00:02<00:00, 217MB/s]\n",
      "model.safetensors:  97%|█████████▋| 650M/670M [00:03<00:00, 217MB/s]\n",
      "model.safetensors: 100%|██████████| 670M/670M [00:03<00:00, 215MB/s]\n",
      "onnx/config.json: 100%|██████████| 632/632 [00:00<00:00, 6.55MB/s]\n",
      "model.onnx:   0%|          | 0.00/1.34G [00:00<?, ?B/s]\n",
      "model.onnx:   2%|▏         | 31.5M/1.34G [00:00<00:05, 246MB/s]\n",
      "model.onnx:   5%|▍         | 62.9M/1.34G [00:00<00:05, 227MB/s]\n",
      "model.onnx:   7%|▋         | 94.4M/1.34G [00:00<00:05, 221MB/s]\n",
      "model.onnx:   9%|▉         | 126M/1.34G [00:00<00:05, 218MB/s] \n",
      "model.onnx:  12%|█▏        | 157M/1.34G [00:00<00:05, 215MB/s]\n",
      "model.onnx:  14%|█▍        | 189M/1.34G [00:00<00:05, 217MB/s]\n",
      ".gitattributes: 100%|██████████| 1.52k/1.52k [00:00<00:00, 14.0MB/s]\n",
      "model.onnx:  16%|█▋        | 220M/1.34G [00:01<00:05, 219MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 191/191 [00:00<00:00, 2.83MB/s]\n",
      "model.onnx:  19%|█▉        | 252M/1.34G [00:01<00:04, 218MB/s]\n",
      "model.onnx:  21%|██        | 283M/1.34G [00:01<00:04, 217MB/s]\n",
      "README.md: 100%|██████████| 67.9k/67.9k [00:00<00:00, 82.4MB/s]\n",
      "model.onnx:  24%|██▎       | 315M/1.34G [00:01<00:04, 220MB/s]\n",
      "config.json: 100%|██████████| 619/619 [00:00<00:00, 8.38MB/s]\n",
      "model.onnx:  26%|██▌       | 346M/1.34G [00:01<00:04, 221MB/s]\n",
      "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]\n",
      "model.onnx:  28%|██▊       | 377M/1.34G [00:01<00:04, 222MB/s]\n",
      "model.safetensors:  88%|████████▊ | 587M/670M [00:02<00:00, 221MB/s]\u001b[32m [repeated 22x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "model.onnx:  31%|███       | 409M/1.34G [00:01<00:04, 222MB/s]\n",
      "model.onnx:  33%|███▎      | 440M/1.34G [00:01<00:04, 222MB/s]\n",
      "model.onnx:  35%|███▌      | 472M/1.34G [00:02<00:03, 221MB/s]\n",
      "model.onnx:  38%|███▊      | 503M/1.34G [00:02<00:03, 218MB/s]\n",
      "model.safetensors: 100%|██████████| 670M/670M [00:03<00:00, 218MB/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "onnx/config.json: 100%|██████████| 632/632 [00:00<00:00, 8.58MB/s]\n",
      "model.onnx:   0%|          | 0.00/1.34G [00:00<?, ?B/s]\n",
      "model.onnx:  85%|████████▍ | 1.13G/1.34G [00:05<00:00, 218MB/s]\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "model.onnx:  92%|█████████▏| 1.23G/1.34G [00:05<00:00, 218MB/s]\n",
      "model.onnx:  94%|█████████▍| 1.26G/1.34G [00:05<00:00, 219MB/s]\n",
      "model.onnx:  96%|█████████▋| 1.29G/1.34G [00:05<00:00, 218MB/s]\n",
      "model.onnx:  99%|█████████▉| 1.32G/1.34G [00:06<00:00, 217MB/s]\n",
      "model.onnx: 100%|██████████| 1.34G/1.34G [00:06<00:00, 219MB/s]\n",
      "onnx/special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 1.31MB/s]\n",
      "onnx/tokenizer.json: 100%|██████████| 712k/712k [00:00<00:00, 10.2MB/s]\n",
      "onnx/tokenizer_config.json: 100%|██████████| 342/342 [00:00<00:00, 2.20MB/s]\n",
      "onnx/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 65.1MB/s]\n",
      "pytorch_model.bin:   0%|          | 0.00/670M [00:00<?, ?B/s]\n",
      "pytorch_model.bin:   5%|▍         | 31.5M/670M [00:00<00:02, 247MB/s]\n",
      "pytorch_model.bin:   9%|▉         | 62.9M/670M [00:00<00:02, 232MB/s]\n",
      "pytorch_model.bin:  14%|█▍        | 94.4M/670M [00:00<00:02, 226MB/s]\n",
      "pytorch_model.bin:  19%|█▉        | 126M/670M [00:00<00:02, 226MB/s] \n",
      "pytorch_model.bin:  23%|██▎       | 157M/670M [00:00<00:02, 225MB/s]\n",
      "pytorch_model.bin:  28%|██▊       | 189M/670M [00:00<00:02, 224MB/s]\n",
      "pytorch_model.bin:  33%|███▎      | 220M/670M [00:00<00:02, 221MB/s]\n",
      "pytorch_model.bin:  38%|███▊      | 252M/670M [00:01<00:01, 220MB/s]\n",
      "pytorch_model.bin:  42%|████▏     | 283M/670M [00:01<00:01, 219MB/s]\n",
      "pytorch_model.bin:  47%|████▋     | 315M/670M [00:01<00:01, 217MB/s]\n",
      "pytorch_model.bin:  52%|█████▏    | 346M/670M [00:01<00:01, 217MB/s]\n",
      "onnx/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]\n",
      "pytorch_model.bin:  56%|█████▋    | 377M/670M [00:01<00:01, 220MB/s]\n",
      "onnx/tokenizer.json: 100%|██████████| 712k/712k [00:00<00:00, 5.23MB/s]\n",
      "pytorch_model.bin:  61%|██████    | 409M/670M [00:01<00:01, 220MB/s]\n",
      "pytorch_model.bin:  66%|██████▌   | 440M/670M [00:01<00:01, 218MB/s]\n",
      "pytorch_model.bin:  70%|███████   | 472M/670M [00:02<00:00, 219MB/s]\n",
      "pytorch_model.bin:  75%|███████▌  | 503M/670M [00:02<00:00, 220MB/s]\n",
      "pytorch_model.bin:  94%|█████████▍| 629M/670M [00:03<00:00, 195MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 670M/670M [00:03<00:00, 207MB/s]\n",
      "model.onnx:  89%|████████▉ | 1.20G/1.34G [00:05<00:00, 220MB/s]\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "sentence_bert_config.json: 100%|██████████| 57.0/57.0 [00:00<00:00, 679kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 1.29MB/s]\n",
      "model.onnx: 100%|██████████| 1.34G/1.34G [00:06<00:00, 220MB/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]\n",
      "tokenizer.json: 100%|██████████| 712k/712k [00:00<00:00, 5.56MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 342/342 [00:00<00:00, 3.41MB/s]\n",
      "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.79MB/s]\n",
      "onnx/special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 1.66MB/s]\n",
      "modules.json: 100%|██████████| 385/385 [00:00<00:00, 4.91MB/s]\n",
      "onnx/tokenizer_config.json: 100%|██████████| 342/342 [00:00<00:00, 3.33MB/s]\n",
      "onnx/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 73.2MB/s]\n",
      "pytorch_model.bin:   0%|          | 0.00/670M [00:00<?, ?B/s]\n",
      "pytorch_model.bin:  89%|████████▉ | 598M/670M [00:02<00:00, 223MB/s]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "tokenizer.json: 100%|██████████| 712k/712k [00:00<00:00, 48.3MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 36.5MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ceda756c2844a99e2c9ce9c2ef72ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=8338, ip=10.0.41.156)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/arrow_block.py:148: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=8338, ip=10.0.41.156)\u001b[0m   return transform_pyarrow.concat(tables)\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=8338, ip=10.0.41.156)\u001b[0m Could not construct Arrow block from numpy array; encountered values of unsupported numpy type `17` in column named 'metadata', which cannot be casted to an Arrow data type. Falling back to using pandas block type, which is slower and consumes more memory. For maximum performance, consider applying the following suggestions before ingesting into Ray Data in order to use native Arrow block types:\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=8338, ip=10.0.41.156)\u001b[0m - Expand out each key-value pair in the dict column into its own column\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=8338, ip=10.0.41.156)\u001b[0m - Replace `None` values with an Arrow supported data type\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=8338, ip=10.0.41.156)\u001b[0m \n",
      "pytorch_model.bin: 100%|██████████| 670M/670M [00:02<00:00, 224MB/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "sentence_bert_config.json: 100%|██████████| 57.0/57.0 [00:00<00:00, 786kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 1.95MB/s]\n",
      "2024-03-09 22:14:27,093\tWARNING actor_pool_map_operator.py:294 -- To ensure full parallelization across an actor pool of size 2, the Dataset should consist of at least 2 distinct blocks. Consider increasing the parallelism when creating the Dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 269 ms, sys: 105 ms, total: 374 ms\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if (DATA_DIR / \"full_scale\" / \"03_embeddings\").exists():\n",
    "    shutil.rmtree(DATA_DIR / \"full_scale\" / \"03_embeddings\")\n",
    "(\n",
    "    ds\n",
    "    .write_json(\n",
    "        num_rows_per_file=50,\n",
    "        path=DATA_DIR / \"full_scale\" / \"03_embeddings\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 456K\n",
      "-rw-r--r-- 1 ray users 453K Mar  9 22:14 3_000000_000000.json\n"
     ]
    }
   ],
   "source": [
    "!ls -llh {DATA_DIR / \"full_scale\" / \"03_embeddings\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_PINECONE_API_KEY = \"9386359a-0227-4d5b-80d9-b1bb7600dd08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\", YOUR_PINECONE_API_KEY)\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1024,\n",
       "              'host': 'canopy--shanker-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--shanker-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--cong-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--cong-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'shanker-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'shanker-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--atlas-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--atlas-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'pink-salmon-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pink-salmon',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--dylans-rag-bootcamp-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--dylans-rag-bootcamp-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'atlas-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'atlas-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'test-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'test-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'brhoades-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'brhoades',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'test-index-jr-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'test-index-jr',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'i-like-turtles-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'i-like-turtles',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--salmon-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--salmon',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--pieter-12345-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--pieter-12345',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'joshua-c-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'joshua-c',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'y-okitsu-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'y-okitsu-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--brendanwhiting-weeeeeee-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--brendanwhiting-weeeeeee',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--rushil-rox-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--rushil-rox',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'jay-destories-rag-02-embeddings-generation-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'jay-destories-rag-02-embeddings-generation',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'amitpphatak-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'amitpphatak',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'pwaivers-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pwaivers',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'anya-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'anya',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--y-okitsu-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--y-okitsu-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--yangyong-anyscale-demo-kb-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--yangyong-anyscale-demo-kb',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'hien-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'hien-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--robert-index-3423423-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--robert-index-3423423',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'dylans-rag-bootcamp-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'dylans-rag-bootcamp-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--samsara-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--samsara',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'ert-test-x-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'ert-test-x',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--amitpphatak-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--amitpphatak',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'mango-jelly-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'mango-jelly',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--joshua-c-demo-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--joshua-c-demo',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--mo-really-good-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--mo-really-good-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'gang-rag-bootcamp-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'gang-rag-bootcamp',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'sachin-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'sachin-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'satancisco-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'satancisco',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'banda-ki-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'banda-ki',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'marwan-ray-docs-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'marwan-ray-docs',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'om-rag-dev-2024-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'om-rag-dev-2024',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'another-good-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'another-good-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--mango-jelly-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--mango-jelly',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'pieter-12345-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pieter-12345',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'althaf-pinecone-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'althaf-pinecone-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--dufwagwitfip-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--dufwagwitfip',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--pwaivers-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--pwaivers',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--test-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--test-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'chris-rag-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'chris-rag',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'roie-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'roie-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--canopy-demo-anyscale-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--canopy-demo-anyscale',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'rushil-rox-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'rushil-rox',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--luo-r-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--luo-r',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--marwan-ray-docs-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--marwan-ray-docs',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'deep-pulusani-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'deep-pulusani',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(\n",
    "    index_name: str,\n",
    "    cloud: str,\n",
    "    region: str,\n",
    "    metric: str,\n",
    "    embedding_dimension: int,\n",
    "    index_type: str,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\", YOUR_PINECONE_API_KEY))\n",
    "    existing_index_names = {index.name for index in pc.list_indexes().indexes}\n",
    "\n",
    "    if index_name in existing_index_names:\n",
    "        pc.delete_index(index_name)\n",
    "\n",
    "    if index_type == \"serverless\":\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=embedding_dimension,\n",
    "            metric=metric,\n",
    "            spec=ServerlessSpec(cloud=cloud, region=region),\n",
    "        )\n",
    "    elif index_type == \"pod\":\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=embedding_dimension,\n",
    "            metric=metric,\n",
    "            spec=PodSpec(\n",
    "                environment=\"gcp-starter\",\n",
    "                metadata_config={\"indexed\": []},\n",
    "                **kwargs,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = \"aws\"\n",
    "region = \"us-west-2\"\n",
    "metric = \"cosine\"\n",
    "index_type = \"serverless\"  # \"serverless\" or \"pod\"\n",
    "index_name = \"shanker-index\" # A unique name for the index under your organization\n",
    "embedding_dimension = 1024  # From the model page of thenlper/gte-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index(\n",
    "    index_name=index_name,\n",
    "    cloud=cloud,\n",
    "    region=region,\n",
    "    metric=metric,\n",
    "    index_type=index_type,\n",
    "    embedding_dimension=embedding_dimension,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954cba061477441291952faa54096803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset(\n",
       "   num_blocks=64,\n",
       "   num_rows=5,\n",
       "   schema={\n",
       "      id: string,\n",
       "      source: string,\n",
       "      text: string,\n",
       "      meta...: struct<primary_category: string, published: string, title: string, updated: string>,\n",
       "      embeddings: list<item: double>\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ray.data.read_json(DATA_DIR / \"full_scale\" / \"03_embeddings/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pinecone_vectors(row):\n",
    "    row_hash = joblib.hash(row)\n",
    "    page_name = row[\"source\"].split(\"/\")[-1]\n",
    "    section_name = row[\"source\"].split(\"#\")[-1]\n",
    "    return {\n",
    "        \"id\": f\"{page_name}#{section_name}#{row_hash}\", # sample ID prefix\n",
    "        \"values\": row[\"embeddings\"],\n",
    "        \"metadata\": {\n",
    "            \n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "ds = ds.map(convert_to_pinecone_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 22:24:35,091\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=64 for operator ReadJSON to satisfy parallelism at least twice the available number of CPUs (32).\n",
      "2024-03-09 22:24:35,092\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 64, each read task output is split into 64 smaller blocks.\n",
      "2024-03-09 22:24:35,092\tINFO streaming_executor.py:110 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON] -> TaskPoolMapOperator[Map(convert_to_pinecone_vectors)] -> LimitOperator[limit=1]\n",
      "2024-03-09 22:24:35,092\tINFO streaming_executor.py:111 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-03-09 22:24:35,093\tINFO streaming_executor.py:113 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e022371a82d6446f865ca153acba45d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = ds.take_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 22:24:41,301\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=64 for operator ReadJSON to satisfy parallelism at least twice the available number of CPUs (32).\n",
      "2024-03-09 22:24:41,302\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 64, each read task output is split into 64 smaller blocks.\n",
      "2024-03-09 22:24:41,302\tINFO streaming_executor.py:110 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON] -> TaskPoolMapOperator[Map(convert_to_pinecone_vectors)->MapBatches(get_size_of_batch)]\n",
      "2024-03-09 22:24:41,303\tINFO streaming_executor.py:111 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-03-09 22:24:41,303\tINFO streaming_executor.py:113 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff446a3ac084633972b7ca02bd4dd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    1.000000\n",
       "mean     0.001591\n",
       "std           NaN\n",
       "min      0.001591\n",
       "25%      0.001591\n",
       "50%      0.001591\n",
       "75%      0.001591\n",
       "max      0.001591\n",
       "Name: size_in_mb, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "def get_size_of_batch(batch):\n",
    "    size_of_batch_in_bytes = pd.DataFrame(batch).memory_usage(deep=True).sum().sum()\n",
    "    size_of_batch_in_mb = size_of_batch_in_bytes / 1024**2\n",
    "    return {\"size_in_mb\": [size_of_batch_in_mb]}\n",
    "\n",
    "\n",
    "out = ds.map_batches(get_size_of_batch, batch_size=batch_size).to_pandas()\n",
    "out[\"size_in_mb\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 22:25:00,016\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=64 for operator ReadJSON to satisfy parallelism at least twice the available number of CPUs (32).\n",
      "2024-03-09 22:25:00,016\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 64, each read task output is split into 64 smaller blocks.\n",
      "2024-03-09 22:25:00,017\tINFO streaming_executor.py:110 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON] -> ActorPoolMapOperator[Map(convert_to_pinecone_vectors)->MapBatches(UpsertVectors)]\n",
      "2024-03-09 22:25:00,018\tINFO streaming_executor.py:111 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-03-09 22:25:00,018\tINFO streaming_executor.py:113 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2024-03-09 22:25:00,070\tINFO actor_pool_map_operator.py:126 -- Map(convert_to_pinecone_vectors)->MapBatches(UpsertVectors): Waiting for 9 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0ad1437733499685ab0ca0524c9fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 22:25:01,882\tWARNING actor_pool_map_operator.py:294 -- To ensure full parallelization across an actor pool of size 9, the Dataset should consist of at least 9 distinct blocks. Consider increasing the parallelism when creating the Dataset.\n",
      "\u001b[36m(MapWorker(Map(convert_to_pinecone_vectors)->MapBatches(UpsertVectors)) pid=14649, ip=10.0.41.156)\u001b[0m Could not construct Arrow block from numpy array; encountered values of unsupported numpy type `17` in column named 'metadata', which cannot be casted to an Arrow data type. Falling back to using pandas block type, which is slower and consumes more memory. For maximum performance, consider applying the following suggestions before ingesting into Ray Data in order to use native Arrow block types:\n",
      "\u001b[36m(MapWorker(Map(convert_to_pinecone_vectors)->MapBatches(UpsertVectors)) pid=14649, ip=10.0.41.156)\u001b[0m - Expand out each key-value pair in the dict column into its own column\n",
      "\u001b[36m(MapWorker(Map(convert_to_pinecone_vectors)->MapBatches(UpsertVectors)) pid=14649, ip=10.0.41.156)\u001b[0m - Replace `None` values with an Arrow supported data type\n",
      "\u001b[36m(MapWorker(Map(convert_to_pinecone_vectors)->MapBatches(UpsertVectors)) pid=14649, ip=10.0.41.156)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "approx_total_batches = ds.count() // batch_size\n",
    "approx_total_batches\n",
    "concurrency = 9\n",
    "approx_num_upserts_per_connection = approx_total_batches // concurrency\n",
    "approx_num_upserts_per_connection\n",
    "\n",
    "pinecone_namespace = \"example-namespace\"\n",
    "\n",
    "\n",
    "class UpsertVectors:\n",
    "    def __init__(self):\n",
    "        self.pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\", YOUR_PINECONE_API_KEY))\n",
    "        self.index = self.pc.Index(index_name)\n",
    "        self.namespace = pinecone_namespace\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        self.index.upsert(\n",
    "            vectors=[\n",
    "                {\n",
    "                    \"id\": id_,\n",
    "                    \"values\": values,\n",
    "                    \"metadata\": metadata,\n",
    "                }\n",
    "                for id_, values, metadata in zip(\n",
    "                    batch[\"id\"], batch[\"values\"], batch[\"metadata\"]\n",
    "                )\n",
    "            ],\n",
    "            namespace=self.namespace,\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "ds = ds.map_batches(\n",
    "    UpsertVectors,\n",
    "    concurrency=concurrency,\n",
    "    batch_size=batch_size,\n",
    "    num_cpus=1,\n",
    ")\n",
    "\n",
    "\n",
    "df_written = ds.to_pandas().drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def verify_index(index_name: str, num_expected_vectors: int):\n",
    "    index = pc.Index(index_name)\n",
    "    stats = index.describe_index_stats()\n",
    "\n",
    "    while stats.total_vector_count != num_expected_vectors:\n",
    "        time.sleep(5)\n",
    "        stats = index.describe_index_stats()\n",
    "\n",
    "\n",
    "verify_index(index_name=index_name, num_expected_vectors=df_written.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'read_units': 6}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the default number of replicas for a Ray Serve deployment?\"\n",
    "\n",
    "model = SentenceTransformer('thenlper/gte-large', device=get_device())\n",
    "query_embedding = model.encode(query).tolist()\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "result = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=5,\n",
    "    namespace=pinecone_namespace,\n",
    ")\n",
    "\n",
    "result[\"matches\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'read_units': 5}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result = index.query(\n",
    "    vector=query_embedding, top_k=5, include_metadata=True, namespace=pinecone_namespace\n",
    ")\n",
    "\n",
    "result[\"matches\"]\n",
    "\n",
    "\n",
    "scores = [match[\"score\"] for match in result[\"matches\"]]\n",
    "scores\n",
    "\n",
    "\n",
    "score_threshold = 0.93  # determined based on data distribution\n",
    "matches_above_threshold = [\n",
    "    match for match in result[\"matches\"] if match[\"score\"] > score_threshold\n",
    "]\n",
    "len(matches_above_threshold), len(result[\"matches\"])\n",
    "\n",
    "\n",
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=5,\n",
    "    namespace=pinecone_namespace,\n",
    ")\n",
    "\n",
    "response[\"usage\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'read_units': 6}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=20,\n",
    "    namespace=pinecone_namespace,\n",
    "    include_metadata=True,\n",
    ")\n",
    "response[\"usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'read_units': 6}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=1,\n",
    "    namespace=pinecone_namespace,\n",
    "    include_metadata=True,\n",
    ")\n",
    "response[\"usage\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'read_units': 5}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=100,\n",
    "    namespace=pinecone_namespace,\n",
    ")\n",
    "\n",
    "response[\"usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_query_results(results):\n",
    "    for query_results in results:\n",
    "        print('query: ' + query_results.query + '\\n')\n",
    "        for document in query_results.documents:\n",
    "            print('document: ' + document.text.replace(\"\\n\", \"\\\\n\"))\n",
    "            print(\"title: \" + document.metadata[\"title\"])\n",
    "            print('source: ' + document.source)\n",
    "            print(f\"score: {document.score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base.record_encoder import AnyscaleRecordEncoder\n",
    "\n",
    "anyscale_record_encoder = AnyscaleRecordEncoder(\n",
    "    api_key=os.environ[\"ANYSCALE_API_KEY\"],\n",
    "    base_url=os.environ[\"ANYSCALE_BASE_URL\"],\n",
    "    batch_size=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.tokenizer import Tokenizer\n",
    "Tokenizer.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.environ.get('PINECONE_API_KEY') or '9386359a-0227-4d5b-80d9-b1bb7600dd08'\n",
    "os.environ[\"ANYSCALE_BASE_URL\"] = 'https://api.endpoints.anyscale.com/v1'\n",
    "os.environ[\"ANYSCALE_API_KEY\"] = os.environ.get('ANYSCALE_API_KEY') or 'esecret_f6dz2g16nnrai635si83z8upk8'\n",
    "\n",
    "from canopy.knowledge_base import KnowledgeBase\n",
    "\n",
    "INDEX_NAME = \"shanker-index\" # Set the index name here\n",
    "\n",
    "kb = KnowledgeBase(index_name=INDEX_NAME, record_encoder=anyscale_record_encoder)\n",
    "\n",
    "\n",
    "\n",
    "#from canopy.knowledge_base import list_canopy_indexes\n",
    "#list_canopy_indexes()\n",
    "#if not any(name.endswith(INDEX_NAME) for name in list_canopy_indexes()):\n",
    "#    print(name)\n",
    "#    kb.create_canopy_index()\n",
    "\n",
    "kb.connect()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: p1 pod capacity\n",
      "\n",
      "document: ### s1 pods\\n\\n\\nThese storage-optimized pods provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements.\\n\\n\\nEach s1 pod has enough capacity for around 5M vectors of 768 dimensions.\\n\\n\\n### p1 pods\\n\\n\\nThese performance-optimized pods provide very low query latencies, but hold fewer vectors per pod than s1 pods. They are ideal for applications with low latency requirements (<100ms).\\n\\n\\nEach p1 pod has enough capacity for around 1M vectors of 768 dimensions.\n",
      "title: indexes\n",
      "source: https://docs.pinecone.io/docs/indexes\n",
      "score: 0.918963\n",
      "\n",
      "document: ## Pod storage capacity\\n\\n\\nEach **p1** pod has enough capacity for 1M vectors with 768 dimensions.\\n\\n\\nEach **s1** pod has enough capacity for 5M vectors with 768 dimensions.\\n\\n\\n## Metadata\\n\\n\\nMax metadata size per vector is 40 KB.\\n\\n\\nNull metadata values are not supported. Instead of setting a key to hold a null value, we recommend you remove that key from the metadata payload.\\n\\n\\nMetadata with high cardinality, such as a unique value for every vector in a large index, uses more memory than expected and can cause the pods to become full.\n",
      "title: limits\n",
      "source: https://docs.pinecone.io/docs/limits\n",
      "score: 0.90533584\n",
      "\n",
      "document: ## Number of vectors\\n\\n\\nThe most important consideration in sizing is the [number of vectors](/docs/insert-data/) you plan on working with. As a rule of thumb, a single p1 pod can store approximately 1M vectors, while a s1 pod can store 5M vectors. However, this can be affected by other factors, such as dimensionality and metadata, which are explained below.\n",
      "title: choosing-index-type-and-size\n",
      "source: https://docs.pinecone.io/docs/choosing-index-type-and-size\n",
      "score: 0.9039885\n",
      "\n",
      "document: | Pod type | Dimensions | Estimated max vectors per pod |\\n| --- | --- | --- |\\n| p1 | 512 | 1,250,000 |\\n|  | 768 | 1,000,000 |\\n|  | 1024 | 675,000 |\\n| p2 | 512 | 1,250,000 |\\n|  | 768 | 1,100,000 |\\n|  | 1024 | 1,000,000 |\\n| s1 | 512 | 8,000,000 |\\n|  | 768 | 5,000,000 |\\n|  | 1024 | 4,000,000 |\\n\\n\\nPinecone does not support fractional pod deployments, so always round up to the next nearest whole number when choosing your pods. \\n\\n\\n# Queries per second (QPS)\n",
      "title: choosing-index-type-and-size\n",
      "source: https://docs.pinecone.io/docs/choosing-index-type-and-size\n",
      "score: 0.89091545\n",
      "\n",
      "document: ---\\n\\n* [Table of Contents](#)\\n* + [Overview](#overview)\\n\t+ [Pods, pod types, and pod sizes](#pods-pod-types-and-pod-sizes)\\n\t\t- [Starter plan](#starter-plan)\\n\t\t- [s1 pods](#s1-pods)\\n\t\t- [p1 pods](#p1-pods)\\n\t\t- [p2 pods](#p2-pods)\\n\t\t- [Pod size and performance](#pod-size-and-performance)\\n\t\t- [Distance metrics](#distance-metrics)\n",
      "title: indexes\n",
      "source: https://docs.pinecone.io/docs/indexes\n",
      "score: 0.88990456\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +49m20s)\u001b[0m Cluster is terminating (reason: user action).\n"
     ]
    }
   ],
   "source": [
    "from canopy.models.data_models import Query\n",
    "results = kb.query([Query(text=\"p1 pod capacity\")])\n",
    "\n",
    "print_query_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: p1 pod capacity\n",
      "\n",
      "document: ## Pod storage capacity\\n\\n\\nEach **p1** pod has enough capacity for 1M vectors with 768 dimensions.\\n\\n\\nEach **s1** pod has enough capacity for 5M vectors with 768 dimensions.\\n\\n\\n## Metadata\\n\\n\\nMax metadata size per vector is 40 KB.\\n\\n\\nNull metadata values are not supported. Instead of setting a key to hold a null value, we recommend you remove that key from the metadata payload.\\n\\n\\nMetadata with high cardinality, such as a unique value for every vector in a large index, uses more memory than expected and can cause the pods to become full.\n",
      "title: limits\n",
      "source: https://docs.pinecone.io/docs/limits\n",
      "score: 0.90533584\n",
      "\n",
      "document: # Limits\\n\\n[Suggest Edits](/edit/limits)This is a summary of current Pinecone limitations. For many of these, there is a workaround or we're working on increasing the limits.\\n\\n\\n## Upserts\\n\\n\\nMax vector dimensionality is 20,000.\\n\\n\\nMax size for an upsert request is 2MB. Recommended upsert limit is 100 vectors per request.\\n\\n\\nVectors may not be visible to queries immediately after upserting. You can check if the vectors were indexed by looking at the total with `describe_index_stats()`, although this method may not work if the index has multiple replicas. Pinecone is eventually consistent.\\n\\n\\nPinecone supports sparse vector values of sizes up to 1000 non-zero values.\\n\\n\\n## Queries\\n\\n\\nMax value for `top_k`, the number of results to return, is 10,000. Max value for `top_k` for queries with `include_metadata=True` or `include_data=True` is 1,000.\\n\\n\\n## Fetch and Delete\\n\\n\\nMax vectors per fetch or delete request is 1,000.\\n\\n\\n## Namespaces\\n\\n\\nThere is no limit to the number of <namespaces> per index.\n",
      "title: limits\n",
      "source: https://docs.pinecone.io/docs/limits\n",
      "score: 0.7974198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from canopy.models.data_models import Query\n",
    "results = kb.query([Query(text=\"p1 pod capacity\",\n",
    "                          metadata_filter={\"source\": \"https://docs.pinecone.io/docs/limits\"},\n",
    "                          top_k=2)])\n",
    "\n",
    "print_query_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.context_engine import ContextEngine\n",
    "context_engine = ContextEngine(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"query\": \"capacity of p1 pods\",\n",
      "    \"snippets\": [\n",
      "      {\n",
      "        \"source\": \"https://docs.pinecone.io/docs/indexes\",\n",
      "        \"text\": \"### s1 pods\\n\\n\\nThese storage-optimized pods provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements.\\n\\n\\nEach s1 pod has enough capacity for around 5M vectors of 768 dimensions.\\n\\n\\n### p1 pods\\n\\n\\nThese performance-optimized pods provide very low query latencies, but hold fewer vectors per pod than s1 pods. They are ideal for applications with low latency requirements (<100ms).\\n\\n\\nEach p1 pod has enough capacity for around 1M vectors of 768 dimensions.\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"https://docs.pinecone.io/docs/limits\",\n",
      "        \"text\": \"## Pod storage capacity\\n\\n\\nEach **p1** pod has enough capacity for 1M vectors with 768 dimensions.\\n\\n\\nEach **s1** pod has enough capacity for 5M vectors with 768 dimensions.\\n\\n\\n## Metadata\\n\\n\\nMax metadata size per vector is 40 KB.\\n\\n\\nNull metadata values are not supported. Instead of setting a key to hold a null value, we recommend you remove that key from the metadata payload.\\n\\n\\nMetadata with high cardinality, such as a unique value for every vector in a large index, uses more memory than expected and can cause the pods to become full.\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"https://docs.pinecone.io/docs/choosing-index-type-and-size\",\n",
      "        \"text\": \"## Number of vectors\\n\\n\\nThe most important consideration in sizing is the [number of vectors](/docs/insert-data/) you plan on working with. As a rule of thumb, a single p1 pod can store approximately 1M vectors, while a s1 pod can store 5M vectors. However, this can be affected by other factors, such as dimensionality and metadata, which are explained below.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n",
      "# tokens in context returned: 434\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "result = context_engine.query([Query(text=\"capacity of p1 pods\", top_k=5)], max_context_tokens=512)\n",
    "\n",
    "print(result.to_text(indent=2))\n",
    "print(f\"\\n# tokens in context returned: {result.num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.chat_engine import ChatEngine\n",
    "from canopy.llm.anyscale import AnyscaleLLM\n",
    "from canopy.chat_engine.query_generator import InstructionQueryGenerator\n",
    "\n",
    "anyscale_llm = AnyscaleLLM(\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    api_key=os.environ[\"ANYSCALE_API_KEY\"],\n",
    "    base_url=os.environ[\"ANYSCALE_BASE_URL\"],\n",
    ")\n",
    "\n",
    "chat_engine = ChatEngine(\n",
    "    context_engine,\n",
    "    query_builder=InstructionQueryGenerator(\n",
    "        llm=anyscale_llm,\n",
    "    ),\n",
    "    llm=anyscale_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from canopy.models.data_models import Messages, UserMessage, AssistantMessage\n",
    "\n",
    "def chat(new_message: str, history: Messages) -> Tuple[str, Messages]:\n",
    "    messages = history + [UserMessage(content=new_message)]\n",
    "    response = chat_engine.chat(messages)\n",
    "    assistant_response = response.choices[0].message.content\n",
    "    return assistant_response, messages + [AssistantMessage(content=assistant_response)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Each p1 pod has enough capacity for around 1M vectors of 768 dimensions.\n",
       "\n",
       "Source: https://docs.pinecone.io/docs/indexes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "history = []\n",
    "response, history = chat(\"What is the capacity of p1 pods?\", history)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " P1 pods are suitable for applications with low latency requirements, specifically less than 100ms.\n",
       "\n",
       "Source: https://docs.pinecone.io/docs/indexes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response, history = chat(\"And for what latency requirements does it fit?\", history)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " I do not have any information about Nachum, Yingjie Miao, and Mustafa Safdari from the provided context.\n",
       "\n",
       "Source: [{\"query\": \"Are Nachum, Yingjie Miao, and Mustafa Safdari known individuals?\", \"snippets\": [{\"source\": \"https://docs.pinecone.io/docs/langchain\", \"text\": \"```\\n\\n\\n```python\\n{'question': 'who was Benito Mussolini?',\\n 'answer': 'Benito Mussolini was an Italian politician and journalist who was the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party and invented the ideology of Fascism. He became dictator of Italy by the end of 1927 and was friends with German dictator Adolf Hitler. Mussolini attacked Greece and failed to conquer it. He was removed by the Great Council of Fascism in 1943 and was executed by a partisan on April 28, 1945. After the war, several Neo-Fascist movements have had success in Italy, the most important being the Movimento Sociale Italiano. His granddaughter Alessandra Mussolini has outspoken views similar to Fascism. \\\\n',\\n 'sources': 'https://simple.wikipedia.org/wiki/Benito%20Mussolini, https://simple.wikipedia.org/wiki/Fascism'}\"}, {\"source\": \"https://docs.pinecone.io/docs/using-public-datasets\", \"text\": \"\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\"}, {\"source\": \"https://docs.pinecone.io/docs/metadata-filtering\", \"text\": \"(\\\"E\\\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], {\\\"genre\\\": \\\"drama\\\"})\\n])\"}, {\"source\": \"https://docs.pinecone.io/docs/insert-data\", \"text\": \"(\\\"E\\\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], {\\\"genre\\\": \\\"drama\\\"})\\n])\"}, {\"source\": \"https://docs.pinecone.io/docs/langchain\", \"text\": \"[Document(page_content='Benito Amilcare Andrea Mussolini KSMOM GCTE (29 July 1883 \\u2013 28 April 1945) was an Italian politician and journalist. He was also the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party.\\\\n\\\\nBiography\\\\n\\\\nEarly life\\\\nBenito Mussolini was named after Benito Juarez, a Mexican opponent of the political power of the Roman Catholic Church, by his anticlerical (a person who opposes the political interference of the Roman Catholic Church in secular affairs) father. Mussolini\\\\'s father was a blacksmith. Before being involved in politics, Mussolini was a newspaper editor (where he learned all his propaganda skills) and elementary school teacher.\\\\n\\\\nAt first, Mussolini was a socialist, but when he wanted Italy to join the First World War, he was thrown out of the socialist party. He \\\\'invented\\\\' a new ideology, Fascism, much out of Nationalist\\\\xa0and Conservative views.\\\\n\\\\nRise to power and becoming dictator\\\\nIn 1922, he took power by having a large group of men, \\\"Black Shirts,\\\" march on Rome and threaten to take\"}]}] + What is the capacity of p1 pods?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "response, history = chat(\"Do you know about Nachum, Yingjie Miao, Mustafa Safdari?\", history)\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
