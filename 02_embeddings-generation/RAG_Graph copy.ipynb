{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /home/ray/anaconda3/lib/python3.10/site-packages (4.1.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Tuple, List, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"NEO4J_URI\"] = \"neo4j+s://2c4210e0.databases.neo4j.io\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"kJblCzArjVoBrEpIK-U4BUIoxy8YcLo8V4icKzi-6Kg\"\n",
    "\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Retrieval Augmented \\nGeneration workshop \\nRoie Schwaber-Cohen, Staff Developer Advocate (Pinecone) \\nConﬁdential information and property of Pinecone Systems, Inc. No part of these materials may be \\ncopied, used, shared, or disclosed except with written permission of Pinecone Systems, Inc.', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import psutil\n",
    "import ray\n",
    "import torch\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec, PodSpec\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../01_overview/RAG_Overview.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "\n",
    "pages[0]\n",
    "\n",
    "\n",
    "#text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "#documents = text_splitter.split_documents(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37969/3365253351.py:13: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(self._delete_all)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph cleared.\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "\n",
    "class GraphDBManager:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def clear_graph(self):\n",
    "        with self.driver.session() as session:\n",
    "            session.write_transaction(self._delete_all)\n",
    "\n",
    "    @staticmethod\n",
    "    def _delete_all(tx):\n",
    "        query = \"MATCH (n) DETACH DELETE n\"\n",
    "        tx.run(query)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uri = os.getenv(\"NEO4J_URI\")\n",
    "    user = os.getenv(\"NEO4J_USERNAME\")\n",
    "    password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "    db_manager = GraphDBManager(uri, user, password)\n",
    "    db_manager.clear_graph()\n",
    "    db_manager.close()\n",
    "\n",
    "    print(\"Graph cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='Roie Schwaber-Cohen', type='Person'), Node(id='Pinecone Systems, Inc.', type='Organization')], relationships=[Relationship(source=Node(id='Roie Schwaber-Cohen', type='Person'), target=Node(id='Pinecone Systems, Inc.', type='Organization'), type='STAFF_DEVELOPER_ADVOCATE')], source=Document(page_content='Retrieval Augmented \\nGeneration workshop \\nRoie Schwaber-Cohen, Staff Developer Advocate (Pinecone) \\nConﬁdential information and property of Pinecone Systems, Inc. No part of these materials may be \\ncopied, used, shared, or disclosed except with written permission of Pinecone Systems, Inc.', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 0})), GraphDocument(nodes=[], relationships=[], source=Document(page_content='2Quick Refresher', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 1})), GraphDocument(nodes=[Node(id='Llms', type='Entity'), Node(id='Natural Language Interface', type='Entity'), Node(id='Reasoning Engine', type='Entity')], relationships=[Relationship(source=Node(id='Llms', type='Entity'), target=Node(id='Natural Language Interface', type='Entity'), type='IS_A'), Relationship(source=Node(id='Llms', type='Entity'), target=Node(id='Reasoning Engine', type='Entity'), type='IS_A')], source=Document(page_content='•LLMs don’t know anything about our data. \\n•Consider LLMs as a Natural Language Interface  or a reasoning \\nengine instead of the source of truth. \\n•We query our knowledge based on the user’s prompt to retrieve \\ncontent we consider relevant .\\n•We inject the relevant content into the context window  of the LLM as \\nthe basis for future responses. \\n3Contextualized Meaning \\nGround LLMs', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 2})), GraphDocument(nodes=[Node(id='Embeddings Models', type='Concept'), Node(id='Knowledge Base', type='Concept'), Node(id='Llm', type='Concept'), Node(id='Metadata', type='Concept'), Node(id='Retrieval Augmented Generated (Rag)', type='Concept'), Node(id='Vector Database', type='Concept')], relationships=[Relationship(source=Node(id='Embeddings Models', type='Concept'), target=Node(id='Knowledge Base', type='Concept'), type='EMBEDDED_IN'), Relationship(source=Node(id='Knowledge Base', type='Concept'), target=Node(id='Llm', type='Concept'), type='TEACHES'), Relationship(source=Node(id='Metadata', type='Concept'), target=Node(id='Retrieval Augmented Generated (Rag)', type='Concept'), type='IMPROVES'), Relationship(source=Node(id='Retrieval Augmented Generated (Rag)', type='Concept'), target=Node(id='Vector Database', type='Concept'), type='WITH')], source=Document(page_content='•The user prompt is likely to be semantically ambiguous. \\n•We can use embeddings models to extract the semantic meaning from \\nthe user prompt, and to match it to data we care about. \\n•We embed our knowledge base which then allows us to query inject \\nsemantically relevant content into the context window. \\n•We can teach the LLM to say “I don’t know”: Using the similarity score, \\nwe can ﬁlter out responses that don’t pass a given threshold. \\n•We can leverage metadata to improve relevance and performance \\n4Retrieval Augmented Generated (RAG) \\nRAG with a vector database', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 3})), GraphDocument(nodes=[], relationships=[], source=Document(page_content='5Architecture', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 4})), GraphDocument(nodes=[Node(id='Rag Architecture', type='Concept'), Node(id='Ingestion', type='Concept')], relationships=[Relationship(source=Node(id='Rag Architecture', type='Concept'), target=Node(id='Ingestion', type='Concept'), type='PART_OF')], source=Document(page_content='RAG Architecture - Ingestion \\n6', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 5})), GraphDocument(nodes=[Node(id='Rag Architecture', type='Concept'), Node(id='Application', type='Concept')], relationships=[Relationship(source=Node(id='Rag Architecture', type='Concept'), target=Node(id='Application', type='Concept'), type='RELATED_TO')], source=Document(page_content='RAG Architecture - Application \\n7', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 6})), GraphDocument(nodes=[Node(id='Things', type='Concept')], relationships=[Relationship(source=Node(id='Things', type='Concept'), target=Node(id='Consider', type='Action'), type='INVOLVES')], source=Document(page_content='8Things to consider', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 7})), GraphDocument(nodes=[Node(id='Indexing', type='Concept'), Node(id='Embedding Model', type='Concept'), Node(id='Query', type='Concept'), Node(id='Retrieved Data', type='Concept'), Node(id='Content-Aware', type='Strategy'), Node(id='Programmatic', type='Strategy'), Node(id='Traditional Nlu Strategies', type='Approach'), Node(id='Topic Modeling', type='Strategy'), Node(id='Ner', type='Strategy'), Node(id='Chunking Strategies', type='Strategy')], relationships=[Relationship(source=Node(id='Indexing', type='Concept'), target=Node(id='Embedding Model', type='Concept'), type='RELATION'), Relationship(source=Node(id='Query', type='Concept'), target=Node(id='Retrieved Data', type='Concept'), type='RELATION'), Relationship(source=Node(id='Content-Aware', type='Strategy'), target=Node(id='Programmatic', type='Strategy'), type='RELATION'), Relationship(source=Node(id='Traditional Nlu Strategies', type='Approach'), target=Node(id='Topic Modeling', type='Strategy'), type='RELATION'), Relationship(source=Node(id='Traditional Nlu Strategies', type='Approach'), target=Node(id='Ner', type='Strategy'), type='RELATION'), Relationship(source=Node(id='Traditional Nlu Strategies', type='Approach'), target=Node(id='Chunking Strategies', type='Strategy'), type='RELATION')], source=Document(page_content='•What are you indexing? \\n•What embedding model are you using? \\n•Relation to query and retrieved data \\n•“Content-aware” vs programmatic \\n•Applying “traditional” NLU strategies like topic modeling, NER etc. \\n9Chunking strategies', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 8})), GraphDocument(nodes=[Node(id='Filtering', type='Concept'), Node(id='Context Enrichment', type='Concept'), Node(id='Ranking Boost', type='Concept'), Node(id='Domain Filtering', type='Concept'), Node(id='Temporal Relevance', type='Concept'), Node(id='Metadata', type='Concept')], relationships=[Relationship(source=Node(id='Metadata', type='Concept'), target=Node(id='Filtering', type='Concept'), type='USES'), Relationship(source=Node(id='Metadata', type='Concept'), target=Node(id='Context Enrichment', type='Concept'), type='USES'), Relationship(source=Node(id='Metadata', type='Concept'), target=Node(id='Ranking Boost', type='Concept'), type='USES'), Relationship(source=Node(id='Metadata', type='Concept'), target=Node(id='Domain Filtering', type='Concept'), type='USES'), Relationship(source=Node(id='Metadata', type='Concept'), target=Node(id='Temporal Relevance', type='Concept'), type='USES')], source=Document(page_content='•Filtering: \\n•Context Enrichment: Adds extra layers of information for more nuanced \\nresponses. \\n•Ranking Boost: Uses metadata like credibility for better document \\nselection. \\n•Domain Filtering: Allows targeted retrieval based on subject tags. \\n•Temporal Relevance: Utilizes timestamps for timely results. \\n10 Using metadata', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 9})), GraphDocument(nodes=[Node(id='Truelens', type='System'), Node(id='Galileo', type='System'), Node(id='Langsmith', type='System'), Node(id='Llamaindex', type='System'), Node(id='Latency', type='Performancemetric'), Node(id='F1-Score', type='Performancemetric'), Node(id='User Feedback', type='Performancemetric'), Node(id='Monitoring And Evaluation', type='Concept')], relationships=[Relationship(source=Node(id='Truelens', type='System'), target=Node(id='Monitoring And Evaluation', type='Concept'), type='MONITORS'), Relationship(source=Node(id='Galileo', type='System'), target=Node(id='Monitoring And Evaluation', type='Concept'), type='MONITORS'), Relationship(source=Node(id='Langsmith', type='System'), target=Node(id='Monitoring And Evaluation', type='Concept'), type='MONITORS'), Relationship(source=Node(id='Llamaindex', type='System'), target=Node(id='Monitoring And Evaluation', type='Concept'), type='MONITORS'), Relationship(source=Node(id='Latency', type='Performancemetric'), target=Node(id='Monitoring And Evaluation', type='Concept'), type='MEASURES'), Relationship(source=Node(id='F1-Score', type='Performancemetric'), target=Node(id='Monitoring And Evaluation', type='Concept'), type='EVALUATES'), Relationship(source=Node(id='User Feedback', type='Performancemetric'), target=Node(id='Monitoring And Evaluation', type='Concept'), type='COMPUTES')], source=Document(page_content='•Use systems such as TrueLens, Galileo, LangSmith and LlamaIndex to \\nmonitor the performance of your application \\n•Latency: Measure time from query to response. \\n•F1-Score: Evaluate accuracy on QA datasets. \\n•User Feedback: Compute user satisfaction index. \\n11 Monitoring and evaluation', metadata={'source': '../01_overview/RAG_Overview.pdf', 'page': 10}))]\n",
      "<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7f975c38e140>\n"
     ]
    }
   ],
   "source": [
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0125\") # gpt-4-0125-preview occasionally has issues\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(pages)\n",
    "print(graph_documents)\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e4c4fbdf564d7fad5bc678f71e34e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# directly show the graph resulting from the given Cypher query\n",
    "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    #display(widget)\n",
    "    return widget\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "# Retriever\n",
    "\n",
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "\n",
    "# Extract entities from text\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, organization, or business entities that \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting organization and person entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Roie Schwaber-Cohen']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_chain.invoke({\"question\": \"Who is Roie Schwaber-Cohen?\"}).names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roie Schwaber-Cohen - STAFF_DEVELOPER_ADVOCATE -> Pinecone Systems, Inc.\n",
      "Llms - IS_A -> Natural Language Interface\n",
      "Llms - IS_A -> Reasoning Engine\n",
      "Embeddings Models - EMBEDDED_IN -> Knowledge Base\n",
      "Knowledge Base - TEACHES -> Llm\n",
      "Metadata - IMPROVES -> Retrieval Augmented Generated (Rag)\n",
      "Metadata - USES -> Filtering\n",
      "Metadata - USES -> Context Enrichment\n",
      "Metadata - USES -> Ranking Boost\n",
      "Metadata - USES -> Domain Filtering\n",
      "Metadata - USES -> Temporal Relevance\n",
      "Retrieval Augmented Generated (Rag) - WITH -> Vector Database\n",
      "Rag Architecture - RELATED_TO -> Application\n",
      "Rag Architecture - PART_OF -> Ingestion\n",
      "Things - INVOLVES -> Consider\n",
      "Indexing - RELATION -> Embedding Model\n",
      "Query - RELATION -> Retrieved Data\n",
      "Content-Aware - RELATION -> Programmatic\n",
      "Traditional Nlu Strategies - RELATION -> Topic Modeling\n",
      "Traditional Nlu Strategies - RELATION -> Ner\n",
      "Traditional Nlu Strategies - RELATION -> Chunking Strategies\n",
      "Truelens - MONITORS -> Monitoring And Evaluation\n",
      "Galileo - MONITORS -> Monitoring And Evaluation\n",
      "Langsmith - MONITORS -> Monitoring And Evaluation\n",
      "Llamaindex - MONITORS -> Monitoring And Evaluation\n",
      "Latency - MEASURES -> Monitoring And Evaluation\n",
      "F1-Score - EVALUATES -> Monitoring And Evaluation\n",
      "User Feedback - COMPUTES -> Monitoring And Evaluation\n"
     ]
    }
   ],
   "source": [
    "print(structured_retriever(\"Who is Roie Schwaber-Cohen?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Which company did Roie Schwaber-Cohen belong to?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Roie Schwaber-Cohen belonged to Pinecone Systems, Inc.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Which company did Roie Schwaber-Cohen belong to?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: What does Roie Schwaber-Cohen present?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Roie Schwaber-Cohen presents at the Retrieval Augmented Generation workshop.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What he presents?\",\n",
    "        \"chat_history\": [(\"Which company did Roie Schwaber-Cohen belong to?\", \"Pinecone Systems, Inc.\")],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: What is the NVMe protocol?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'m sorry, I cannot provide an answer to the question \"What is NVMe protocol?\" based on the provided context.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What is NVMe protocal?\",\n",
    "        \"chat_history\": [(\"Which company did Roie Schwaber-Cohen belong to?\", \"Pinecone Systems, Inc.\")],\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
