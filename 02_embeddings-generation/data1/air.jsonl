{"id":"2210.03945","source":"http:\/\/arxiv.org\/pdf\/2210.03945","text":"UNDERSTANDING HTML WITH LARGE LANGUAGE\nMODELS\nIzzeddin Gur, O\ufb01r Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang\nAakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust\nGoogle Research\nfizzeddin,ofirnachum,yingjiemiao,msafdari,austinvhuang\nchowdhery,sharannarang,nfiedel,sandrafaust g@google.com\nABSTRACT\nLarge language models (LLMs) have shown exceptional performance on a va-\nriety of natural language tasks. Yet, their capabilities for HTML understanding\n\u2013 i.e., parsing the raw HTML of a webpage, with applications to automation of\nweb-based tasks, crawling, and browser-assisted retrieval \u2013 have not been fully\nexplored. We contribute HTML understanding models (\ufb01ne-tuned LLMs) and an\nin-depth analysis of their capabilities under three tasks: (i) Semantic Classi\ufb01ca-\ntionof HTML elements, (ii) Description Generation for HTML inputs, and (iii)\nAutonomous Web Navigation of HTML pages. While previous work has devel-\noped dedicated architectures and training procedures for HTML understanding,\nwe show that LLMs pretrained on standard natural language corpora transfer re-\nmarkably well to HTML understanding tasks. For instance, \ufb01ne-tuned LLMs are\n12% more accurate at semantic classi\ufb01cation compared to models trained exclu-\nsively on the task dataset. Moreover, when \ufb01ne-tuned on data from the MiniWoB\nbenchmark, LLMs successfully complete 50% more tasks using 192x less data\ncompared to the previous best supervised model. Out of the LLMs we evalu-\nate, we show evidence that T5-based models are ideal due to their bidirectional\nencoder-decoder architecture. To promote further research on LLMs for HTML\nunderstanding, we create and open-source a large-scale HTML dataset distilled\nand auto-labeled from CommonCrawl.1\n1 I NTRODUCTION\nWeb crawling (Olston et al., 2010), form-\ufb01lling (Diaz et al., 2013; Gur et al., 2021), or information\nretrieving web agents (Nogueira & Cho, 2016) are important for both automating and assisting\nusers in web-based tasks. These and similar applications rely on models that can search for speci\ufb01c\ncontent or controls on a web page as well as navigate a website autonomously. Since a web page in\nits raw form is represented as an HTML-based text sequence, the success of models for web-based\ntasks relies on their ability to understand HTML semantics, structure, and embedded interactions.\nThe predominant approach to web automation and HTML understanding is to train specialized mod-\nels, i.e., gathering application-speci\ufb01c datasets and designing neural network (NN) architectures to\nleverage inductive biases of the HTML\u2019s structure; see, e.g., Liu et al. (2018); Toyama et al. (2021);\nGur et al. (2021); Humphreys et al. (2022). However, both dataset collection and neural architecture\ndesign are expensive, time-consuming, and require highly-specialized, domain-speci\ufb01c knowledge.\nMeanwhile, in the natural language processing (NLP) literature, large language models (LLMs) have\nemerged as a solution to the dif\ufb01culties of dataset collection and specialized NN design (Kaplan\net al., 2020; Bommasani et al., 2021). A popular paradigm in NLP is to take an off-the-shelf LLM\n\u2013 pretrained on a large text corpus via an unsupervised and task-agnostic learning objective \u2013 and\neither \ufb01ne-tune or prompt the LLM on a small task-speci\ufb01c dataset. This paradigm has shown\nexceptional performance on a variety of NLP tasks (Xue et al., 2020; Brown et al., 2020; Austin\net al., 2021). Whether LLMs can be applied to HTML understanding \u2013 especially given the much\nlarger context and sequence lengths \u2013 remains an under-explored question.\n1See visualizations of the results at https:\/\/sites.google.com\/view\/llm4html\/home .\n1arXiv:2210.03945v2  [cs.LG]  19 May 2023\n<html> \n   <body> \n      <form class= \"login-form\" >\n   <div> \n            <label class= \"form-label\" for= \u201duName\u201d >\n               Enter Email Address \n            <\/label> \n      <label class= \"form-label\" for= \u201dpass\u201d >\n               Enter Password: \n            <\/label> \n   <\/div> \n         <div> \n <input type= \"email\"  id=\"uName\u201d >\n<input type= \"password\"  id=\"pass\" >\n<span class= \"hidden\" >\n               Please enter your password. \n            <\/span> \n         <\/div> \n         <button type= \"submit\" >Sign In <\/button> \n       <\/form> \n   <\/body> \n<\/html> (a)\n<div><label class= \"form-label\" for= \u201duName\u201d >Email Address <\/label><label \nclass= \"form-label\" for= \u201dpass\u201d >Enter Password: <\/label><\/div><div><input \ntype= \"email\"  id=\"uName\u201d target ><input type= \"password\"  id=\"pass\" ><span \nclass= \"hidden\" >Please enter your password. <\/span><\/div> (b)\nFigure 1: a) HTML example page with a highlighted salient element, an element of interest (dashed box).\nAll canonical tasks evaluate a distinct interaction with this element, either by classifying it as one of a set of\ncategories, generating a text description of its purpose, or applying an action as part of a sequential navigation\nof a multi-page website. b) LLM architectures overview. Dashed boxes denote sub-modules that are speci\ufb01c to\neither encoder-only or encoder-decoder models. For encoder-only models, we add an extra classi\ufb01cation layer.\nDecoder-only models (not in the diagram) are similar to encoder-decoder models, the main difference is that\nthe HTML snippet is fed to the decoder and processed from left-to-right.\nIn this paper, we investigate whether LLMs can be applied to HTML understanding to produce\nbetter-performing, more sample-ef\ufb01cient HTML understanding models and without the need for\ncustom NN architecture design. To that end, we present a suite of three benchmarking tasks for\nHTML understanding that capture the essence of these applications and require understanding both\nstructure and content. First, we devise Semantic Classi\ufb01cation as a task that requires a model to\nclassify a given HTML element into one of a set of categories, such as address, email, password\netc., with application to automated form-\ufb01lling. Second, we present Description Generation , a\nlabel-extraction task where a model is given an HTML snippet and is asked to produce a natural\nlanguage description. For instance for an email \ufb01eld, the description might be \u201cPlease enter your\nemail address.\u201d Note that in the majority of web pages, this connection between input elements and\ndescription content is only implicit in the raw HTML code and inferring such links is a prerequisite\nfor higher-level navigation objectives. The third task is Autonomous Web Navigation (Shi et al.,\n2017). A model is presented with an HTML page paired with a natural language command and\nmust apply appropriate actions on a sequence of HTML pages to satisfy the command. See Figure\n1a for a simpli\ufb01ed example of these tasks.\nWith these benchmark tasks in hand, we evaluate the transfer capabilities of a variety of pretrained\nLLMs (Table 1), varying in architecture (encoder-only, encoder-decoder, or decoder-only), model\nsize (from 24.6M to 62B parameters), and training data corpora (both including and excluding pre-\ntraining NLP and HTML corpus). While prior work universally pre-parses the HTML as input to the\nmodel (Gur et al., 2021; Liu et al., 2018; Nakano et al., 2021), ours \u2013 to the best of our knowledge \u2013 is\nthe \ufb01rst work that uses raw, unprocessed HTML. Our results show that LLMs demonstrate a remark-\nable level of HTML understanding across all tasks, with up to 192\u0002more sample-ef\ufb01ciency than\nmodels trained from scratch, and achieving a new SoTA for supervised learning on the MiniWoB\nbenchmark suite (Shi et al., 2017). The encoder-decoder architectures with bi-directional attention\nshow the best performance across the board even when their pretraining does not include HTML. In\naddition, we show that the performance scales sub-linearly with the model size.\nThe broader objective of this research is to advance the integration of LLMs with autonomous web\nagents. It has only been in the last year that researchers have begun to utilize LLMs outside of\nNLP and integrate them as core capabilities in autonomy (Lu et al. (2021); Ahn et al. (2022)). In\nthis context, LLMs are reasoning engines for sequential decision making agents interacting with\nenvironments.\nThe present work is the \ufb01rst in the research literature to embed an LLM and train it as an agent for\nautonomous web navigation. This requires new implementations to adapt LLM training for behavior\n2\ncloning in addition to designing interfaces for integrating text generation into a perception-compute-\naction cycle operating in a stateful web environment. Our implementation allows us to answer new\nquestions regarding trade-offs among various model characteristics.\nWe believe these contributions expand the scope of language models and connect their unique capa-\nbilities with autonomous agents for the web. We provide a new perspective on machine learning for\nHTML understanding and web automation, showing that pretrained LLMs can achieve signi\ufb01cant\nperformance on such tasks, reducing the need for specialized architectures and training protocols.\nTo encourage further research in this direction, we open sourced2model weights for agents used in\nthe WoB environment and our dataset for description generation.\n2 R ELATED WORK\nHTML Understanding Autonomous web navigation has been a popular application for neural net-\nwork models, and a variety of works propose simulated websites for training web-based agents, with\napplication to task ful\ufb01llment (Yao et al., 2022; Gur et al., 2021; Burns et al., 2022; Mazumder &\nRiva, 2020; Shi et al., 2017; Liu et al., 2018) as well as information retrieval or question-answering\n(Adolphs et al., 2021; Nogueira & Cho, 2016). Simulated websites provide an easy way to evaluate\nmodels online, and for this reason we use the existing MiniWoB benchmark (Shi et al., 2017) for our\nweb navigation setting. However, it is still important to have a mechanism for evaluating models on\na wide variety of real-world websites. This was the key motivation for generating our own dataset\nfor the description generation task, which is distilled and auto-labeled from CommonCrawl and is a\nkey contribution of our paper.\nAlongside these benchmarks, many works have developed models for web navigation and related\nsubtasks (Pasupat et al., 2018; Bommasani et al., 2021; He et al., 2021; Gur et al., 2021; Humphreys\net al., 2022; Liu et al., 2018; Jia et al., 2019). These works often rely on specialized neural network\narchitectures that leverage inductive biases of HTML structure, or on preprocessing of HTML to\nmake it easier to input to a model (Li et al. (2021a;b)). In contrast, our work takes a minimalist\napproach, providing HTML in text form with minimal processing and using widely-adopted trans-\nformer networks.\nLLMs and HTML Works that explore the intersection of LLMs and HTML generally fall into two\ncategories. The \ufb01rst category uses LLMs to assist web navigation (Nakano et al., 2021; Yao et al.,\n2022), and typically relies on a custom preprocessing to map the context and structure of a web page\nto natural language, thus severely restricting what HTML pages the model can parse. The second\ncategory pretrains LLMs on a large corpora of HTML text (Aghajanyan et al., 2021). However,\nthese works typically restrict the model evaluation to standard NLP tasks, e.g., summarization and\nquestion\/answering as opposed to tasks more relevant to HTML understanding and web automation.\nOur work can be thought of as the reverse: We keep the pretraining of LLMs unchanged and focus\non the mechanisms for transferring the pretrained LLMs to HTML-relevant tasks.\n3 B RIEF BACKGROUND ON HTML ASSEMI-STRUCTURED TEXT DATA\nHTML is a markup language, used to organize web page structure andcontent . Consider the\nexample HTML page in Figure 1a. This web page includes two adjacent input elements, one for\ne-mail and another for password, with their corresponding label s on a separate branch of the page.\nThese input s and label s are one of many possible elements that serve as HTML building blocks.\nEach element has a set of attributes \u2013 key and value pair \u2013 that describe the element\u2019s content, such\nas style and human-readable text. When rendered in a browser, these attributes will be responsible\nfor how the element is shown and where it is positioned. In the example in Figure 1a, the \ufb01rst\ninput has three attributes, tag=\"input\" ,type=\"email\" , and id=\"uName\" , that identify\nthe element as an email input with an identi\ufb01er (\u201cuName\u201d) that can be accessed programmatically.\n2https:\/\/console.cloud.google.com\/storage\/browser\/gresearch\/webllm\n3\nModel\nTask Dataset Size Input Architecture Output Task Output\nAutonomous Web Navigation MiniWoB Demos (Shi et al., 2017) 12K PageEnc-DecText DictionaryDec\nSemantic Classi\ufb01cation Annotated Shopping Webpages (Gur et al., 2021) 28K Snippet All Text Category\nDescription Generation CommonCrawl (new) 85K SnippetEnc-DecText TextDec\nTable 1: Task, dataset, and model summary. All models receive raw HTML. Autonomous Web Navigation\nreceives the entire HTML, while the other tasks receive HTML snippets extracted given salient element.\n4 C ANONICAL TASKS FOR HTML U NDERSTANDING\nWe devise three canonical tasks to study HTML understanding capabilities of LLM-based web\nagents. These tasks require correctly interpreting both structure and content to varying degrees\nto make predictions, with autonomous navigation being the most challenging capability of the three.\nAutonomous Web Navigation . This task evaluates how well a model navigates multi-page web-\nsites as a sequential decision-making problem (Shi et al., 2017; Liu et al., 2018). At the beginning\nof an episode, the agent is given a natural language instruction, e.g. Enter the username \u201clyda\u201d\nand the password \u201cN22t\u201d into the text \ufb01elds and press login . The agent applies actions to a se-\nquence of HTML pages, where each action is of the form function(selector, text) . The\nfunction is one of click ortype,selector is an integer pointer that uniquely identi\ufb01es an ele-\nment, and text is a text to input if the type functionality is activated. An episode terminates when\neither the page reaches a terminal state (e.g., the \u2018sign in\u2019 button is clicked) or the maximum number\nof steps is reached.\nSemantic Classi\ufb01cation .Many HTML understanding applications require a model that can classify\nHTML elements into standardized categories. For example, in automated form-\ufb01lling (Diaz et al.,\n2013; Gur et al., 2021), it is useful to identify a \u2018submit button\u2019 across many websites (e.g., shopping,\n\ufb02ight booking, utility application) with various button representations (e.g., position, color, or text).\nThus, we formulate Semantic Classi\ufb01cation as classifying elements into role categories. Take the\nexample HTML in Figure 1a which includes two input elements and a submit button . Let\u2019s\npick the \ufb01rst input as an element of interest to be classi\ufb01ed by the system, also called a salient\nelement . The system should classify this element as username , since it appears on a login page and\nit has a label with Email Address which is typically associated with the username in form-\ufb01lling\napplications. To solve this, the system can aggregate information from multiple sources in the page\n\u2013 the label that says Enter Email Address , theinput attributes ( type=\u201cemail\u201d andid=\u201cuName\u201d ),\nor even the ordering of other elements in the page such as \u2018password\u2019 and \u2018sign in\u2019.\nDescription Generation .Motivated by applications in accessibility-minded web browser con-\ntrol (Jorgensen & Binsted, 2005), we formulate description generation as an extractive problem\nwhere the goal is to locate the textual description of an element in the HTML and generate it as\noutput. For instance, the description of the salient element in Figure 1a is Enter Email Address ;\nwhen rendered, this label will appear above the \u2018email\u2019 input \ufb01eld. HTML provides a large\namount of \ufb02exibility, and so in general a descriptive text that appears alongside a speci\ufb01c element\nwhen rendered can be very far from that element when looking at the HTML plaintext. Thus, this\ntask evaluates a model\u2019s ability to understand the structure of HTML as it would appear to a user,\ndespite not having access to the rendered web page directly.\n5 D ATASETS\nEach of our canonical tasks requires a separate dataset, with the description generation task using a\nnewly contributed, auto-labelled dataset based on CommonCrawl.\nAutonomous Web Navigation .We use the 12K demonstrations included in the publicly available\nMiniWoB benchmark (Shi et al., 2017), which encompass 62 website applications ranging from\nemail forwarding to social media interactions. Each demonstration is a sequence of (instruction,\nHTML, action) tuples. Every element in a MiniWoB demonstration is accompanied by a reference\nnumber unique within its respective pages. This number can be used as an element selector, making\nthe action space uni\ufb01ed across all tasks and time steps. For instance, the action in Figure 1a would be\n4\ntype(ref=5, \u201dusername@email.com\u201d) , where 5 refers to the index of the input when counted from\ntop-to-bottom. As model input, we concatenate the natural language instruction and HTML into a\nsingle text input sequence. Similarly, we treat the action as a text sequence for the model to predict.\nSemantic Classi\ufb01cation .We use a dataset of 28K labelled examples, containing 66 different cat-\negories, of the form (HTML, element, category) , previously used in the context of environment\ngeneration (Gur et al., 2021). The dataset consists of HTMLs from real-world shopping websites\nand categories relevant to form-\ufb01lling during payment and checkout on these websites.\nDescription Generation .For this task, we derive a dataset from CommonCrawl.3CommonCrawl\ndoes not include renderings or annotations that would reveal what text in the HTML is associated\nwith which elements. Instead, we infer descriptions of various elements by exploiting a special\nattribute in the HTML schema known as for. As an example in Figure 1a, the \ufb01rst label in\nthe HTML has a for attribute with value uName , which is the idof the element described by\nlabel ; in this case, the idis that of the \ufb01rst input in the page. This annotation does not affect\nthe rendering of the page and is typically used for accessibility purposes. We utilize the information\ngiven by these for attributes to create a large-scale dataset to study description generation. A small\nsample is available in the supplemental material, while the entire dataset will be available upon\npublication.\nSpeci\ufb01cally, we collected 100 WARC (from April 2019) \ufb01les from the CommonCrawl project and\nextracted all HTML label s that have a for attribute. Removing non-Unicode and alphanumeric\ntext in HTML label s results in a 400K example datset. We balance the distribution of labels,\neffectively downsampling the dataset to 85Ksamples. Each example is represented as (HTML,\nelement, description) , where HTML is the HTML plaintext of the page, element is the element\nwhose idattribute matches that appearing in the label \u2019sfor attribute, and description is the text\ninside the label element (see example in Figure 1a). More details of the dataset can be found in\nAppendix A.1.\n6 P RE-PROCESSING\nIn treating HTML as token sequences, we minimize any HTML tree pre-processing prior to model\ninput. We thus provide HTML as raw text (i.e., sequences of text tokens) and only apply a snippet\nextraction pre-processing for pages which are too large to \ufb01t into the typical LLMs context windows.\nSnippet Extraction. Real HTML pages can grow extremely large, reaching thousands of elements,\nfar beyond the context window of the largest LLM that we studied (1920 tokens in PaLM (Chowdh-\nery et al., 2022)). LLMs typically truncate such long sequences, which can be detrimental to HTML\nunderstanding as HTMLs are not linearly structured. We take an element-centric approach and ex-\ntract HTML snippets (a small portion of HTML code) surrounding a salient element (Figure 5). A\nsimple heuristic, which controls the tree\u2019s width and depth, guides the process: Start with a salient\nelement and traverse its ancestors in the HTML tree until a stopping condition is satis\ufb01ed. As we\ntraverse up, we estimate the height of the tree and the increased number of descendants of the new\nroot. We stop when either metric violates a pre-de\ufb01ned limit and take the resulting sub-tree as the\nsnippet. We mark the salient element using a special attribute, called target , to distinguish it from\nother elements. We perform the snippet extraction for the semantic classi\ufb01cation and description\ngeneration datasets, and keep the full HTML pages in MiniWoB because these pages are typically\nmuch smaller than real-world HTML.\nHTML un-Parsing. We provide the models with the unparsed plaintext HTML in the form of\na sequence of tokens. This canonical representation does not require speci\ufb01c model architectures\nsuch as hierarchical networks (Liu et al., 2018; Gur et al., 2021) and can be fed into any LLM. We\ntransform all datasets by converting every HTML page or snippet into a sequence. For MiniWoB,\nwe additionally concatenate (action history, instruction, HTML) tuples into a single sequence.\n3http:\/\/commoncrawl.org\n5\n7 M ODEL TRAINING\nWe study a variety of transformer-based LLMs (Vaswani et al., 2017) with different sizes and archi-\ntectures for HTML understanding tasks (Table 1). In the rest of the text, we pre\ufb01x models \ufb01ne-tuned\nforAutonomous Web Navigation ,Description Generation , and Semantic Classi\ufb01cation with WebN-\n, WebD-, and WebC-, respectively. For instance, WebD\u2013T5-3B is the three billion parameter T5\nmodel (Raffel et al., 2020) \ufb01ne-tuned for the Description Generation task. The rest of this section\nelaborates on training details.\nEncoder-Decoder and Decoder-only Models. We train encoder-decoder models, i.e., T5 (Raffel\net al., 2020), and decoder-only models, i.e., LaMDA (Thoppilan et al., 2022) and PaLM (Chowdh-\nery et al., 2022), with text input and text output (Figure 1b). Inputs are raw HTML pages or snippet\ntexts; similarly, outputs are categories, natural language descriptions, or actions represented as text.\nNamely, for Semantic Classi\ufb01cation we use the textual representation of categories, similar to previ-\nous classi\ufb01cation problems in NLP (Raffel et al., 2020). For Autonomous Web Navigation , actions\nare converted into text by \ufb01rst converting them into key and value pairs and then concatenating the\npairs.\nMany websites in MiniWoB require multiple interactions, such as click-button-sequence orclick-\ncheckboxes , where each interaction might cause a subtle change in the website state. For instance,\nafter clicking on a checkbox in the click-checkboxes website, its value \ufb02ips from positive to negative\nor the other way around, which is not always re\ufb02ected in LLMs\u2019 predictions and leads to action\nrepetitions. We solve this issue by augmenting tuples in the dataset with a sequence of past actions,\n(action history, instruction, HTML, action) , and allowing LLMs to learn from past experience.\nEncoder-only Models. We train encoder-only models, i.e., BERT (Devlin et al., 2018), with text\ninput and categorical output. We keep semantic categories as discrete one-hot classes. To train\nencoder-only models, we add a new classi\ufb01cation layer after the \ufb01nal encoder layer to produce a\ndistribution over semantic categories. In addition to the typical BERT models, we study Mobile-\nBERT (Sun et al., 2020), distilled from BERT-large with inverted bottlenecks, and Albert-XL (Lan\net al., 2020), with parameter sharing and embedding split.\n8 R ESULTS\nWe now present the results of \ufb01ne-tuned LLMs for HTML understanding. We compare the models\u2019\nperformance with the existing baselines where possible (autonomous web navigation) and against\nother LLM architectures and training regimes (all tasks). Sections 8.1, 8.2, and 8.3 evaluate task-\nspeci\ufb01c performance, while Section 8.4 assesses the performance across all the tasks.\nMetrics: For autonomous web navigation we evaluate models\u2019 Success Rate , which is averaged over\n100 episodes per task. For the other tasks, we use Accuracy to measure exact match between predic-\ntion and ground truth. In the description generation task, we additionally provide evaluations using\nalternative \u2018soft\u2019 text evaluation metrics, BLEU andROUGE-1 , measuring the similarity between\npredicted and ground truth text.\n8.1 A UTONOMOUS WEBNAVIGATION RESULTS\nForAutonomous Web Navigation we \ufb01ne-tune two WebN- encoder-decoder architectures (WebN-\nT5-large and WebN-T5-3B) on 12k demonstrations from human-annotated real websites. We eval-\nuate the models on MiniWob (Liu et al., 2018) benchmark, and compare with specialized architec-\ntures trained using supervised learning (SL) on 2.4 million human expert demonstrations CC-Net\n(SL) (Humphreys et al., 2022), and two RL models bootstrapped with SL, CC-Net (SL) (CC-Net\n(SL & RL) (Humphreys et al., 2022), and WGE (SL & RL) (Liu et al., 2018)). Additionally, we\ncompare with the decoder-only architecture (WebN-Lambda-1B) and perform an ablation study on\nthe impact of including the action history in the input.\nComparison to SoTA. Since previous works report success on only a subset of websites in Mini-\nWoB, we evaluate on 48 out of 62 websites that are common across all models. Table 8 in the\nAppendix reports \ufb01ne-grained results while Figure 2a presents results averaged over all websites.\nCompared to CC-Net (SL) which is trained on all 2.4M demonstrations, WebN-T5-3B improves the\n6\n2.4M \nDemos 12K \nDemos (a) Baseline comparison.Model Name Success (%) Model Size\nT5-large 18.1 800M\nLaMDA-1B 15.6 1B\nT5-3B 11.1 3B\nWebN-T5-large 46.4 800M\nWebN-LaMDA-1B 48.8 1B\nWebN-T5-3B 51.8 3B\n(b) Pre-training effect.\nFigure 2: a) WebN\u2013T5* performance compared to the previous SOTA models on MiniWoB benchmark.\nWebN-T5-3B improves the task success 16% while using 192 times less data, compared to the best supervised\nlearning (SL) model, CC-Net (SL). LLMs performance is only surpassed by works utilizing RL, requiring or-\nders of magnitude more online experience interaction with websites. b) LLMs with and without pretraining\nonAutonomous Web Navigation task. Those with pretraining (denoted by the \u2018WebN-\u2019 pre\ufb01x) show a 2.5-4.5x\nperformance improvement.\nModel Name Test (%) Dev (%) Model Size Code in training Corpus\nWebC-MobileBERT 78.1 77.7 24.6 M\n0%WebC-Albert-XL 83.5 83.1 58.9 M\nWebC-BERT-smallest 84.4 83.6 38.7 M\nWebC-BERT-small 84.4 85.2 52.8 M\nWebC-BERT-medium 85.2 84.5 67 M\nWebC-BERT-base 83.9 84.8 109.5 M\nWebC-BERT-large 84.1 85.8 335.2 M\nWebC-T5-base 86.8 89.9 250 M\nWebC-T5-large 87.0 89.3 800 M\nWebC-T5-3B 87.7 90.3 3 B\nWebC-LaMDA-1B 87.4 87.1 1 B 12.5% Code\nWebC-PaLM-8B 86.6 89.9 8 B 5% Code (0.875% HTML)\nWebC-PaLM-62B 88.7 90.5 62 B 5% Code (0.875% HTML)\nT5-large 76.4 75.2 800 M\n0% T5-3B 77.2 73.8 3 B\nPaLM-8B 73.3 70.1 8 B\nTable 2: LLMs performance on the Semantic Classi\ufb01cation task. Fine-tuning off-the-shelf pretrained LLMs\n(model names with pre\ufb01x \u2018Web*\u2019) helps LLMs transfer better compared to training the same architecture from\nscratch on the HTML dataset (model names without pre\ufb01x \u2018Web*\u2019), improving the accuracy of PaLM-8B more\nthan 12%. While WebC-PaLM-62B clearly performed better than all other models, we found WebC-T5-large\nto be competitive with much larger models such as WebC-LaMDA-1B or WebC-PaLM-8B.\nsuccess 16% while only training on 12K publicly-available demonstrations, yielding over 192x im-\nprovement in sample-ef\ufb01ciency. We \ufb01nd that all choices of LLMs outperform previous SL models.\nNotably, WebN-T5-3B signi\ufb01cantly improves on websites requiring multiple-action sequences such\nasclick checkboxes or websites requiring entering text such as login user (Table 8). We observe that\nthe performance of LLMs is only surpassed by previous works utilizing RL, which require orders of\nmagnitude more online experience interaction. Extending our \ufb01ne-tuned LLMs to an RL setting is\na promising avenue for future work.\nAction history ablation. Across all LLMs we consistently observe a decrease in success, on av-\nerage 6.4%, when past actions are excluded from the inputs (Figure 2a). Action history helps with\nwebsites that require entering multiple texts, as well as understanding minor changes that could be\ndif\ufb01cult to detect (e.g. click checkboxes andmulti layout ).multi layout requires entering 3 different\ntexts in the website where the layout is randomized at each episode, yet, surprisingly, even the (rel-\natively smaller) WebN-T5-large model without action history outperforms the CC-Net (SL) model;\nillustrating that incorporating action history is not the only contributing factor for the better success.\n7\nCategories Figure 3: Accuracy per classi\ufb01cation category of the WebC-T5-3B model on the development dataset.\nNew Height Test (%) Dev (%)\ndescendants (%)\n25 3 87.7 90.3\n25 4 88.6 89.2\n50 3 88.4 90.0\n50 4 89.3 89.2\n300 5 87.8 88.8\n500 7 75.8 74.5\n(a)\nData SizeAccuracy\n55606570758085\n500 1000 1500 2000WebC-PaLM WebC-T5-3B\nT5-3B (full data \/ no pretraining) (b)\nFigure 4: a) Effect of snippet extraction parameters on WebC-T5-3B. Increases above 50% in new descendants\nand height of 4. Large increases in both parameters lead to large snippets and decrease in accuracy. b) Accu-\nracy over training data size. Using only 1000 labeled examples (4.4% of all training dataset), WebC-T5-3B\noutperforms T5-3B (full data without pretraining) which is trained on allavailable labeled data (approximately\n30k examples), and outperforms WebC-PaLM-8B which is an order of magnitude larger.\n8.2 S EMANTIC CLASSIFICATION TASK RESULTS\nTo evaluate the Semantic Classi\ufb01cation task, we compare the T5 encoder-decoder architecture\u2019s\nthree size variants (WebC-T5-base, WebC-T5-large, and WebC-T5-3B) \ufb01ne-tuned on 22K real,\nhuman-labeled training websites. We compare with a \ufb01ne-tuned encoder only architectures\n(WebC-*BERT*), three \ufb01ne-tuned decoder-only architectures (WebC-LaMDA and PaLM), and both\nencoder-decoder and decoder-only models trained on human labeled websites from scratch. Results\nare presented in Table-2, where we \ufb01nd that all WebC-LLMs perform well and signi\ufb01cantly better\nthan the same architectures without pretraining.\nAccuracy per category. In Figure 3, we present accuracy distribution of the WebC-T5-3B model\non the development dataset. The \ufb01ne-tuned encoder-decoder model performs strongly on a majority\nof the categories (Figure 3), even on those with very few samples. For instance, the model is 100%\naccurate on password newwhich has only 56 training examples, because the class is unambiguous.\nOn the other hand, unsurprisingly, the performance drops when the category is ambiguous, such as\nin the email category which is frequently mistaken as username .\nSnippet generation ablation. Two hyper-parameters govern snippet generation: percentage of\nnew descendants and height of the new root. While small variations of both parameters do not\nchange the performance, increasing both degrades the performance signi\ufb01cantly (Table 4a). With\nnew descendants up to 500% and height up to 7, the performance drops by more than 15%. Note\nthat snippet generation returns the full-page HTML when both parameters increase inde\ufb01nitely.\nData size impact. When varying the \ufb01ne-tuning training data sizes (1, 5, 10, 20, or 50 samples per\nclass) in Figure 4b, WebC-T5-3B slightly outperforms WebC-PaLM-8B which is an order of mag-\nnitude larger. Compared to T5-3B that is trained on all available HTML data without pretraining,\nWebC-T5-3B achieves better performance while using only 3.4% of labeled data (1000 samples),\n8\nTest Dev\nModel Name Accuracy (%) BLEU ROUGE-1 Accuracy (%) BLEU ROUGE-1\nWebD-T5-large 83.2 90.2 90.5 84.3 91.7 91.5\nWebD-LaMDA-1B 83.3 87.5 90.2 84.3 88.6 91.2\nWebD-T5-3B 84 90.8 90.9 85.2 92.1 91.9\nClosest Description 57.4 24.4 59.2 60.8 23.9 62.1\nTable 3: Description generation accuracy of LLMs.\nthus highlighting the bene\ufb01t of using standard off-the-shelf pretrained LLMs for HTML understand-\ning.\n8.3 D ESCRIPTION GENERATION TASK RESULTS\nForDescription Generation we split the CommonCrawl dataset based on URL top-level domains to\ntest LLMs\u2019 capabilities to generalize to unseen HTML. We \ufb01ne-tune encoder-decoder architectures\n(WebD\u2013T5*) and decoder-only models (WebD\u2013LaMDA*), with results presented in Table 3. We\nalso evaluate a strong heuristic baseline which simply \ufb01nds the description closest to the salient\nelement in the HTML text (Closest Description).\nAccuracy and Similarity Performance We show results of our evaluations in Table 3. All models\nachieve high scores across all metrics, achieving \u001984% on the accuracy in terms of exact match and\na higher non-exact match score based on BLEU and ROUGE-1 ( \u001991%). This difference indicates\nthat the models are capable of locating the descriptions, but not always generating the exact output.\n8.4 HTML U NDERSTANDING LLM SPERFORMANCE ANALYSIS ACROSS TASKS\nWe now analyze our results in aggregate to derive our main conclusions.\n8.4.1 P RETRAINING EFFECT : PRETRAINING ON LARGE TEXT CORPORA MATTERS\nFine-tuned pretrained LLMs outperform LLMs trained on HTML-only data, improving the perfor-\nmance by more than 34.1% on the Autonomous Web Navigation (Table 2b), and 10% to 12.7% on\ntheSemantic Classi\ufb01cation task (Table 2).\nSince Autonomous Web Navigation is the most dif\ufb01cult task, the improved performance is an en-\ncouraging evidence of the value of LLMs in HTML understanding tasks. Speci\ufb01cally, we observe\nthat LLMs without pretraining are comparable to \ufb01ne-tuned pretrained models only on websites that\nrequire simple text matching. In contrast, for websites such as click checkboxes , text matching is\nharder and we \ufb01nd that pretraining is key to good performance. We also found that without pretrain-\ning, model outputs were frequently in an incorrect format such as invalid dictionaries or invalid refs\nwith non-integer values. This suggests that the large corpora used for pretraining helps models to\nlearn general HTML structure.\n8.4.2 A RCHITECTURE EFFECT : T5- BASED MODELS PERFORM BESTACROSS ALLTASKS\nEncoder-decoder T5 based models perform better across all three tasks. On the Autonomous Web\nNavigation task, encoder-decoder (WebN-T5) architectures are better or comparable to WebN-\nLaMDA-1B (Figure 2a). On the Semantic Classi\ufb01cation , the smallest encoder-decoder model\n(WebC-T5-base) performs comparably to much larger decoder-only models (WebC-LaMDA-1B or\nWebC-PaLM-8B) and the largest encoder-only model (WebC-BERT-large) which has 85M more pa-\nrameters (Table 2). We also observe that decoder-only PaLM-8B performs worse than much-smaller\nencoder-decoder T5-large when trained only on HTML data. Finally, on the Description Generation\nencoder-decoder architecture has higher BLEU score.\nOne possible explanation for the strong performance of T5-based moels is the encoder-decoder\narchitecture of these models. Namely, T5 models utilize an encoder with a bidirectional attention\nmechanism, not present in the LaMDA and PaLM decoders. The bidirectional attention mechanism\ncan process HTML pages from both ends, potentially overcoming the loss of information when\ntree-structured HTML pages are converted into a \ufb01xed linear text sequences.\n9\n8.4.3 M ODEL SIZEEFFECT : SIZE(SUB-LINEARLY ) M ATTERS\nAcross the tasks it appears that the architecture plays an important role in the model performance.\nModel size and performance are also positively correlated, although they reach diminishing returns.\nFor instance, the model performance is roughly O(log log n)with respect to model size on Seman-\ntic Classi\ufb01cation (Figure 4b in Appendix). On the Autonomous Web Navigation task, performance\ngrows slowly with the model size (Table 8), while on the Description Generation it plateaus (Ta-\nble 3).\n8.5 D ISCUSSION\nBi-directional attention vs training corpora: Pretraining on large corpora matters, yielding \u00144.5x\nperformance improvements. Larger models tend to be better and we credit the bidirectional attention\nfor T5\u2019s best overall performance across the tasks. PaLM and LaMDA include HTML and other\ncode in their pretraining corpora, while BERT and T5 architectures did not, showing that pretraining\non HTML is not necessary for strong performance when \ufb01ne-tuned for HTML understanding. This\nstrengthens the hypothesis behind the role of the bidirectional attention, and opens up the possibility\nto further improve the performance of T5 architectures by pretraining them on corpora with HTML.\nPractical impact on labeling: When available, the pretrained LLMs need very little new expert\ndata (200x and 30x reduction on the web navigation and classi\ufb01cation tasks, respectively). This has\na big potential impact on practical applications, reducing the data collection time and cost by orders\nof magnitude.\nBigger is not always better: When choosing the model size, the expected performance gains (sub-\nlinear at best and asymptotic at worst) should be considered alongside the model\u2019s training and\ninference time and cost. For instance, on the classi\ufb01cation task, the largest model WebC-PaLM-62B\ntakes several days to \ufb01ne-tune, and evaluates at 30 Hz, while WebC-T5-large \ufb01ne-tunes in several\nhours and evaluates at 700 Hz \u2013 an order of magnitude more expensive for a single percent uplift in\naccuracy. BERT models on the other hand train in minutes. If the application does not require high\nprecision, these might be a good choice.\nContext window is a bottleneck: The major bottleneck for the HTML understanding tasks seems to\nbe the context window length that the current LLMs support, even with models that accept 1000+ to-\nkens. It remains prohibitive to evaluate web navigation tasks on real websites that are orders of mag-\nnitude larger than pages in MiniWob. Similarly, we observed that increasing the snippet size leads\nto major performance degradation. This makes HTML understanding an interesting benchmark for\nfuture LLM development. For instance, new methods may need to be developed to compress the\nstate representation of web content for use in LLM context windows.\n9 C ONCLUSION\nWe presented canonical tasks and \ufb01ne-tuned LLMs for HTML understanding. The comprehensive\nevaluations and analyses over a range of architectures, dataset sizes, and baselines yields practical\n\ufb01ndings and highlights current limitations of these models. We \ufb01nd that a) pretraining is critical for\nthe performance and can reduce labeled data requirements, improving sample ef\ufb01ciency up to 200x;\nb) model architecture is the second-most important factor, and T5 models with bidirectional attention\nand encoder-decoder architecture perform the best across the board; c) given a choice, model size\nshould be evaluated in the context of the model\u2019s training and inference performance, as the model\nsize sub-linearly correlates with its performance. Finally, the proposed HTML understanding tasks\nhighlight the relatively short context window that limits current LLMs, suggesting possibilities for\nfuture research that incorporate or eliminate this constraint.\nREFERENCES\nLeonard Adolphs, Benjamin Boerschinger, Christian Buck, Michelle Chen Huebscher, Massimil-\niano Ciaramita, Lasse Espeholt, Thomas Hofmann, and Yannic Kilcher. Boosting search engines\nwith interactive agents. arXiv preprint arXiv:2109.00527 , 2021.\n10\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke\nZettlemoyer. Htlm: Hyper-text pre-training and prompting of language models. arXiv preprint\narXiv:2107.06955 , 2021.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691 , 2022.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732 , 2021.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\nnities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems , 33:1877\u20131901, 2020.\nAndrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A Plummer.\nInteractive mobile app navigation with uncertain or under-speci\ufb01ed natural language commands.\narXiv preprint arXiv:2202.02312 , 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\nOscar Diaz, Itziar Otaduy, and Gorka Puente. User-driven automation of web form \ufb01lling. In\nInternational Conference on Web Engineering , pp. 171\u2013185. Springer, 2013.\nIzzeddin Gur, Natasha Jaques, Yingjie Miao, Jongwook Choi, Manoj Tiwari, Honglak Lee, and\nAleksandra Faust. Environment generation for zero-shot compositional reinforcement learning.\nAdvances in Neural Information Processing Systems , 34:4157\u20134169, 2021.\nZecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu, Nevan Wichers, Gabriel Schu-\nbiner, Ruby Lee, and Jindong Chen. Actionbert: Leveraging user actions for semantic under-\nstanding of user interfaces. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence ,\nvolume 35, pp. 5931\u20135938, 2021.\nPeter C Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair\nMuldal, Josh Abramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. A data-driven\napproach for learning to control computers. In International Conference on Machine Learning ,\npp. 9466\u20139482. PMLR, 2022.\nSheng Jia, Jamie Ryan Kiros, and Jimmy Ba. DOM-q-NET: Grounded RL on structured lan-\nguage. In International Conference on Learning Representations , 2019. URL https:\/\/\nopenreview.net\/forum?id=HJgd1nAqFX .\nChuck Jorgensen and Kim Binsted. Web browser control using emg based sub vocal speech recog-\nnition. In Proceedings of the 38th Annual Hawaii International Conference on System Sciences ,\npp. 294c\u2013294c. IEEE, 2005.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361 , 2020.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. Albert: A lite bert for self-supervised learning of language representations. In International\nConference on Learning Representations , 2020.\n11\nChenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. Structurallm:\nStructural pre-training for form understanding. arXiv preprint arXiv:2105.11210 , 2021a.\nJunlong Li, Yiheng Xu, Lei Cui, and Furu Wei. Markuplm: Pre-training of text and markup language\nfor visually-rich document understanding. arXiv preprint arXiv:2110.08518 , 2021b.\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement\nlearning on web interfaces using work\ufb02ow-guided exploration. arXiv preprint arXiv:1802.08802 ,\n2018.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. arXiv preprint arXiv:2103.05247 , 2021.\nSahisnu Mazumder and Oriana Riva. Flin: A \ufb02exible natural language interface for web navigation.\narXiv preprint arXiv:2010.12844 , 2020.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.\nRodrigo Nogueira and Kyunghyun Cho. End-to-end goal-driven web navigation. Advances in neural\ninformation processing systems , 29, 2016.\nChristopher Olston, Marc Najork, et al. Web crawling. Foundations and Trends\u00ae in Information\nRetrieval , 4(3):175\u2013246, 2010.\nPanupong Pasupat, Tian-Shun Jiang, Evan Zheran Liu, Kelvin Guu, and Percy Liang. Mapping\nnatural language commands to web elements. arXiv preprint arXiv:1808.09132 , 2018.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uni\ufb01ed text-to-text\ntransformer. J. Mach. Learn. Res. , 21(140):1\u201367, 2020.\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An\nopen-domain platform for web-based agents. In International Conference on Machine Learning ,\npp. 3135\u20133144. PMLR, 2017.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobile-\nBERT: a compact task-agnostic BERT for resource-limited devices. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics . Association for Computational\nLinguistics, 2020.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven\nZheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin,\nJames Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi\nZhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson,\nAlejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,\nMatthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil,\nBlaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. Lamda: Language\nmodels for dialog applications. CoRR , 2022.\nDaniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali\nAhmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: a reinforcement learn-\ning platform for android. arXiv preprint arXiv:2105.13231 , 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems , 30, 2017.\n12\ne xpand \no ne l e v el \nup s ali en t \nel emen t s s nip pe t \ng ener a t i o n <html> \n   <body> \n      <form class= \"login-form\" >\n         <div> \n            <label class= \"form-label\" for= \u201duName\u201d >\n               Enter Email Address \n            <\/label> \n      <label class= \"form-label\" for= \u201dpass\u201d >\n               Enter Password: \n            <\/label> \n         <\/div> \n         <div> \n  <input type= \"email\"  id=\"uName\u201d >\n            <input type= \"password\"  id=\"pass\" >\n            <span class= \"hidden\" >\n               Please enter your password. \n            <\/span> \n         <\/div> \n         <button type= \"submit\" >Sign In <\/button> \n       <\/form> \n   <\/body> \n<\/html> HTML \n<input  name= \"uName\" >\n<input  name= \"pass\" >\n<button  type= \"submit\" ><input type= \"email\"  id=\"uName\u201d >if e xpand ab l e : \ne xpand \n<div> \n  <input type= \"email\"  id=\"uName\u201d >\n  <input type= \"password\"  id=\"pass\" >\n  <span class= \"hidden\" >\n     Please enter your password. \n  <\/span> \n<\/div> o t her wis e \no u t p u t <input type= \"email\"            \nid=\"uName\u201d target >\nFigure 5: High-level overview of our pre-processing pipeline for generating snippets from a full HTML web-\npage. Given the page, we detect salient elements and for each one of them we extract snippets by recursively\nmoving up in the HTML tree until a validation heuristic fails.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R \u00b4emi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface\u2019s\ntransformers: State-of-the-art natural language processing. CoRR , abs\/1910.03771, 2019. URL\nhttp:\/\/arxiv.org\/abs\/1910.03771 .\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv\npreprint arXiv:2010.11934 , 2020.\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-\nworld web interaction with grounded language agents. arXiv preprint arXiv:2207.01206 , 2022.\nA A PPENDIX\nA.1 D ATASET DETAIL\nExamining the description distribution, we found the original 400Kdataset to be very skewed; only\n20 descriptions (such as Email andPassword ) were covering 50% of the dataset. We sub-sampled the\ndataset so that each unique description has at most 10 data points. We also found that for attributes\nare almost always de\ufb01ned for HTML label s. This could cause a model to over\ufb01t and just \ufb01nd the\nlabel element in the HTML and ignore everything else. To avoid this sort of \u2018cheating\u2019 we replace\nthe tags of HTML label s by randomly sampling from fdiv, span, a, label g. These tags\nare also frequently used to inject text in HTML but they are very rarely used with for attributes.\nFinally, we removed examples where there are only a single text in the HTML since models can\ntrivially generate descriptions by \ufb01nding the only text in the HTML, which biases model weights\nand evaluation metrics. After this \ufb01nal step, we have a total of 85Klabeled examples.\nA.1.1 S NIPPET GENERATION\nIn Figure 5, we give a high-level overview of our snippet generation procedure.\nA.2 A DDITIONAL RESULTS\nA.2.1 S EMANTIC CLASSIFICATION\nError Analysis. We manually examined 50 errors of T5-3B model over the development set (Ta-\nble 4) and assigned them into one of the 9 error types that we devised. We found that 32% of the\nerrors are due to lack of information in the HTML snippets, which is mainly the result of lost in-\nformation during snippet extraction process. Annotation errors or email\/username ambiguity make\nup 30% of the errors. These can\u2019t be improved without revising the annotated data or adding extra\ninformation to resolve the ambiguity. We also found that the model sometimes picks a more general\ncategory, or a nearby text misleads the model; the latter usually happens when the HTML snippet is\nlong where majority of the elements are noise.\n13\nError Type Percentage of Examples\nNot enough information in the HTML snippet 30\nIncorrect annotation (ex: \u201dunknown role\u201d instead of \u201dorganization\u201d) 12\nAnnotation tool translates user selection incorrectly 8\nEmail\/Username ambiguity 10\nMore general category (ex: \u201dheader\u201d instead of \u201dcart header\u201d) 8\nImmediate neighboring text misleads 8\nIncorrect date formatting (ex: \u201dmm\u201d instead of \u201dmmm\u201d) 4\nNo information in the HTML snippet 2\nOthers 18\nTable 4: Types of errors over 50 manually examined examples. 32% of errors are due to lack of information\nin HTML snippets, 30% of errors are related to annotations or can\u2019t be improved due to ambiguity (email\/user-\nname), and the remaining errors are incorrect predictions by the model.\nFew-Shot Prompting In Table 5, we present few-shot prompting performance of a 540B PaLM\nmodel. We probe the model using a prompt template <html> Role: <category> with 1 ex-\nample per category and generate categories using greedy-decoding. In our preliminary experiments,\nwe found that few-shot prompting achieves only 45.6 accuracy, much lower than a model \ufb01ne-tuned\non the same data (Figure 6). We found two common problems \u2013 the model is not able to canonicalize\npredictions into categories and many of the examples are dropped due to context length.\nModel Name Test Dev\nPaLM-540B 64.2 60.3\n- w\/o Example Cleaning 57.9 57.2\n- w\/o Category Rewriting 52.1 50.7\n- w\/o Dictionary Mapping 45.6 45.1\nTable 5: Few-shot prompting performance with differ-\nent pre- and post-processing steps.We developed post-processing methods to al-\nleviate the canonicalization problem and pre-\nprocessing methods to reduce lengths of ex-\namples. Adding a dictionary-based mapping\non predictions \u2013 a manually curated paraphrase\ndictionary \u2013 improves the performance to 52.1.\nWe also tried rewriting predictions by chang-\ning the order of tokens around \u201d \u201d such as\nname \ufb01rstto\ufb01rst name which further improved\nthe performance to 57.9. Finally, we cleaned\nexamples in the prompt by removing certain el-\nements such as \u201dsvg\u201d, \u201dpath\u201d, \u201dimg\u201d , and \u201diframe\u201d and also removing class attribute from every\nelement; this pre-processing step gives 64.2.\nFigure 6: Performance comparison w.r.t. increasing model size. As the model size increases, we\nobserve an increase in overall accuracy with PaLM-62B model achieving the highest accuracy while\nbeing 7x larger than PaLM-8B.\n14\nA.3 S AMPLE EPISODES FROM MINIWOB\nSee Table 6 for an example episode of web navigation inferred by a \ufb01ne-tuned LLM.\nA.4 D ETAILED MINIWOB R ESULTS\nSee Table 7 for detailed performance of various models on MiniWob.\nA.5 R ESOURCE REQUIREMENTS\nSee Table 8.\nA.6 S TRUCTURE DEPENDENCE ABLATION STUDY\nWe conducted an ablation study to examine the sensitivity of model performance to preserving\nstructural information. To do so, we evaluate the model\u2019s performance on HTML input with criti-\ncal structure components removed. We kept the order of elements and their attributes \ufb01xed while\ncorrupting the nesting structure by removing closing tags.\nRemoving closing tags corresponds to a valid traversal (BFS) and keeps the order of elements the\nsame as the text based input.\nAs a simple example:\n<div id=\"form\"><div><input id=\"username\"><\/div><\/div>\nwould be converted into:\n<div id=\"form\"><div><input id=\"username\">\nWe evaluated the trained WebN-T5-3B model on the same set of synthetic websites from the\nMiniWoB benchmark with this aspect of structure removed from the HTML pages. WebN-T5-\n3B achieves a 45.4% success rate, 6% lower than before, suggesting that WebN-T5-3B is at least\npartially dependent on the DOM topology.\nA.7 T ASK-SPECIFIC MODELS\nAn alternative to LLMs is to adapt bespoke task-speci\ufb01c architectures tailored towards processing\nof structured documents and HTML (Li et al. (2021b;a)).\nStructuralLM (Li et al. (2021a)) is an approach speci\ufb01cally tailored for document understanding\n(i.e., combinations of images and text), and thus makes several simplifying assumptions for its model\nthat limit its applicability to HTML understanding (i.e., trees of elements with a richer structure and\nfunctionality). It is trained only on the textual content of a document - the markup information is\nignored. For example, any input \ufb01eld or dropdown in a document would be missing from the model\ninputs. All of the tasks we study require knowledge of this information. For example, in autonomous\nnavigation the model needs to interact with input elements (e.g. text, checkboxes, dropdowns) such\nas username and password in the login-user task in MiniWoB. Typically, a \u201ctype\u201d action with a\nreference to an element and a text argument is generated by the model. Without knowing which\ninput elements are available in the page, it is impossible to generate a reference to any input element.\nWhile MarkupLM (Li et al. (2021b)) is better tailored for understanding HTML pages, it has similar\ndrawbacks as StructuralLM in that it focuses solely on text and structure of text while ignoring\neverything else in the markup. To illustrate our point better, we used the open source implementation\nof MarkupLM from the HuggingFace library (Wolf et al. (2019)) to process the sample HTML\nsnippet in Figure-1(b). The MarkupLM ignores all input elements, both username and password,\nand generates <s>Email AddressEnter Password:Please enter your password. <\/s>which is the\ntext input to the MarkupLM Transformer. Classifying this text as username or password is not\npossible without the additional context on which input element is the salient element (in this context\nit is the username). See below for the code to reproduce our result.\n15\nfrom transformers import MarkupLMProcessor\nprocessor = MarkupLMProcessor.from_pretrained(f\"microsoft\/markuplm-base\")\nsnippet = \u2019\u2019\u2019<div><label class=\"form-label\" for=\"uName\">Email Address\n<\/label><label class=\"form-label\" for=\"pass\">Enter Password:\n<\/label><\/div><div><input type=\"email\" id=\"uName\" target><input\ntype=\"password\" id=\"pass\"><span class=\"hidden\">Please enter your password.\n<\/span><\/div>\u2019\u2019\u2019\nencoding = processor(snippet)\nprint(processor.batch_decode(encoding[\"input_ids\"]))\nMarkupLM is also evaluated on NLP-like tasks such as QA or entity classi\ufb01cation where understand-\ning page content is paramount, whereas we focus on HTML understanding tasks such as autonomous\nnavigation where both content and the page\u2019s layout structure need to be understood.\nWe perform a quantitative evaluation of MarkupLM on our tasks to understand how signi\ufb01cant\nthese limitations are. We \ufb01ne-tune the MarkupLM-base model on the semantic classi\ufb01cation task,\nusing the same setup as other WebC models but with the suggested hyperparameters from (Li et al.\n(2021b)). We use the MarkupLM implementation from the HuggingFace library (Wolf et al. (2019)).\nOn development and test sets, MarkupLM-base achieves 65% and 66% accuracy, respectively. These\nresults are more than 16% lower compared to similar size WebC-BERT-base results that we report\nin our work. This suggests that although domain speci\ufb01c models may be suitable for processing\nHTML for NLP tasks, the generality, \ufb02exibility, and sample ef\ufb01ciency LLMs provide advantages\nfor autonomous navigation tasks.\n16\nTable 6: A sample web page and corresponding episode using the T5-3B model. At each time step,\nprevious actions, instruction, and HTML are concatenated into a single HTML text. Note that at the\nbeginning of episode, there is no past actions and we simply concatenate instruction and HTML.\nAction is generated as a sequence of tokens which is later parsed into a dictionary. The refin the\naction points to an element that has a refattribute with the same value. For instance, at the beginning\nof episode, ref: 6 corresponds to an input with ref=6 . At the end of the episode, the model clicks on\nthe submit button and the episode terminates.\nWeb page\nHTML Text Action Text\nfaction: click, ref: 6 g\nfaction: click, ref: 10 g\nfaction: click, ref: 12 g\nfaction: click, ref: 14 g 17\nfaction: click, ref: 16 g\nfaction: click, ref: 17 g\n18\nTable 7: Success rate comparison of various models in MiniWoB tasks. Baseline results are borrowed from\n(Humphreys et al., 2022). Note that these are normalized between 0 and 1.\nTASK Human CC-Net CC-Net World Work\ufb02ow Learning DOM-Q-Net Work\ufb02ow Learning Aggregated Aggregated\nWebN-T5-3B WebN-T5-3B (SL & RL) (SL) of guided to (RL) guided to SOTA SOTA\n(no history) bits exploration navigate exploration navigate (SL & RL) (Augmented)\n(SL & RL) (SL & RL) the web (Augmented) the web\n(RL) (Augmented)\nbisect-angle 0.92 n\/a n\/a 0.97 0.29 0.8 n\/a n\/a n\/a n\/a n\/a 0.8 0.8\nbook-\ufb02ight 0.87 0 0 0.87 0 0 0 n\/a n\/a 0 1 0 1\nchase-circle 0.82 n\/a n\/a 0.93 0.8 1 n\/a n\/a n\/a n\/a n\/a 1 1\nchoose-date-easy 0.99 0.03 0.05 0.99 0.42 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nchoose-date-medium 0.98 0 0 0.99 0.26 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nchoose-date 0.97 0 0 0.97 0.12 0 0 n\/a 1 0 n\/a 1 1\nchoose-list 0.98 0.26 0.14 0.99 0.19 0.25 0.16 0.26 n\/a 0.16 0.26 0.26 0.26\ncircle-center 0.96 n\/a n\/a 0.97 0.36 0.98 n\/a n\/a n\/a n\/a n\/a 0.98 0.98\nclick-button-sequence 0.94 1 1 1 0.47 0.22 0.99 n\/a 1 1 n\/a 1 1\nclick-button 0.98 1 0.96 1 0.78 0.62 1 1 1 1 1 1 1\nclick-checkboxes-large 0.87 0.22 0 0.71 0 n\/a 0.68 n\/a n\/a 0.84 n\/a 0.68 0.84\nclick-checkboxes-soft 0.73 0.54 0.43 0.95 0.04 n\/a 0.51 n\/a n\/a 0.94 n\/a 0.51 0.94\nclick-checkboxes-transfer 0.98 0.63 0.34 0.99 0.36 n\/a 0.64 n\/a n\/a 0.64 n\/a 0.64 0.64\nclick-checkboxes 0.97 0.96 0.84 0.98 0.32 0.48 0.98 n\/a 1 1 n\/a 1 1\nclick-collapsible-2 0.97 0 0.01 0.98 0.17 0.11 0.65 n\/a n\/a 0.99 n\/a 0.65 0.99\nclick-collapsible 0.99 0 0.01 1 0.81 0.98 1 1 n\/a 1 1 1 1\nclick-color 0.97 0.27 0.23 1 0.82 0.23 1 n\/a n\/a 1 n\/a 1 1\nclick-dialog-2 0.99 0.24 0.35 1 0.88 0.53 1 n\/a n\/a 1 n\/a 1 1\nclick-dialog 1 1 1 1 0.95 1 1 1 1 1 1 1 1\nclick-link 0.99 1 0.96 0.99 0.59 0.31 1 1 1 1 1 1 1\nclick-menu-2 0.98 n\/a n\/a 0.83 0.52 0.16 n\/a n\/a n\/a n\/a n\/a 0.16 0.16\nclick-menu 0.97 0.37 0.38 0.94 0.22 0.13 n\/a n\/a n\/a n\/a n\/a 0.13 0.13\nclick-option 0.99 0.87 0.78 0.99 0.21 0.28 1 n\/a 1 1 n\/a 1 1\nclick-pie 0.98 0.51 0.14 0.97 0.15 0.15 0.32 1 n\/a 0.32 1 1 1\nclick-scroll-list 0.91 0 0 0.6 0.01 0.07 n\/a n\/a n\/a n\/a n\/a 0.07 0.07\nclick-shades 0.91 0 0 1 0.04 0.27 0.22 n\/a n\/a 0.99 n\/a 0.27 0.99\nclick-shape 0.88 0.53 0.54 0.95 0.11 0.11 0.64 n\/a n\/a 0.64 n\/a 0.64 0.64\nclick-tab-2-easy 0.99 n\/a n\/a 0.99 0.61 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nclick-tab-2-hard 0.96 0.12 0.13 0.98 0.19 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nclick-tab-2-medium 0.97 n\/a n\/a 0.99 0.54 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nclick-tab-2 0.97 0.18 0.09 0.98 0.27 0.08 0.64 n\/a 1 0.98 n\/a 1 1\nclick-tab 0.99 0.74 1 1 0.95 0.97 0.55 1 1 1 1 1 1\nclick-test-2 0.99 1 1 1 0.95 0.83 1 n\/a 1 1 n\/a 1 1\nclick-test-transfer 0.99 n\/a n\/a 1 0.94 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nclick-test 1 1 1 1 1 1 1 n\/a 1 1 n\/a 1 1\nclick-widget 0.83 1 0.97 1 0.56 0.34 0.93 n\/a 1 0.93 n\/a 1 1\ncopy-paste-2 0.94 n\/a n\/a 0.63 0.01 0 n\/a n\/a n\/a n\/a n\/a 0 0\ncopy-paste 0.94 n\/a n\/a 0.79 0.04 0 n\/a n\/a n\/a n\/a n\/a 0 0\ncount-shape 0.82 0.41 0.43 0.85 0.21 0.18 0.59 n\/a n\/a 0.76 n\/a 0.59 0.76\ncount-sides 0.98 n\/a n\/a 1 0.74 0.3 n\/a n\/a n\/a n\/a n\/a 0.3 0.3\ndrag-box 0.99 n\/a n\/a 1 0.61 0.31 n\/a n\/a n\/a n\/a n\/a 0.31 0.31\ndrag-cube 0.99 n\/a n\/a 0.79 0.23 0.18 n\/a n\/a n\/a n\/a n\/a 0.18 0.18\ndrag-item 0.98 n\/a n\/a 1 0.61 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\ndrag-items-grid 0.87 n\/a n\/a 0.98 0.05 0.01 n\/a n\/a n\/a n\/a n\/a 0.01 0.01\ndrag-items 0.93 n\/a n\/a 0.99 0.13 0.41 n\/a n\/a n\/a n\/a n\/a 0.41 0.41\ndrag-shapes 0.96 n\/a n\/a 0.99 0.26 0.92 n\/a n\/a n\/a n\/a n\/a 0.92 0.92\ndrag-sort-numbers 0.92 n\/a n\/a 0.97 0.11 0.66 n\/a n\/a n\/a n\/a n\/a 0.66 0.66\nemail-inbox-delete 0.99 n\/a n\/a 1 0.22 n\/a n\/a n\/a 1 n\/a n\/a 1 1\nemail-inbox-forward-nl-turk 0.88 0.33 0.09 1 0 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nemail-inbox-forward-nl 0.91 0.60 0.09 1 0 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nemail-inbox-forward 0.96 n\/a n\/a 1 0.01 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nemail-inbox-important 0.99 n\/a n\/a 1 0.3 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nemail-inbox-nl-turk 0.93 0.23 0.26 1 0.05 n\/a 0.77 n\/a n\/a 0.93 n\/a 0.77 0.93\nemail-inbox-noscroll 0.96 n\/a n\/a 1 0.13 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nemail-inbox-reply 0.91 n\/a n\/a 1 0 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nemail-inbox-star-reply 0.95 n\/a n\/a 1 0.11 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nemail-inbox 0.96 0.38 0.21 1 0.09 0.03 0.43 n\/a 0.54 0.99 n\/a 0.54 0.99\nenter-date 0.97 0 0 1 0.02 0.61 0 1 n\/a 0.96 1 1 1\nenter-password 0.96 0.97 0.92 1 0.02 0 0.99 1 1 1 1 1 1\nenter-text-2 0.91 n\/a n\/a 0.98 0.04 0 n\/a n\/a n\/a n\/a n\/a 0 0\nenter-text-dynamic 0.97 0.98 0.92 1 0.39 1 1 1 1 1 1 1 1\nenter-text 0.98 0.89 0.99 1 0.35 0 1 n\/a 1 1 n\/a 1 1\nenter-time 0.98 0 0.01 0.97 0.04 0.08 0.52 n\/a n\/a 0.9 n\/a 0.52 0.9\n\ufb01nd-midpoint 0.94 n\/a n\/a 0.97 0.35 0.31 n\/a n\/a n\/a n\/a n\/a 0.31 0.31\n\ufb01nd-word 0.96 n\/a n\/a 0.88 0.05 0 n\/a n\/a n\/a n\/a n\/a 0 0\nfocus-text-2 0.99 1 1 1 0.96 0.83 1 n\/a 1 1 n\/a 1 1\nfocus-text 1 1 1 1 0.99 0.95 1 n\/a 1 1 n\/a 1 1\ngrid-coordinate 0.87 0.49 0.42 1 0.66 0.26 1 n\/a n\/a 1 n\/a 1 1\nguess-number 0.99 0 0 1 0.21 0.2 0 n\/a n\/a 0 n\/a 0.2 0.2\nhighlight-text-2 0.97 n\/a n\/a 1 0.4 0.13 n\/a n\/a n\/a n\/a n\/a 0.13 0.13\nhighlight-text 0.97 n\/a n\/a 1 0.51 0.9 n\/a n\/a n\/a n\/a n\/a 0.9 0.9\nidentify-shape 0.98 0.88 0.89 1 0.68 0.36 0.9 n\/a n\/a 1 n\/a 0.9 1\nlogin-user-popup 0.94 0.72 0.40 1 0.02 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nlogin-user 0.96 0.82 0.64 1 0 0 0.99 1 1 1 1 1 1\nmoving-items 0.18 n\/a n\/a 0.88 0.13 0.78 n\/a n\/a n\/a n\/a n\/a 0.78 0.78\nmulti-layouts 0.95 0.83 0.48 1 0 n\/a 0.99 n\/a n\/a 1 n\/a 0.99 1\nmulti-orderings 0.96 0.88 0.64 1 0 n\/a 0.05 n\/a n\/a 1 n\/a 0.05 1\nnavigate-tree 0.98 0.91 0.99 0.99 0.32 0.2 0.99 1 1 0.99 1 1 1\nnumber-checkboxes 0.96 n\/a n\/a 0.99 0 0.16 n\/a n\/a n\/a n\/a n\/a 0.16 0.16\nread-table-2 0.95 n\/a n\/a 0.94 0 0 n\/a n\/a n\/a n\/a n\/a 0 0\nread-table 0.97 n\/a n\/a 0.97 0.01 0 n\/a n\/a n\/a n\/a n\/a 0 0\nresize-textarea 0.94 n\/a n\/a 1 0.27 0.11 n\/a n\/a n\/a n\/a n\/a 0.11 0.11\nright-angle 0.87 n\/a n\/a 0.98 0.26 0.38 n\/a n\/a n\/a n\/a n\/a 0.38 0.38\nscroll-text-2 0.97 n\/a n\/a 1 0.88 0.96 n\/a n\/a n\/a n\/a n\/a 0.96 0.96\nscroll-text 0.97 n\/a n\/a 0.96 0.04 0 n\/a n\/a n\/a n\/a n\/a 0 0\nsearch-engine 0.97 0.34 0.34 1 0.15 0 0.26 n\/a 1 0.99 n\/a 1 1\nsimon-says 0.62 n\/a n\/a 0 0.02 0.28 n\/a n\/a n\/a n\/a n\/a 0.28 0.28\nsimple-algebra 0.86 n\/a n\/a 0.75 0.03 0.04 n\/a n\/a n\/a n\/a n\/a 0.04 0.04\nsimple-arithmetic 0.96 n\/a n\/a 0.86 0.38 0.07 n\/a n\/a n\/a n\/a n\/a 0.07 0.07\nsocial-media-all 0.89 0 0 0.75 0 n\/a 0.01 n\/a n\/a 0.01 1 0.01 1\nsocial-media-some 0.91 0.02 0 0.85 0.01 n\/a 0.01 n\/a n\/a 0.42 n\/a 0.01 0.42\nsocial-media 0.96 0.21 0.24 0.9 0.03 0.23 0.39 n\/a 1 1 n\/a 1 1\nterminal 0.88 n\/a n\/a -0.01 0 0 n\/a n\/a n\/a n\/a n\/a 0 0\ntext-editor 0.88 n\/a n\/a 0.98 0.11 0.01 n\/a n\/a n\/a n\/a n\/a 0.01 0.01\ntext-transform 0.86 n\/a n\/a 0.6 0.19 0 n\/a n\/a n\/a n\/a n\/a 0 0\ntic-tac-toe 0.71 0.48 0.40 0.83 0.32 0.34 0.37 n\/a n\/a 0.47 n\/a 0.37 0.47\nunicode-test 0.99 n\/a n\/a 1 0.86 n\/a n\/a n\/a n\/a n\/a n\/a n\/a n\/a\nuse-autocomplete 0.98 0.22 0.15 1 0.07 0 0.78 n\/a n\/a 0.98 n\/a 0.78 0.98\nuse-colorwheel-2 0.94 n\/a n\/a 0.95 0.38 1 n\/a n\/a n\/a n\/a n\/a 1 1\nuse-colorwheel 0.9 n\/a n\/a 0.98 0.68 1 n\/a n\/a n\/a n\/a n\/a 1 1\nuse-slider-2 0.97 n\/a n\/a 0.95 0.03 0.15 n\/a n\/a n\/a n\/a n\/a 0.15 0.15\nuse-slider 0.98 n\/a n\/a 0.91 0.18 0.51 n\/a n\/a n\/a n\/a n\/a 0.51 0.51\nuse-spinner 0.98 0.07 0.05 1 0.47 0.17 0.04 n\/a n\/a 0.04 n\/a 0.17 0.17\nvisual-addition 0.97 n\/a n\/a 0.99 0.36 0.01 n\/a n\/a n\/a n\/a n\/a 0.01 0.01\n19\nTable 8: Resource requirements and running time of LLMs.\nModel Name Model Size TPU version Batch size Input sequence length Examples per sec (training) Examples per sec (inference)\nPaLM 62B TPU v4 8 1920 9.313 30.51\nPaLM 8B TPU v4 32 1920 64.4 184.3\nT5 3B TPU v4 128 512 163.8 734.5\nLaMDA 1B TPU v2 128 512 363.1 1416\n20","metadata":{"primary_category":"cs.LG","published":"20221008","title":"Understanding HTML with Large Language Models","updated":"20230519"}}
{"id":"1711.05101","source":"http:\/\/arxiv.org\/pdf\/1711.05101","text":"Published as a conference paper at ICLR 2019\nDECOUPLED WEIGHT DECAY REGULARIZATION\nIlya Loshchilov & Frank Hutter\nUniversity of Freiburg\nFreiburg, Germany,\nfilya,fhg@cs.uni-freiburg.de\nABSTRACT\nL2regularization and weight decay regularization are equivalent for standard\nstochastic gradient descent (when rescaled by the learning rate), but as we demon-\nstrate this is notthe case for adaptive gradient algorithms, such as Adam. While\ncommon implementations of these algorithms employ L 2regularization (often\ncalling it \u201cweight decay\u201d in what may be misleading due to the inequivalence we\nexpose), we propose a simple modi\ufb01cation to recover the original formulation of\nweight decay regularization by decoupling the weight decay from the optimization\nsteps taken w.r.t. the loss function. We provide empirical evidence that our pro-\nposed modi\ufb01cation (i) decouples the optimal choice of weight decay factor from\nthe setting of the learning rate for both standard SGD and Adam and (ii) substan-\ntially improves Adam\u2019s generalization performance, allowing it to compete with\nSGD with momentum on image classi\ufb01cation datasets (on which it was previously\ntypically outperformed by the latter). Our proposed decoupled weight decay has\nalready been adopted by many researchers, and the community has implemented\nit in TensorFlow and PyTorch; the complete source code for our experiments is\navailable at https:\/\/github.com\/loshchil\/AdamW-and-SGDW\n1 I NTRODUCTION\nAdaptive gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton,\n2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have become\na default method of choice for training feed-forward and recurrent neural networks (Xu et al., 2015;\nRadford et al., 2015). Nevertheless, state-of-the-art results for popular image classi\ufb01cation datasets,\nsuch as CIFAR-10 and CIFAR-100 Krizhevsky (2009), are still obtained by applying SGD with\nmomentum (Gastaldi, 2017; Cubuk et al., 2018). Furthermore, Wilson et al. (2017) suggested that\nadaptive gradient methods do not generalize as well as SGD with momentum when tested on a\ndiverse set of deep learning tasks, such as image classi\ufb01cation, character-level language modeling\nand constituency parsing. Different hypotheses about the origins of this worse generalization have\nbeen investigated, such as the presence of sharp local minima (Keskar et al., 2016; Dinh et al.,\n2017) and inherent problems of adaptive gradient methods (Wilson et al., 2017). In this paper, we\ninvestigate whether it is better to use L 2regularization or weight decay regularization to train deep\nneural networks with SGD and Adam. We show that a major factor of the poor generalization of the\nmost popular adaptive gradient method, Adam, is due to the fact that L 2regularization is not nearly\nas effective for it as for SGD. Speci\ufb01cally, our analysis of Adam leads to the following observations:\nL2regularization and weight decay are not identical. The two techniques can be made equiv-\nalent for SGD by a reparameterization of the weight decay factor based on the learning\nrate; however, as is often overlooked, this is not the case for Adam. In particular, when\ncombined with adaptive gradients, L 2regularization leads to weights with large historic\nparameter and\/or gradient amplitudes being regularized less than they would be when us-\ning weight decay.\nL2regularization is not effective in Adam. One possible explanation why Adam and other\nadaptive gradient methods might be outperformed by SGD with momentum is that common\ndeep learning libraries only implement L 2regularization, not the original weight decay.\nTherefore, on tasks\/datasets where the use of L 2regularization is bene\ufb01cial for SGD (e.g.,\n1arXiv:1711.05101v3  [cs.LG]  4 Jan 2019\nPublished as a conference paper at ICLR 2019\non many popular image classi\ufb01cation datasets), Adam leads to worse results than SGD with\nmomentum (for which L 2regularization behaves as expected).\nWeight decay is equally effective in both SGD and Adam. For SGD, it is equivalent to L 2\nregularization, while for Adam it is not.\nOptimal weight decay depends on the total number of batch passes\/weight updates. Our\nempirical analysis of SGD and Adam suggests that the larger the runtime\/number of batch\npasses to be performed, the smaller the optimal weight decay.\nAdam can substantially bene\ufb01t from a scheduled learning rate multiplier. The fact that Adam\nis an adaptive gradient algorithm and as such adapts the learning rate for each parameter\ndoes notrule out the possibility to substantially improve its performance by using a global\nlearning rate multiplier, scheduled, e.g., by cosine annealing.\nThe main contribution of this paper is to improve regularization in Adam by decoupling the weight\ndecay from the gradient-based update . In a comprehensive analysis, we show that Adam generalizes\nsubstantially better with decoupled weight decay than with L 2regularization, achieving 15% relative\nimprovement in test error (see Figures 2 and 3); this holds true for various image recognition datasets\n(CIFAR-10 and ImageNet32x32), training budgets (ranging from 100 to 1800 epochs), and learning\nrate schedules (\ufb01xed, drop-step, and cosine annealing; see Figure 1). We also demonstrate that our\ndecoupled weight decay renders the optimal settings of the learning rate and the weight decay factor\nmuch more independent, thereby easing hyperparameter optimization (see Figure 2).\nThe main motivation of this paper is to improve Adam to make it competitive w.r.t. SGD with\nmomentum even for those problems where it did not use to be competitive. We hope that as a result,\npractitioners do not need to switch between Adam and SGD anymore, which in turn should reduce\nthe common issue of selecting dataset\/task-speci\ufb01c training algorithms and their hyperparameters.\n2 D ECOUPLING THE WEIGHT DECAY FROM THE GRADIENT -BASED UPDATE\nIn the weight decay described by Hanson & Pratt (1988), the weights \u0012decay exponentially as\n\u0012t+1= (1\u0000\u0015)\u0012t\u0000\u000brft(\u0012t); (1)\nwhere\u0015de\ufb01nes the rate of the weight decay per step and rft(\u0012t)is thet-th batch gradient to be\nmultiplied by a learning rate \u000b. For standard SGD, it is equivalent to standard L 2regularization:\nProposition 1 (Weight decay = L 2reg for standard SGD) .Standard SGD with base learning rate \u000b\nexecutes the same steps on batch loss functions ft(\u0012)with weight decay \u0015(de\ufb01ned in Equation 1)\nas it executes without weight decay on freg\nt(\u0012) =ft(\u0012) +\u00150\n2k\u0012k2\n2, with\u00150=\u0015\n\u000b.\nThe proofs of this well-known fact, as well as our other propositions, are given in Appendix A.\nDue to this equivalence, L 2regularization is very frequently referred to as weight decay, including\nin popular deep learning libraries. However, as we will demonstrate later in this section, this equiva-\nlence does nothold for adaptive gradient methods. One fact that is often overlooked already for the\nsimple case of SGD is that in order for the equivalence to hold, the L 2regularizer\u00150has to be set to\n\u0015\n\u000b, i.e., if there is an overall best weight decay value \u0015, the best value of \u00150is tightly coupled with\nthe learning rate \u000b. In order to decouple the effects of these two hyperparameters, we advocate to\ndecouple the weight decay step as proposed by Hanson & Pratt (1988) (Equation 1).\nLooking \ufb01rst at the case of SGD, we propose to decay the weights simultaneously with the update\nof\u0012tbased on gradient information in Line 9 of Algorithm 1. This yields our proposed variant of\nSGD with momentum using decoupled weight decay ( SGDW ). This simple modi\ufb01cation explicitly\ndecouples\u0015and\u000b(although some problem-dependent implicit coupling may of course remain as\nfor any two hyperparameters). In order to account for a possible scheduling of both \u000band\u0015, we\nintroduce a scaling factor \u0011tdelivered by a user-de\ufb01ned procedure SetScheduleMultiplier (t).\nNow, let\u2019s turn to adaptive gradient algorithms like the popular optimizer Adam Kingma & Ba\n(2014), which scale gradients by their historic magnitudes. Intuitively, when Adam is run on a loss\nfunctionfplus L 2regularization, weights that tend to have large gradients in fdo not get regularized\nas much as they would with decoupled weight decay, since the gradient of the regularizer gets scaled\n2\nPublished as a conference paper at ICLR 2019\nAlgorithm 1 SGD with L 2regularization and SGD with decoupled weight decay (SGDW) , both\nwith momentum\n1:given initial learning rate \u000b2I R, momentum factor \f12I R, weight decay\/L 2regularization factor \u00152I R\n2:initialize time stept 0, parameter vector \u0012t=02I Rn, \ufb01rst moment vector mt=0 0, schedule\nmultiplier\u0011t=02I R\n3:repeat\n4:t t+ 1\n5:rft(\u0012t\u00001) SelectBatch (\u0012t\u00001) .select batch and return the corresponding gradient\n6: gt rft(\u0012t\u00001)+\u0015\u0012t\u00001\n7:\u0011t SetScheduleMultiplier (t) .can be \ufb01xed, decay, be used for warm restarts\n8: mt \f1mt\u00001+\u0011t\u000bgt\n9: \u0012t \u0012t\u00001\u0000mt\u0000\u0011t\u0015\u0012t\u00001\n10:until stopping criterion is met\n11:return optimized parameters \u0012t\nAlgorithm 2 Adam with L 2regularization and Adam with decoupled weight decay (AdamW)\n1:given\u000b= 0:001;\f1= 0:9;\f2= 0:999;\u000f= 10\u00008;\u00152I R\n2:initialize time stept 0, parameter vector \u0012t=02I Rn, \ufb01rst moment vector mt=0 0, second moment\nvector vt=0 0, schedule multiplier \u0011t=02I R\n3:repeat\n4:t t+ 1\n5:rft(\u0012t\u00001) SelectBatch (\u0012t\u00001) .select batch and return the corresponding gradient\n6: gt rft(\u0012t\u00001)+\u0015\u0012t\u00001\n7: mt \f1mt\u00001+ (1\u0000\f1)gt .here and below all operations are element-wise\n8: vt \f2vt\u00001+ (1\u0000\f2)g2\nt\n9: ^mt mt=(1\u0000\ft\n1) . \f 1is taken to the power of t\n10: ^vt vt=(1\u0000\ft\n2) . \f 2is taken to the power of t\n11:\u0011t SetScheduleMultiplier (t) .can be \ufb01xed, decay, or also be used for warm restarts\n12: \u0012t \u0012t\u00001\u0000\u0011t\u0010\n\u000b^mt=(p^vt+\u000f)+\u0015\u0012t\u00001\u0011\n13:until stopping criterion is met\n14:return optimized parameters \u0012t\nalong with the gradient of f. This leads to an inequivalence of L 2and decoupled weight decay\nregularization for adaptive gradient algorithms:\nProposition 2 (Weight decay6=L2reg for adaptive gradients) .LetOdenote an optimizer that has\niterates\u0012t+1 \u0012t\u0000\u000bMtrft(\u0012t)when run on batch loss function ft(\u0012)without weight decay,\nand\u0012t+1 (1\u0000\u0015)\u0012t\u0000\u000bMtrft(\u0012t)when run on ft(\u0012)with weight decay, respectively, with\nMt6=kI(wherek2R). Then, forOthere exists no L 2coef\ufb01cient\u00150such that running Oon batch\nlossfreg\nt(\u0012) =ft(\u0012)+\u00150\n2k\u0012k2\n2without weight decay is equivalent to running Oonft(\u0012)with decay\n\u00152R+.\nWe decouple weight decay and loss-based gradient updates in Adam as shown in line 12 of Algo-\nrithm 2; this gives rise to our variant of Adam with decoupled weight decay ( AdamW ).\nHaving shown that L 2regularization and weight decay regularization differ for adaptive gradient\nalgorithms raises the question of how they differ and how to interpret their effects. Their equivalence\nfor standard SGD remains very helpful for intuition: both mechanisms push weights closer to zero,\nat the same rate. However, for adaptive gradient algorithms they differ: with L 2regularization, the\nsums of the gradient of the loss function and the gradient of the regularizer (i.e., the L 2norm of the\nweights) are adapted, whereas with decoupled weight decay, only the gradients of the loss function\nare adapted (with the weight decay step separated from the adaptive gradient mechanism). With\nL2regularization both types of gradients are normalized by their typical (summed) magnitudes, and\ntherefore weights xwith large typical gradient magnitude sare regularized by a smaller relative\namount than other weights. In contrast, decoupled weight decay regularizes all weights with the\nsame rate\u0015, effectively regularizing weights xwith largesmore than standard L 2regularization\n3\nPublished as a conference paper at ICLR 2019\ndoes. We demonstrate this formally for a simple special case of adaptive gradient algorithm with a\n\ufb01xed preconditioner:\nProposition 3 (Weight decay = scale-adjusted L2reg for adaptive gradient algorithm with \ufb01xed\npreconditioner) .LetOdenote an algorithm with the same characteristics as in Proposition 2, and\nusing a \ufb01xed preconditioner matrix Mt=diag(s)\u00001(withsi>0for alli). Then,Owith base\nlearning rate \u000bexecutes the same steps on batch loss functions ft(\u0012)with weight decay \u0015as it\nexecutes without weight decay on the scale-adjusted regularized batch loss\nfsreg\nt(\u0012) =ft(\u0012) +\u00150\n2\u000b\r\r\u0012\fps\r\r2\n2; (2)\nwhere\fandp\u0001denote element-wise multiplication and square root, respectively, and \u00150=\u0015\n\u000b.\nWe note that this proposition does notdirectly apply to practical adaptive gradient algorithms, since\nthese change the preconditioner matrix at every step. Nevertheless, it can still provide intuition about\nthe equivalent loss function being optimized in each step: parameters \u0012iwith a large inverse pre-\nconditionersi(which in practice would be caused by historically large gradients in dimension i) are\nregularized relatively more than they would be with L 2regularization; speci\ufb01cally, the regularization\nis proportional topsi.\n3 J USTIFICATION OF DECOUPLED WEIGHT DECAY VIA A VIEW OF\nADAPTIVE GRADIENT METHODS AS BAYESIAN FILTERING\nWe now discuss a justi\ufb01cation of decoupled weight decay in the framework of Bayesian \ufb01ltering for\na uni\ufb01ed theory of adaptive gradient algorithms due to Aitchison (2018). After we posted a prelim-\ninary version of our current paper on arXiv, Aitchison noted that his theory \u201cgives us a theoretical\nframework in which we can understand the superiority of this weight decay over L2regularization,\nbecause it is weight decay, rather than L2regularization that emerges through the straightforward ap-\nplication of Bayesian \ufb01ltering.\u201d(Aitchison, 2018). While full credit for this theory goes to Aitchison,\nwe summarize it here to shed some light on why weight decay may be favored over L2regulariza-\ntion.\nAitchison (2018) views stochastic optimization of nparameters\u00121;:::;\u0012nas a Bayesian \ufb01ltering\nproblem with the goal of inferring a distribution over the optimal values of each of the parameters \u0012i\ngiven the current values of the other parameters \u0012\u0000i(t)at time stept. When the other parameters do\nnot change this is an optimization problem, but when they do change it becomes one of \u201ctracking\u201d\nthe optimizer using Bayesian \ufb01ltering as follows. One is given a probability distribution P(\u0012tj\ny1:t)of the optimizer at time step tthat takes into account the data y1:tfrom the \ufb01rst tmini\nbatches, a state transition prior P(\u0012t+1j\u0012t)re\ufb02ecting a (small) data-independent change in this\ndistribution from one step to the next, and a likelihood P(yt+1j\u0012t+1)derived from the mini batch\nat stept+ 1. The posterior distribution P(\u0012t+1jy1:t+1)of the optimizer at time step t+ 1\ncan then be computed (as usual in Bayesian \ufb01ltering) by marginalizing over \u0012tto obtain the one-\nstep ahead predictions P(\u0012t+1jy1:t)and then applying Bayes\u2019 rule to incorporate the likelihood\nP(yt+1j\u0012t+1). Aitchison (2018) assumes a Gaussian state transition distribution P(\u0012t+1j\u0012t)and\nan approximate conjugate likelihood P(yt+1j\u0012t+1), leading to the following closed-form update\nof the \ufb01ltering distribution\u2019s mean:\n\u0016post=\u0016prior +\u0006post\u0002g; (3)\nwheregis the gradient of the log likelihood of the mini batch at time t. This result implies a precon-\nditioner of the gradients that is given by the posterior uncertainty \u0006postof the \ufb01ltering distribution:\nupdates are larger for parameters we are more uncertain about and smaller for parameters we are\nmore certain about. Aitchison (2018) goes on to show that popular adaptive gradient methods, such\nas Adam and RMSprop, as well as Kronecker-factorized methods are special cases of this frame-\nwork.\nDecoupled weight decay very naturally \ufb01ts into this uni\ufb01ed framework as part of the state-transition\ndistribution: Aitchison (2018) assumes a slow change of the optimizer according to the following\nGaussian:\nP(\u0012t+1j\u0012t) =N((I\u0000A)\u0012t;Q); (4)\n4\nPublished as a conference paper at ICLR 2019\nFigure 1: Adam performs better with decoupled weight decay (bottom row, AdamW) than with L2\nregularization (top row, Adam). We show the \ufb01nal test error of a 26 2x64d ResNet on CIFAR-10\nafter 100 epochs of training with \ufb01xed learning rate (left column), step-drop learning rate (with drops\nat epoch indexes 30, 60 and 80, middle column) and cosine annealing (right column). AdamW leads\nto a more separable hyperparameter search space, especially when a learning rate schedule, such as\nstep-drop and cosine annealing is applied. Cosine annealing yields clearly superior results.\nwhereQis the covariance of Gaussian perturbations of the weights, and Ais a regularizer to avoid\nvalues growing unboundedly over time. When instantiated as A=\u0015\u0002I, this regularizer Aplays\nexactly the role of decoupled weight decay as described in Equation 1, since this leads to multiplying\nthe current mean estimate \u0012tby(1\u0000\u0015)at each step. Notably, this regularization is also directly\napplied to the prior and does not depend on the uncertainty in each of the parameters (which would\nbe required for L2regularization).\n4 E XPERIMENTAL VALIDATION\nWe now evaluate the performance of decoupled weight decay under various training budgets\nand learning rate schedules. Our experimental setup follows that of Gastaldi (2017), who pro-\nposed, in addition to L 2regularization, to apply the new Shake-Shake regularization to a 3-branch\nresidual DNN that allowed to achieve new state-of-the-art results of 2.86% on the CIFAR-10\ndataset (Krizhevsky, 2009). We used the same model\/source code based on fb.resnet.torch1. We\nalways used a batch size of 128 and applied the regular data augmentation procedure for the CI-\nFAR datasets. The base networks are a 26 2x64d ResNet (i.e. the network has a depth of 26, 2\nresidual branches and the \ufb01rst residual block has a width of 64) and a 26 2x96d ResNet with 11.6M\nand 25.6M parameters, respectively. For a detailed description of the network and the Shake-Shake\nmethod, we refer the interested reader to Gastaldi (2017). We also perform experiments on the Im-\nageNet32x32 dataset (Chrabaszcz et al., 2017), a downsampled version of the original ImageNet\ndataset with 1.2 million 32 \u000232 pixels images.\n4.1 E VALUATING DECOUPLED WEIGHT DECAY WITHDIFFERENT LEARNING RATE\nSCHEDULES\nIn our \ufb01rst experiment, we compare Adam with L2regularization to Adam with decoupled weight\ndecay (AdamW), using three different learning rate schedules: a \ufb01xed learning rate, a drop-step\n1https:\/\/github.com\/xgastaldi\/shake-shake\n5\nPublished as a conference paper at ICLR 2019\nFigure 2: The Top-1 test error of a 26 2x64d ResNet on CIFAR-10 measured after 100 epochs. The\nproposed SGDW and AdamW (right column) have a more separable hyperparameter space.\nschedule, and a cosine annealing schedule (Loshchilov & Hutter, 2016). Since Adam already adapts\nits parameterwise learning rates it is not as common to use a learning rate multiplier schedule with\nit as it is with SGD, but as our results show such schedules can substantially improve Adam\u2019s per-\nformance, and we advocate not to overlook their use for adaptive gradient algorithms.\nFor each learning rate schedule and weight decay variant, we trained a 2x64d ResNet for 100 epochs,\nusing different settings of the initial learning rate \u000band the weight decay factor \u0015. Figure 1 shows\nthat decoupled weight decay outperforms L2regularization for all learning rate schedules, with\nlarger differences for better learning rate schedules. We also note that decoupled weight decay leads\nto a more separable hyperparameter search space, especially when a learning rate schedule, such\nas step-drop and cosine annealing is applied. The \ufb01gure also shows that cosine annealing clearly\noutperforms the other learning rate schedules; we thus used cosine annealing for the remainder of\nthe experiments.\n4.2 D ECOUPLING THE WEIGHT DECAY AND INITIAL LEARNING RATEPARAMETERS\nIn order to verify our hypothesis about the coupling of \u000band\u0015, in Figure 2 we compare the perfor-\nmance of L 2regularization vs. decoupled weight decay in SGD (SGD vs. SGDW, top row) and in\nAdam (Adam vs. AdamW, bottom row). In SGD (Figure 2, top left), L 2regularization is not decou-\npled from the learning rate (the common way as described in Algorithm 1), and the \ufb01gure clearly\nshows that the basin of best hyperparameter settings (depicted by color and top-10 hyperparameter\nsettings by black circles) is not aligned with the x-axis or y-axis but lies on the diagonal. This sug-\ngests that the two hyperparameters are interdependent and need to be changed simultaneously, while\nonly changing one of them might substantially worsen results. Consider, e.g., the setting at the top\nleft black circle ( \u000b= 1=2,\u0015= 1=8\u00030:001); only changing either \u000bor\u0015by itself would worsen\nresults, while changing both of them could still yield clear improvements. We note that this coupling\nof initial learning rate and L 2regularization factor might have contributed to SGD\u2019s reputation of\nbeing very sensitive to its hyperparameter settings.\nIn contrast, the results for SGD with decoupled weight decay (SGDW) in Figure 2 (top right) show\nthat weight decay and initial learning rate are decoupled. The proposed approach renders the two\nhyperparameters more separable: even if the learning rate is not well tuned yet (e.g., consider the\nvalue of 1\/1024 in Figure 2, top right), leaving it \ufb01xed and only optimizing the weight decay factor\n6\nPublished as a conference paper at ICLR 2019\nFigure 3: Learning curves (top row) and generalization results (bottom row) obtained by a 26\n2x96d ResNet trained with Adam and AdamW on CIFAR-10. See text for details. SuppFigure 4 in\nthe Appendix shows the same qualitative results for ImageNet32x32.\nwould yield a good value (of 1\/4*0.001). This is not the case for SGD with L 2regularization (see\nFigure 2, top left).\nThe results for Adam with L 2regularization are given in Figure 2 (bottom left). Adam\u2019s best hy-\nperparameter settings performed clearly worse than SGD\u2019s best ones (compare Figure 2, top left).\nWhile both methods used L 2regularization, Adam did not bene\ufb01t from it at all: its best results ob-\ntained for non-zero L 2regularization factors were comparable to the best ones obtained without the\nL2regularization, i.e., when \u0015= 0. Similarly to the original SGD, the shape of the hyperparameter\nlandscape suggests that the two hyperparameters are coupled.\nIn contrast, the results for our new variant of Adam with decoupled weight decay (AdamW) in\nFigure 2 (bottom right) show that AdamW largely decouples weight decay and learning rate. The\nresults for the best hyperparameter settings were substantially better than the best ones of Adam\nwith L 2regularization and rivaled those of SGD and SGDW.\nIn summary, the results in Figure 2 support our hypothesis that the weight decay and learning rate\nhyperparameters can be decoupled, and that this in turn simpli\ufb01es the problem of hyperparameter\ntuning in SGD and improves Adam\u2019s performance to be competitive w.r.t. SGD with momentum.\n4.3 B ETTER GENERALIZATION OF ADAM W\nWhile the previous experiment suggested that the basin of optimal hyperparameters of AdamW is\nbroader and deeper than the one of Adam, we next investigated the results for much longer runs of\n1800 epochs to compare the generalization capabilities of AdamW and Adam.\nWe \ufb01xed the initial learning rate to 0.001 which represents both the default learning rate for Adam\nand the one which showed reasonably good results in our experiments. Figure 3 shows the results\nfor 12 settings of the L 2regularization of Adam and 7 settings of the normalized weight decay of\nAdamW (the normalized weight decay represents a rescaling formally de\ufb01ned in Appendix B.1; it\namounts to a multiplicative factor which depends on the number of batch passes). Interestingly,\nwhile the dynamics of the learning curves of Adam and AdamW often coincided for the \ufb01rst half\nof the training run, AdamW often led to lower training loss and test errors (see Figure 3 top left\nand top right, respectively). Importantly, the use of L 2weight decay in Adam did not yield as good\n7\nPublished as a conference paper at ICLR 2019\nFigure 4: Top-1 test error on CIFAR-10 (left) and Top-5 test error on ImageNet32x32 (right).\nFor a better resolution and with training loss curves, see SuppFigure 5 and SuppFigure 6 in the\nsupplementary material.\nresults as decoupled weight decay in AdamW (see also Figure 3, bottom left). Next, we investigated\nwhether AdamW\u2019s better results were only due to better convergence or due to better generalization.\nThe results in Figure 3 (bottom right) for the best settings of Adam and AdamW suggest that AdamW\ndid not only yield better training loss but also yielded better generalization performance for similar\ntraining loss values . The results on ImageNet32x32 (see SuppFigure 4 in the Appendix) yield the\nsame conclusion of substantially improved generalization performance.\n4.4 A DAM WR WITH WARM RESTARTS FOR BETTER ANYTIME PERFORMANCE\nIn order to improve the anytime performance of SGDW and AdamW we extended them with the\nwarm restarts we introduced in Loshchilov & Hutter (2016), to obtain SGDWR and AdamWR, re-\nspectively (see Section B.2 in the Appendix). As Figure 4 shows, AdamWR greatly sped up AdamW\non CIFAR-10 and ImageNet32x32, up to a factor of 10 (see the results at the \ufb01rst restart). For the\ndefault learning rate of 0.001, AdamW achieved 15% relative improvement in test error compared to\nAdam both on CIFAR-10 (also see SuppFigure 5) and ImageNet32x32 (also see SuppFigure 6).\nAdamWR achieved the same improved results but with a much better anytime performance. These\nimprovements closed most of the gap between Adam and SGDWR on CIFAR-10 and yielded com-\nparable performance on ImageNet32x32.\n4.5 U SE OF ADAM WON OTHER DATASETS AND ARCHITECTURES\nSeveral other research groups have already successfully applied AdamW in citable works. For exam-\nple, Wang et al. (2018) used AdamW to train a novel architecture for face detection on the standard\nWIDER FACE dataset (Yang et al., 2016), obtaining almost 10x faster predictions than the previous\nstate of the art algorithms while achieving comparable performance. V \u00a8olker et al. (2018) employed\nAdamW with cosine annealing to train convolutional neural networks to classify and characterize\nerror-related brain signals measured from intracranial electroencephalography (EEG) recordings.\nWhile their paper does not provide a comparison to Adam, they kindly provided us with a direct\ncomparison of the two on their best-performing problem-speci\ufb01c network architecture Deep4Net\nand a variant of ResNet. AdamW with the same hyperparameter setting as Adam yielded higher\ntest set accuracy on Deep4Net (73.68% versus 71.37%) and statistically signi\ufb01cantly higher test\nset accuracy on ResNet (72.04% versus 61.34%). Radford et al. (2018) employed AdamW to train\nTransformer (Vaswani et al., 2017) architectures to obtain new state-of-the-art results on a wide\nrange of benchmarks for natural language understanding. Zhang et al. (2018) compared L 2reg-\nularization vs. weight decay for SGD, Adam and the Kronecker-Factored Approximate Curvature\n(K-FAC) optimizer (Martens & Grosse, 2015) on the CIFAR datasets with ResNet and VGG archi-\ntectures, reporting that decoupled weight decay consistently outperformed L 2regularization in cases\nwhere they differ.\n8\nPublished as a conference paper at ICLR 2019\n5 C ONCLUSION AND FUTURE WORK\nFollowing suggestions that adaptive gradient methods such as Adam might lead to worse generaliza-\ntion than SGD with momentum (Wilson et al., 2017), we identi\ufb01ed and exposed the inequivalence\nof L 2regularization and weight decay for Adam. We empirically showed that our version of Adam\nwith decoupled weight decay yields substantially better generalization performance than the com-\nmon implementation of Adam with L 2regularization. We also proposed to use warm restarts for\nAdam to improve its anytime performance.\nOur results obtained on image classi\ufb01cation datasets must be veri\ufb01ed on a wider range of tasks,\nespecially ones where the use of regularization is expected to be important. It would be interesting\nto integrate our \ufb01ndings on weight decay into other methods which attempt to improve Adam, e.g,\nnormalized direction-preserving Adam (Zhang et al., 2017). While we focused our experimental\nanalysis on Adam, we believe that similar results also hold for other adaptive gradient methods,\nsuch as AdaGrad (Duchi et al., 2011) and AMSGrad (Reddi et al., 2018).\n6 A CKNOWLEDGMENTS\nWe thank Patryk Chrabaszcz for help with running experiments with ImageNet32x32; Matthias\nFeurer and Robin Schirrmeister for providing valuable feedback on this paper in several iterations;\nand Martin V \u00a8olker, Robin Schirrmeister, and Tonio Ball for providing us with a comparison of\nAdamW and Adam on their EEG data. We also thank the following members of the deep learning\ncommunity for implementing decoupled weight decay in various deep learning libraries:\n\u000fJingwei Zhang, Lei Tai, Robin Schirrmeister, and Kashif Rasul for their implementations\nin PyTorch (see https:\/\/github.com\/pytorch\/pytorch\/pull\/4429 )\n\u000fPhil Jund for his implementation in TensorFlow described at\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/contrib\/opt\/\nDecoupledWeightDecayExtension\n\u000fSylvain Gugger, Anand Saha, Jeremy Howard and other members of fast.ai for their imple-\nmentation available at https:\/\/github.com\/sgugger\/Adam-experiments\n\u000fGuillaume Lambard for his implementation in Keras available at https:\/\/github.\ncom\/GLambard\/AdamW_Keras\n\u000fYagami Lin for his implementation in Caffe available at https:\/\/github.com\/\nYagami123\/Caffe-AdamW-AdamWR\nThis work was supported by the European Research Council (ERC) under the European Union\u2019s\nHorizon 2020 research and innovation programme under grant no. 716721, by the German Research\nFoundation (DFG) under the BrainLinksBrainTools Cluster of Excellence (grant number EXC 1086)\nand through grant no. INST 37\/935-1 FUGG, and by the German state of Baden-W \u00a8urttemberg\nthrough bwHPC.\nREFERENCES\nLaurence Aitchison. A uni\ufb01ed theory of adaptive stochastic gradient descent as Bayesian \ufb01ltering.\narXiv:1507.02030 , 2018.\nPatryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of ImageNet as an\nalternative to the CIFAR datasets. arXiv:1707.08819 , 2017.\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\nLearning augmentation policies from data. arXiv preprint arXiv:1805.09501 , 2018.\nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize\nfor deep nets. arXiv:1703.04933 , 2017.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. The Journal of Machine Learning Research , 12:2121\u20132159, 2011.\n9\nPublished as a conference paper at ICLR 2019\nXavier Gastaldi. Shake-Shake regularization. arXiv preprint arXiv:1705.07485 , 2017.\nStephen Jos \u00b4e Hanson and Lorien Y Pratt. Comparing biases for minimal network construction with\nback-propagation. In Proceedings of the 1st International Conference on Neural Information\nProcessing Systems , pp. 177\u2013185, 1988.\nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.\nSnapshot ensembles: Train 1, get m for free. arXiv:1704.00109 , 2017.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-\nter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.\narXiv:1609.04836 , 2016.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980 ,\n2014.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\nHao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets.\narXiv preprint arXiv:1712.09913 , 2017.\nIlya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts.\narXiv:1608.03983 , 2016.\nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\ncurvature. In International conference on machine learning , pp. 2408\u20132417, 2015.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv:1511.06434 , 2015.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training. URL https:\/\/s3-us-west-2. amazonaws. com\/openai-\nassets\/research-covers\/language-unsupervised\/language understanding paper. pdf , 2018.\nSashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. Inter-\nnational Conference on Learning Representations , 2018.\nLeslie N Smith. Cyclical learning rates for training neural networks. arXiv:1506.01186v3 , 2016.\nTijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural networks for machine learning , 4(2):26\u2013\n31, 2012.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems , pp. 5998\u20136008, 2017.\nMartin V \u00a8olker, Ji \u02c7r\u00b4\u0131 Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas\nSchulze-Bonhage, Petr Marusi \u02c7c, Wolfram Burgard, and Tonio Ball. Intracranial error detection\nvia deep learning. arXiv preprint arXiv:1805.01667 , 2018.\nJianfeng Wang, Ye Yuan, Gang Yu, and Sun Jian. Sface: An ef\ufb01cient network for face detection in\nlarge scale variations. arXiv preprint arXiv:1804.06559 , 2018.\nAshia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The\nmarginal value of adaptive gradient methods in machine learning. arXiv:1705.08292 , 2017.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich\nZemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual\nattention. In International Conference on Machine Learning , pp. 2048\u20132057, 2015.\nShuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: A face detection bench-\nmark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp.\n5525\u20135533, 2016.\n10\nPublished as a conference paper at ICLR 2019\nGuodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay\nregularization. arXiv preprint arXiv:1810.12281 , 2018.\nZijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu. Normalized direction-preserving adam.\narXiv:1709.04546 , 2017.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V . Le. Learning transferable architectures\nfor scalable image recognition. In arXiv:1707.07012 [cs.CV] , 2017.\n11\nPublished as a conference paper at ICLR 2019\nAppendix\nA F ORMAL ANALYSIS OF WEIGHT DECAY VS L2REGULARIZATION\nProof of Proposition 1\nThe proof for this well-known fact is straight-forward. SGD without weight decay has the following\niterates onfreg\nt(\u0012) =ft(\u0012) +\u00150\n2k\u0012k2\n2:\n\u0012t+1 \u0012t\u0000\u000brfreg\nt(\u0012t) =\u0012t\u0000\u000brft(\u0012t)\u0000\u000b\u00150\u0012t: (5)\nSGD with weight decay has the following iterates on ft(\u0012):\n\u0012t+1 (1\u0000\u0015)\u0012t\u0000\u000brft(\u0012t): (6)\nThese iterates are identical since \u00150=\u0015\n\u000b.\nProof of Proposition 2\nSimilarly to the proof of Proposition 1, the iterates of Owithout weight decay on freg\nt(\u0012) =ft(\u0012) +\n1\n2\u00150k\u0012k2\n2andOwith weight decay \u0015onftare, respectively:\n\u0012t+1 \u0012t\u0000\u000b\u00150Mt\u0012t\u0000\u000bMtrft(\u0012t): (7)\n\u0012t+1 (1\u0000\u0015)\u0012t\u0000\u000bMtrft(\u0012t): (8)\nThe equality of these iterates for all \u0012twould imply \u0015\u0012t=\u000b\u00150Mt\u0012t. This can only hold for all \u0012t\nifMt=kI, withk2R, which is not the case for O. Therefore, no L 2regularizer\u00150k\u0012k2\n2exists\nthat makes the iterates equivalent.\nProof of Proposition 3\nOwithout weight decay has the following iterates on fsreg\nt(\u0012) =ft(\u0012) +\u00150\n2\r\r\u0012\fps\r\r2\n2:\n\u0012t+1 \u0012t\u0000\u000brfsreg\nt(\u0012t)=s (9)\n=\u0012t\u0000\u000brft(\u0012t)=s\u0000\u000b\u00150\u0012t\fs=s (10)\n=\u0012t\u0000\u000brft(\u0012t)=s\u0000\u000b\u00150\u0012t; (11)\nwhere the division by sis element-wise. Owith weight decay has the following iterates on ft(\u0012):\n\u0012t+1 (1\u0000\u0015)\u0012t\u0000\u000brf(\u0012t)=s (12)\n=\u0012t\u0000\u000brf(\u0012t)=s\u0000\u0015\u0012t; (13)\nThese iterates are identical since \u00150=\u0015\n\u000b.\nB A DDITIONAL PRACTICAL IMPROVEMENTS OF ADAM\nHaving discussed decoupled weight decay for improving Adam\u2019s generalization, in this section we\nintroduce two additional components to improve Adam\u2019s performance in practice.\nB.1 N ORMALIZED WEIGHT DECAY\nOur preliminary experiments showed that different weight decay factors are optimal for different\ncomputational budgets (de\ufb01ned in terms of the number of batch passes). Relatedly, Li et al. (2017)\ndemonstrated that a smaller batch size (for the same total number of epochs) leads to the shrinking\neffect of weight decay being more pronounced. Here, we propose to reduce this dependence by nor-\nmalizing the values of weight decay. Speci\ufb01cally, we replace the hyperparameter \u0015by a new (more\nrobust) normalized weight decay hyperparameter \u0015norm , and use this to set \u0015as\u0015=\u0015normq\nb\nBT,\nwherebis the batch size, Bis the total number of training points and Tis the total number of\nepochs.2Thus,\u0015norm can be interpreted as the weight decay used if only one batch pass is al-\nlowed. We emphasize that our choice of normalization is merely one possibility informed by few\nexperiments; a more lasting conclusion we draw is that using some normalization can substantially\nimprove results.\n2In the context of our AdamWR variant discussed in Section B.2, Tis the total number of epochs in the\ncurrent restart.\n1\nPublished as a conference paper at ICLR 2019\nB.2 A DAM WITH COSINE ANNEALING AND WARM RESTARTS\nWe now apply cosine annealing and warm restarts to Adam, following our recent work (Loshchilov\n& Hutter, 2016). There, we proposed Stochastic Gradient Descent with Warm Restarts (SGDR) to\nimprove the anytime performance of SGD by quickly cooling down the learning rate according to a\ncosine schedule and periodically increasing it. SGDR has been successfully adopted to lead to new\nstate-of-the-art results for popular image classi\ufb01cation benchmarks (Huang et al., 2017; Gastaldi,\n2017; Zoph et al., 2017), and we therefore already tried extending it to Adam shortly after proposing\nit. However, while our initial version of Adam with warm restarts had better anytime performance\nthan Adam, it was not competitive with SGD with warm restarts, precisely because L 2regularization\nwas not working as well as in SGD. Now, having \ufb01xed this issue by means of the original weight\ndecay regularization (Section 2) and also having introduced normalized weight decay (Section B.1),\nour original work on cosine annealing and warm restarts directly carries over to Adam.\nIn the interest of keeping the presentation self-contained, we brie\ufb02y describe how SGDR schedules\nthe change of the effective learning rate in order to accelerate the training of DNNs. Here, we\ndecouple the initial learning rate \u000band its multiplier \u0011tused to obtain the actual learning rate at\niterationt(see, e.g., line 8 in Algorithm 1). In SGDR, we simulate a new warm-started run\/restart of\nSGD onceTiepochs are performed, where iis the index of the run. Importantly, the restarts are not\nperformed from scratch but emulated by increasing \u0011twhile the old value of \u0012tis used as an initial\nsolution. The amount by which \u0011tis increased controls to which extent the previously acquired\ninformation (e.g., momentum) is used. Within the i-th run, the value of \u0011tdecays according to a\ncosine annealing (Loshchilov & Hutter, 2016) learning rate for each batch as follows:\n\u0011t=\u0011(i)\nmin+ 0:5(\u0011(i)\nmax\u0000\u0011(i)\nmin)(1 + cos(\u0019Tcur=Ti)); (14)\nwhere\u0011(i)\nminand\u0011(i)\nmax are ranges for the multiplier and Tcuraccounts for how many epochs have\nbeen performed since the last restart. Tcuris updated at each batch iteration tand is thus not\nconstrained to integer values. Adjusting (e.g., decreasing) \u0011(i)\nminand\u0011(i)\nmax at everyi-th restart (see\nalso Smith (2016)) could potentially improve performance, but we do not consider that option here\nbecause it would involve additional hyperparameters. For \u0011(i)\nmax= 1and\u0011(i)\nmin= 0, one can simplify\nEq. (14) to\n\u0011t= 0:5 + 0:5 cos(\u0019Tcur=Ti): (15)\nIn order to achieve good anytime performance, one can start with an initially small Ti(e.g., from\n1% to 10% of the expected total budget) and multiply it by a factor of Tmult (e.g.,Tmult = 2) at\nevery restart. The (i+ 1) -th restart is triggered when Tcur=Tiby settingTcurto 0. An example\nsetting of the schedule multiplier is given in C.\nOur proposed AdamWR algorithm represents AdamW (see Algorithm 2) with \u0011tfollowing Eq. (15)\nand\u0015computed at each iteration using normalized weight decay described in Section B.1. We note\nthat normalized weight decay allowed us to use a constant parameter setting across short and long\nruns performed within AdamWR and SGDWR (SGDW with warm restarts).\nC A NEXAMPLE SETTING OF THE SCHEDULE MULTIPLIER\nAn example schedule of the schedule multiplier \u0011tis given in SuppFigure 1 for Ti=0= 100 and\nTmult = 2. After the initial 100 epochs the learning rate will reach 0 because \u0011t=100 = 0. Then,\nsinceTcur=Ti=0, we restart by resetting Tcur= 0, causing the multiplier \u0011tto be reset to 1 due\nto Eq. (15). This multiplier will then decrease again from 1 to 0, but now over the course of 200\nepochs because Ti=1=Ti=0Tmult = 200 . Solutions obtained right before the restarts, when \u0011t= 0\n(e.g., at epoch indexes 100, 300, 700 and 1500 as shown in SuppFigure 1) are recommended by the\noptimizer as the solutions, with more recent solutions prioritized.\nD A DDITIONAL RESULTS\nWe investigated whether the use of much longer runs (1800 epochs) of \u201cstandard Adam\u201d (Adam\nwith L 2regularization and a \ufb01xed learning rate) makes the use of cosine annealing unnecessary.\n2\nPublished as a conference paper at ICLR 2019\n200 400 600 800 1000 1200 140000.20.40.60.81\nEpochsLearning rate multiplier \u03b7T0=100, Tmult=2\nSuppFigure 1: An example schedule of the learning rate multiplier as a function of epoch index.\nThe \ufb01rst run is scheduled to converge at epoch Ti=0= 100 , then the budget for the next run is\ndoubled asTi=1=Ti=0Tmult = 200 , etc.\nSuppFigure 2 shows the results of standard Adam for a 4 by 4 logarithmic grid of hyperparame-\nter settings (the coarseness of the grid is due to the high computational expense of runs for 1800\nepochs). Even after taking the low resolution of the grid into account, the results appear to be at best\ncomparable to the ones obtained with AdamW with 18 times less epochs and a smaller network (see\nSuppFigure 3, top row, middle). These results are not very surprising given Figure 1 in the main\npaper (which demonstrates both the improvements possible by using some learning rate schedule,\nsuch as cosine annealing, and the effectiveness of decoupled weight decay).\nOur experimental results with Adam and SGD suggest that the total runtime in terms of the number\nof epochs affect the basin of optimal hyperparameters (see SuppFigure 3). More speci\ufb01cally, the\ngreater the total number of epochs the smaller the values of the weight decay should be. SuppFigure\n4 shows that our remedy for this problem, the normalized weight decay de\ufb01ned in Eq. (15), sim-\npli\ufb01es hyperparameter selection because the optimal values observed for short runs are similar to\nthe ones for much longer runs. We used our initial experiments on CIFAR-10 to suggest the square\nroot normalization we proposed in Eq. (15) and double-checked that this is not a coincidence on the\nImageNet32x32 dataset (Chrabaszcz et al., 2017), a downsampled version of the original ImageNet\ndataset with 1.2 million 32 \u000232 pixels images, where an epoch is 24 times longer than on CIFAR-10.\nThis experiment also supported the square root scaling: the best values of the normalized weight de-\ncay observed on CIFAR-10 represented nearly optimal values for ImageNet32x32 (see SuppFigure\n3). In contrast, had we used the same raw weight decay values \u0015for ImageNet32x32 as for CIFAR-\n10 and for the same number of epochs, without the proposed normalization, \u0015would have been\nroughly 5 times too large for ImageNet32x32, leading to much worse performance . The optimal\nnormalized weight decay values were also very similar (e.g., \u0015norm = 0:025and\u0015norm = 0:05)\nacross SGDW and AdamW. These results clearly show that normalizing weight decay can substan-\ntially improve performance; while square root scaling performed very well in our experiments we\nemphasize that these experiments were not very comprehensive and that even better scaling rules\nare likely to exist.\nSuppFigure 4 is the equivalent of Figure 3 in the main paper, but for ImageNet32x32 instead of for\nCIFAR-10. The qualitative results are identical: weight decay leads to better training loss (cross-\nentropy) than L 2regularization, and to an even greater improvement of test error.\nSuppFigure 5 and SuppFigure 6 are the equivalents of Figure 4 in the main paper but supplemented\nwith training loss curves in its bottom row. The results show that Adam and its variants with decou-\npled weight decay converge faster (in terms of training loss) on CIFAR-10 than the corresponding\nSGD variants (the difference for ImageNet32x32 is small). As is discussed in the main paper, when\nthe same values of training loss are considered, AdamW demonstrates better values of test error than\nAdam. Interestingly, SuppFigure 5 and SuppFigure 6 show that the restart variants AdamWR and\nSGDWR also demonstrate better generalization than AdamW and SGDW, respectively.\n3\nPublished as a conference paper at ICLR 2019\nSuppFigure 2: Performance of \u201cstandard Adam\u201d: Adam with L 2regularization and a \ufb01xed learning\nrate. We show the \ufb01nal test error of a 26 2x96d ResNet on CIFAR-10 after 1800 epochs of the\noriginal Adam for different settings of learning rate and weight decay used for L 2regularization.\n4\nPublished as a conference paper at ICLR 2019\nSuppFigure 3: Effect of normalized weight decay. We show the \ufb01nal test Top-1 error on CIFAR-\n10 (\ufb01rst two rows for AdamW without and with normalized weight decay) and Top-5 error on\nImageNet32x32 (last two rows for AdamW and SGDW, both with normalized weight decay) of a\n26 2x64d ResNet after different numbers of epochs (see columns). While the optimal settings of the\nraw weight decay change signi\ufb01cantly for different runtime budgets (see the \ufb01rst row), the values\nof the normalized weight decay remain very similar for different budgets (see the second row) and\ndifferent datasets (here, CIFAR-10 and ImageNet32x32), and even across AdamW and SGDW.\n5\nPublished as a conference paper at ICLR 2019\nSuppFigure 4: Learning curves (top row) and generalization results (Top-5 errors in bottom row)\nobtained by a 26 2x96d ResNet trained with Adam and AdamW on ImageNet32x32.\n6\nPublished as a conference paper at ICLR 2019\nSuppFigure 5: Test error curves (top row) and training loss curves (bottom row) for CIFAR-10.\n7\nPublished as a conference paper at ICLR 2019\nSuppFigure 6: Test error curves (top row) and training loss curves (bottom row) for Ima-\ngeNet32x32.\n8","metadata":{"primary_category":"cs.LG","published":"20171114","title":"Decoupled Weight Decay Regularization","updated":"20190104"}}
{"id":"2305.17493","source":"http:\/\/arxiv.org\/pdf\/2305.17493","text":"THECURSE OF RECURSION :\nTRAINING ON GENERATED DATA MAKES MODELS FORGET\nIlia Shumailov*\nUniversity of OxfordZakhar Shumaylov*\nUniversity of CambridgeYiren Zhao\nImperial College LondonYarin Gal\nUniversity of Oxford\nNicolas Papernot\nUniversity of Toronto & Vector InstituteRoss Anderson\nUniversity of Cambridge & University of Edinburgh\nABSTRACT\nStable Diffusion revolutionised image creation from descriptive text. GPT-2 ,GPT-3(.5) andGPT-4\ndemonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such\nlanguage models to the general public. It is now clear that large language models (LLMs) are here to\nstay, and will bring about drastic change in the whole ecosystem of online text and images. In this\npaper we consider what the future might hold. What will happen to GPT-{n}once LLMs contribute\nmuch of the language found online? We find that use of model-generated content in training causes\nirreversible defects in the resulting models, where tails of the original content distribution disappear.\nWe refer to this effect as model collapse1and show that it can occur in Variational Autoencoders,\nGaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and\nportray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken\nseriously if we are to sustain the benefits of training from large-scale data scraped from the web.\nIndeed, the value of data collected about genuine human interactions with systems will be increasingly\nvaluable in the presence of content generated by LLMs in data crawled from the Internet.\n1 Introduction\nA lot of human communication happens online. Billions of emails are exchanged daily, along with billions of social-\nmedia messages and millions of news articles. Almost all of this material was produced and curated only by humans in\nthe early years of the worldwide web, yet since the turn of the century search engines have come to determine what\npeople can find, and in the past decade smart text editors with spelling and grammar correction have helped tweak what\nwe produce. Now, text can not only be groomed and analysed efficiently; it can also be generated \u2013 by large language\nmodels (LLMs). These models now (arguably) pass a weaker form of the Turing test in the sense that their output\ncannot be reliably distinguished from text written by humans [Solaiman et al., 2019].\nThe development of LLMs is quite involved and requires masses of training data. Anecdotally, some powerful recent\nmodels are trained using scrapes of much of the Internet, then further fine-tuned with reinforcement learning from\nhuman feedback (RLHF) [Griffith et al., 2013, OpenAI, 2023]. This further boosts the effective dataset size. Yet while\ncurrent LLMs [Devlin et al., 2018, Liu et al., 2019, Brown et al., 2020, Zhang et al., 2022], including GPT-4 , were\ntrained on predominantly human-generated text, this may change in the future. If most future models\u2019 training data\nis also scraped from the web, then they will inevitably come to train on data produced by their predecessors. In this\npaper, we investigate what happens when text produced, e.g.by a version of GPT, forms most of the training dataset of\nfollowing models. What happens to GPTversions GPT-{ n}as generation nincreases?2\n1The name is inspired by the Generative Adversarial Networks (GAN) literature on mode collapse, where GANs start producing\na limited set of outputs that all trick the discriminator. Model Collapse is a process whereby models eventually converge to a state\nsimilar to that of a GAN Mode Collapse. The original version of this paper referred to this effect as \u2018model dementia\u2019, but we decided\nto change this following feedback that it trivialised the medical notion of \u2018dementia\u2019 and could cause offence.\n2This is not limited to text models; one can also consider what happens when music created by human composers and played by\nhuman musicians trains models whose output trains other models.arXiv:2305.17493v2  [cs.LG]  31 May 2023\nModel Collapse\nWe discover that learning from data produced by other models causes model collapse \u2013 a degenerative process whereby,\nover time, models forget the true underlying data distribution, even in the absence of a shift in the distribution over time.\nWe give examples of model collapse for Gaussian Mixture Models (GMMs), Variational Autoencoders (V AE) and\nLarge Language models (LLMs). We show that over time we start losing information about the true distribution, which\nfirst starts with tails disappearing, and over the generations learned behaviours start converging to a point estimate with\nvery small variance. Furthermore, we show that this process is inevitable, even for cases with almost ideal conditions\nfor long-term learning i.e.no function estimation error.\nFigure 1: Model Collapse refers to a degenerative learning\nprocess where models start forgetting improbable events\nover time, as the model becomes poisoned with its own\nprojection of reality.Finally, we discuss the broader implications of model\ncollapse . We note that access to the original data dis-\ntribution is crucial: in learning where the tails of the\nunderlying distribution matter, one needs access to real\nhuman-produced data. In other words, the use of LLMs\nat scale to publish content on the Internet will pollute\nthe collection of data to train them: data about human\ninteractions with LLMs will be increasingly valuable.\nThis paper is structured as follows. First, in Sections 3\nand 4 we describe the reasons why model collapse hap-\npens. To best describe the intuition, we present a simple\nexample of a single-dimensional Gaussian where errors\ndue to sampling inevitably cause model collapse , which\nare then extended to a multidimensional generative model\nunder some assumptions. Under both models, similar\nlower bounds are derived on the risk, defined in terms of\nthe Wasserstein distance from the true distribution. Next,\nwe turn to GMMs and V AEs to show that additional\nfunctional approximation errors further exacerbate model\ncollapse . Finally, we discuss the most commonly used\nsetting of fine-tuned language models, where we report\nthat only early signs of model collapse can be detected, if models are fine-tuned as opposed to trained from scratch.\nIn this paper we make the following contributions:\n\u2022 We demonstrate the existence of a degenerative process in learning and name it model collapse ;\n\u2022 We demonstrate that model collapse exists in a variety of different model types and datasets;\n\u2022 We show that, to avoid model collapse , access to genuine human-generated content is essential.\n2 Related work\nIn this section we are going to cover two closest concepts to model collapse from existing literature: catastrophic\nforgetting and data poisoning. Neither is able to explain the phenomenon of model collapse fully, as the setting is\nfundamentally different, but they provide another perspective on the observed phenomenon.\n2.1 Continual learning and catastrophic forgetting\nUnlike traditional machine learning which seeks to learn from a static data distribution, continual learning attempts\nto learn from a dynamic one, where data are supplied in a sequential fashion [Van de Ven and Tolias, 2019]. This\ntends to be task-based, where data are provided with delineated task boundaries; e.g., classifying dogs from cats and\nrecognising handwritten digits. Our work is more similar to task-free continual learning [Aljundi et al., 2019] where\ndata distributions gradually change without the notion of separate tasks. Our work examines a particular scenario in\nwhich the changed data distributions arise from the model itself, as a result of training in the previous iteration.\n2\nModel Collapse\nExample of text outputs of an OPT-125m model affected by Model Collapse \u2013 models degrade over generations,\nwhere each new generation is trained on data produced by the previous generation.\nInput: some started before 1360 \u2014 was typically accomplished by a master mason and a small team of\nitinerant masons, supplemented by local parish labourers, according to Poyntz Wright. But other\nauthors reject this model, suggesting instead that leading architects designed the parish church towers\nbased on early examples of Perpendicular\nOutputs:\nGen 0: Revival architecture such as St. John\u2019s Cathedral in London. The earliest surviving example of\nPerpendicular Revival architecture is found in the 18th @-@ century Church of Our Lady of Guernsey,\nwhich dates from the late 19th century. There are two types of perpendicular churches : those\nGen 1: architecture such as St. Peter\u2019s Basilica in Rome or St. Peter\u2019s Basilica in Buenos Aires. There is no\nevidence that any of these buildings were built during the reign of Pope Innocent III, but it is possible\nthat they may have been built during the reign of his successor, Pope Innocent\nGen 7: architecture in England. In an interview with The New York Times, Wright said : \" I don \u2019t think\nthere is anything wrong with me being able to do what I want to do. It just doesn \u2019t work for me. \" He\nadded : \" I don \u2019t know if you can call it funny,\nGen 9: architecture. In addition to being home to some of the world\u2019s largest populations of black @-@ tailed\njackrabbits, white @-@ tailed jackrabbits, blue @-@ tailed jackrabbits, red @-@ tailed jackrabbits,\nyellow @-\nA typical challenge in continual learning is that the model forgets previous samples when learning new information; this\nis known as catastrophic forgetting [Kirkpatrick et al., 2017]. A typical way of preventing it is to use regularisations\n(Memory Aware Synpass [Aljundi et al., 2018]) or just rely on data ( e.g.Learning without Forgetting [Li and Hoiem,\n2017]). This has an indirect connection to our work, yet differs since the data in the process of model collapse are\ngenerated by different generations of models.\n2.2 Data poisoning\nPoisoning attacks are crafted and inserted during training in order to degrade the model\u2019s performance when de-\nployed [Biggio et al., 2012]. Malicious data can be inserted into training data to induce unintended behaviors that can\nbe activated by special triggers [Gu et al., 2017]. The early literature on data poisoning focused mainly on supervised\nlearning, where classifiers are trained with labeled samples. But with the emergence of contrastive learning [Radford\net al., 2021] and LLMs [Brown et al., 2020], more recent models are trained with large-scale web crawls, making data\npoisoning attacks more feasible on these untrustworthy web sources. Recent studies have demonstrated that web-scale\ndatasets can be poisoned by introducing malicious data into a small percentage of samples [Carlini and Terzis, 2021,\nCarlini et al., 2023].\n3 What is Model Collapse ?\nDefinition 3.1 (Model Collapse ).Model Collapse is a degenerative process affecting generations of learned generative\nmodels, where generated data end up polluting the training set of the next generation of models; being trained on\npolluted data, they then mis-perceive reality. We separate two special cases: early model collapse andlatemodel\ncollapse . In early model collapse the model begins losing information about the tails of the distribution; in the late model\ncollapse model entangles different modes of the original distributions and converges to a distribution that carries little\nresemblance to the original one, often with very small variance.\nNote that this process is different from the process of catastrophic forgetting in that we are considering multiple models\nover time, in which our models do not forget previously learned data, but rather start misinterpreting what they believe\nto be real, by reinforcing their own beliefs.\nThis process occurs due to two specific sources of error compounding over generations and causing deviation from the\noriginal model. Of these, one source of error plays a primary role, and in the absence of it, the process would not occur\nbeyond the first generation.\n3\nModel Collapse\nFigure 2: The high-level description of the feedback mechanism in the learning process. Here, data are assumed to be\nhuman-curated and start off clean; then model 0is trained and data are sampled from it; at step n, data are added to the\noverall data from step n\u22121, and this ensemble is used to train model n. Data obtained with Monte Carlo sampling\nshould ideally be statistically close to the original, provided fitting andsampling procedures are perfect. This process\ndepicts what happens in real life with the Internet \u2013 model-generated data become pervasive.\n3.1 Causes of model collapse\nThere are two main causes for model collapse , one primary and one secondary, which we describe now. Further\nmathematical intuition is provided in Section 4 to explain how these give rise to the errors observed, how different\nsources can compound and how we can quantify the average model divergence rate.\n\u2022Statistical approximation error \u2013 this is the primary type of error, which arises due to the number of samples\nbeing finite, and disappears as the number of samples tends to infinity. This occurs due to a non-zero probability\nthat information can get lost at every step of re-sampling. Figure 12 shows an example of an approximation\nerror. Here, a single-dimensional Gaussian is being approximated from a finite number of samples. Despite\nusing a very large number of points, the errors remain significant; with 107samples we estimate the mean to\nbe0.00024899 \u00b11.89382984 e\u22124, when the true value is 0.\n\u2022Functional approximation error \u2013 this is a secondary type of error, which stems from our function approx-\nimators being insufficiently expressive (or sometimes too expressive outside of the original distribution\nsupport [Nguyen et al., 2015]). It is well known that neural networks are universal functional approximators\nin the limit, but in practice this is not always true. In particular, a neural network can introduce non-zero\nlikelihood outside of the support of the original distribution. A simple example of this error is if we were to try\nfitting a mixture of two Gaussians with a single Gaussian. Even if we have perfect information about the data\ndistribution, model errors will be inevitable. It is important to also note that in the absence of statistical error,\nfunctional approximation error only occurs at the first generation. Once the new distribution belongs to the\nimage of functional approximator, it remains exactly the same over the generations.\nEach of the above can cause model collapse to get worse or better. Better approximation power can even be a double-\nedged sword \u2013 better expressiveness may counteract statistical noise, resulting in a good approximation of the true\ndistribution, but it can equally compound this noise. More often then not, we get a cascading effect where combined\nindividual inaccuracy causes the overall error to grow. Overfitting the density model will cause the model to extrapolate\nincorrectly and might give high density to low-density regions not covered in the training set support; these will then be\nsampled with arbitrary frequency.\nIt is worth mentioning that modern computers also have a further computational error coming from the way floating\npoint numbers are represented. This error is not evenly spread across different floating point ranges, making it hard to\nestimate the precise value of a given number. Such errors are smaller in magnitude and are fixable with more precise\nhardware, making them less influential on model collapse .\n4\nModel Collapse\n4 Theoretical intuition\nIn this section we aim to provide a theoretical intuition for the phenomenon of model collapse . We argue that the process\nofmodel collapse is universal among generative models that recursively train on data generated by previous generations.\nWe construct toy mathematical models, which prove to be simple enough to provide analytical expressions for quantities\nof interest, but also portray the phenomenon of model collapse . We aim to quantify how different sources of error can\naffect the overall end approximation of the original distribution. As discussed in Section 3.1, there are two main sources\nwe are interested in \u2013 statistical error and functional error. Since in the real world one rarely has infinite samples,\nquantifying the functional approximation error alone is of little interest for discussion of model collapse . Therefore, we\nwill examine two simple cases: a discrete distribution in the absence of functional approximation error and a single\ndimensional Gaussian case, which portrays how functional approximation error can compound with statistical error.\nThe overall stochastic process we are going to be considering (which we call Learning with Generational Data ) is\nthe following. Assume that at generation iwe have a dataset Dicomprising of i.i.d. random variables Xi\nj, where\nj\u2208 {1, . . . , M i}denotes the sample number at generation iandMi\u22652. We will denote the distribution of Xiaspi.\nHere we assume that p0denotes the original distribution, from which the data comes from. Going from generation i\nto generation i+ 1, we aim to estimate the distribution of samples in Di, with an approximation p\u03b8i+1. This step is\nwhat we refer to as functional approximation F\u03b8:pi\u2192p\u03b8i+1. We then resample the dataset Di+1from the distribution\npi+1=\u03b1ip\u03b8i+1+\u03b2ipi+\u03b3ip0, with non-negative parameters \u03b1i, \u03b2i, \u03b3isumming up to 1,i.e.they represent proportions\nof data used from different generations. This corresponds to a mixing of data coming from the original distribution ( \u03b3i),\ndata used by the previous generation ( \u03b2i) and data generated by the new model ( \u03b1i). We refer to this as the sampling\nstep. For the mathematical models to come, we consider \u03b1i=\u03b3i= 0i.e.data only from a single step is used, while\nnumerical experiments are performed on more realistic choices of parameters.\n4.1 Discrete distributions with exact approximation\nIn this subsection we consider a discrete probability distribution, which is represented by a histogram, e.g.as shown on\nFigure 3. In what follows we consider the stochastic process in absence of functional approximation error, i.e.F(p) =p.\nIn this case, model collapse arises only due to statistical errors from the sampling step. At first, the tails (low probability\nevents) begin to disappear due to low probability of sampling them, and over time the distribution becomes a delta\nfunction. Denoting the sample size as M, if we consider state iwith probability q\u22641\nM, the expected number of\nsamples with value icoming from those events will be less than 1, which means that in practice we will lose information\nabout them. This is portrayed on Figure 3, where infrequent events get cut off. Considering more generally some state\niwith probability q, using standard conditional probability one can show that the probability of losing information\n(i.e.sampling no data at some generation) is equal to 1\u2212q. But this in turn means that we must converge to a delta\nfunction positioned at some state, and the probability of ending up at a certain state is equal to the probability of\nsampling said state from the original distribution.\nBut how do we show directly that this process is going to turn our distribution into a delta function? By considering the\nprocess as going from Xi\u2192 F \u03b8\u2192pi+1\u2192Xi+1, we see that this forms a Markov Chain, as Xi+1only depends on\nXi. Furthermore, if all the Xi\njhave the same value, then at the next generation the approximated distribution will be\nexactly a delta function, and therefore all of Xi+1\njwill also have the same value. This implies that the Markov chain\ncontains at least one absorbing state, and therefore with probability 1 it will converge to one of the absorbing states.\nThis is a well-known fact, of which a proof is provided in Appendix A.1. For this chain, the only absorbing states are\nthose corresponding to delta functions. As a result, as we follow the progress of model collapse , we are guaranteed\nto end up in a constant state, having lost all the information of the original distribution when the chain is absorbed.3\nBased on the discussion above we see how both early and late stage model collapse must arise in the case of discrete\ndistributions with perfect functional approximation.\n4.2 Single dimensional Gaussian\nFollowing the discussion about discrete distributions, we now move on to considering how both functional approximation\nerror and sampling error can compound (or cancel out) the process of model collapse .\nTo demonstrate this, consider a single dimensional Gaussian X0\u223c N(\u00b5, \u03c32). If we have full faith in the data we\nobserve, the functional approximation involves estimating sample mean and variance and fitting a single dimensional\n3This argument also works in general due to floating point representations being discrete, making the Markov Chain over the\nparameters of the model discrete. Thus as long as the model parameterisation allows for delta functions, we willget to it, as due to\nsampling errors the only possible absorbing states are delta functions.\n5\nModel Collapse\n10\n 5\n 0 5 1001234567log(Count)Real distribution 1\n10\n 5\n 0 5 1001234567log(Count)Real distribution 2\n10\n 5\n 0 5 1001234567Resampled 1 and 2\nlog M\nFigure 3: Shown in the middle is a histogram plot of samples from a Gaussian mixture with means (\u22124,4)and variances\nof1. To the left of it is a similar distribution, but with \u2019fatter\u2019 tails, and on the right the same histograms are shown, but\nwith low probability events being cut off due to finite resampling. Although distributions 1 and 2 are very different,\nwhen resampled (only assuming the expected behaviour), the tails get cut off, leading to the same observed distribution.\nIn this case this is all states with probability less than 1\/M, or equivalently, bins with logCount \u2264logM.\nGaussian. We can estimate them using the unbiased sample mean and variance estimators:\n\u00b5i+1=1\nMiX\njXi\nj;\u03c32\ni+1=1\nMi\u22121X\nj(Xi\nj\u2212\u00b5i+1)2. (1)\nNote here, that if we were to use maximum likelihood estimation, we would instead arrive at a biased variance estimator.\nWith these estimates, the functional approximation step simply corresponds to considering a normal distribution with\nthese parameters, which we can sample from:\nXi+1\nj|\u00b5i+1, \u03c3i+1\u223c N(\u00b5i+1, \u03c32\ni+1). (2)\nThis provides us with the conditional distribution of Xi\nj, which allows us to calculate the full distribution of Xi\nj. From\nEquation (3), we see that even after the first approximation, the distribution of Xi\njis no longer normal, it follows a\nvariance-gamma distribution [Fischer et al., 2023]. However, instead of writing the probability density function at each\ngeneration, we can explicitly construct them in terms of independent random variables. In particular, it is well known\n[Cochran, 1934] that \u00b51and\u03c31are independent, with \u00b51\u223c N(\u00b5,\u03c32\nM0)and(M0\u22121)\u03c32\n1\u223c\u03c32\u0393(M0\u22121\n2,1\n2). In what\nfollows we will denote with Zrandom variables that are distributed with N(0,1)and with Sirandom variables that are\ndistributed with1\nMi\u22121\u22121\u0393(Mi\u22121\u22121\n2,1\n2).\nX0\nj=\u00b5+\u03c3Z0\nj;X1\nj=\u00b5+\u03c3\u221aM0Z1+\u03c3\u221a\nS1Z1\nj;. . . (3)\nXn\nj=\u00b5+\u03c3\u221aM0Z1+\u03c3\u221aM1\u221a\nS1Z2+\u00b7\u00b7\u00b7+\u03c3p\nMn\u22121p\nS1\u00d7 \u00b7\u00b7\u00b7 \u00d7 Sn\u22121Zn+\u03c3p\nS1\u00d7 \u00b7\u00b7\u00b7 \u00d7 SnZn\nj.\nThese are not joint distributions, as ZnandSndepend directly on Zn\u22121\nj, but when considering Xn\njon its own the\nformula above provides all the information about the full distribution.\nThe first thing we may try calculating is the variance. It is possible to find its exact value, but the mean and variance of\nthe square root of gamma distribution are expressed in terms of gamma functions, making the result quite clunky. In\nwhat follows, we will expand everything to second order in each of (1\/Mi)as we assume each sample size to be large\n(in practice this becomes quite accurate after M\u223c100). We then find that\n1\n\u03c32Var(Xn\nj) =1\nM0+1\nM1+\u00b7\u00b7\u00b7+1\nMn\u22121+ 1 + O(2).\nAnd if we were to assume that Mi=Mare constant, we would find that:\nVar(Xn\nj) =\u03c32\u0010\n1 +n\nM\u0011\n;E(Xn\nj) =\u00b5.\n6\nModel Collapse\n100101102103\nevolution0.00.20.40.60.8| |\n estimation of a (=0,=1)\n(a) Mean estimation\n100101102103\nevolution0.00.20.40.60.81.0| |\n estimation of a (=0,=1)\n100\n500\n1000\n10000\n100000\n1000000\n10000000 (b) Standard Deviation\nFigure 4: Recursive fitting-sampling of a 1D Gaussian with different numbers of samples drawn. We find that unless\nsampled a very large number of times, i.e.<100000, both standard deviation and mean get significantly affected. Here\nwe report a single run; while re-running the experiment changes the initial performance, both \u00b5and\u03c3drift over time.\nThe overall graph looks quite similar to that of a Gaussian random walk.\n100101102103\nevolution0.00.10.20.30.4| |\n estimation of a (=0,=1)\n(a) Mean estimation\n100101102103\nevolution0.000.050.100.150.200.250.30| |\n estimation of a (=0,=1)\n100\n500\n1000\n10000 (b) Standard Deviation\nFigure 5: Recursive fitting-sampling of a 1D Gaussian with different numbers of samples drawn. In this plot data get\naccumulated in a pool, from which a fixed sample is drawn. In other words, a model ngets data sampled, its output is\nmixed with data sampled from models 1. . . n , and then the mix gets sampled to fit the model n+ 1. The uncertainty\narising from all of the different modalities appearing in data causes the distribution parameters to jump around quite\nsignificantly.\n100101102103\nevolution0.000.020.040.060.080.10| |\n estimation of a (=0,=1)\n(a) Mean estimation\n100101102103\nevolution0.0000.0250.0500.0750.1000.1250.1500.175| |\n estimation of a (=0,=1)\n100\n500\n1000\n10000 (b) Standard Deviation\nFigure 6: Recursive fitting-sampling of a 1D Gaussian with different number of samples drawn. In this plot data are\naccumulated in a pool, all of which is used to fit a model. In other words, a model ngets data sampled, its output mixed\nwith data sampled from models 1. . . n , and then the result is used to fit the model n+ 1. Over time the variance in\nestimates reduces due to linear growth of data.7\nModel Collapse\nThis means that as n\u2192 \u221e , the variance diverges linearly. This is the same scaling as for a single dimensional Gaussian\nrandom walk. We can further see the similarities in numerical experiments shown on Figure 4 for a range of different\nsample sizes, confirming these theoretical intuitions.\nEven though the variance of Xn\njdiverges, it does not provide us with any information of what the corresponding\nestimates of \u00b5n+1and\u03c32\nn+1are, or how far they are from the original \u00b5and\u03c3. In particular, we may want to consider\nwhat the distance would be between the true distribution and the approximated distribution at step n+ 1. To measure\nthis we can consider the Wasserstein-2 distance between two normals:\nRn+1\nW2:=W2\n2\u0000\nN(\u00b5, \u03c32),N(\u00b5n+1, \u03c32\nn+1)\u0001\n=\u2225\u00b5n+1\u2212\u00b5\u22252+\u2225\u03c3n+1\u2212\u03c3\u22252\nNow we can calculate the risk that occurs due to finite sampling, i.e.what the expected value of the distance is\n(expanding in 1\/Mi):\nE\u00b5n+1,\u03c32\nn+1\u0002\nRn+1\nW2\u0003\n=\u03c32\u00121\nM0+1\nM1+\u00b7\u00b7\u00b7+3\n2Mn\u0013\n+O(2), (4)\nVar\u00b5n+1,\u03c32\nn+1\u0002\nRn+1\nW2\u0003\n=\u03c34\uf8eb\n\uf8ed2\nM2\n0+2\nM2\n1+\u00b7\u00b7\u00b7+3\nM2n+X\ni\u0338=j3\nMiMj\uf8f6\n\uf8f8+O(3). (5)\nThis result allows us to interpret exactly what occurs in this formulation of model collapse . To be precise, due to errors\noccurring from re-sampling the approximated distribution, each generation ends up corresponding to a new step in a\nrandom walk of model parameters. The risk that occurs in this model ends up diverging for a constant sample size at\neach generation. In order for the end distribution approximation to be accurate, and for the distance to be finite, the\nsampling rate Mineeds to increase superlinearly, i.e.one needs to collect increasingly more samples over time, perhaps\nquadratically. However, even in that case the expected distance after nsteps remains non-zero and the only case in\nwhich it does in fact end up being 0is when sampling is infinite at each step. Overall, this only shows us how far on\naverage we go from the original distribution, but the process can only \u2019terminate\u2019 if the estimated variance at a certain\ngeneration becomes small enough, i.e.we effectively turn into a delta function.\nShown on Figures 5 and 6 are different runs of this process for different values of parameters of \u03b1i, \u03b2i, \u03b3ifor different\nsample sizes, which was investigated numerically to see whether they can be enough to overcome model collapse ,\nhowever even in those cases the changes are inevitable, although attenuated.\n4.3 Noisy approximation model\nWith the simple example out of the way, we can now construct a lower bound on the distance of generation ndistribution\nfrom the original and show that without superlinear sampling it similarly diverges in the limit of large n. A nice property\nof Wasserstein-2 distance is that Gaussians provide a universal lower bound for the Wasserstein distance [Gelbrich,\n1990]. In particular, for \u03baand\u03bdprobability measures on a Euclidean N-dimensional space with \u00b5\u03baand\u00b5\u03bdmeans, \u03a3\u03ba\nand\u03a3\u03bdcovariance matrices, we have that\nW2\n2(\u03ba, \u03bd)\u2265 \u2225\u00b5\u03ba\u2212\u00b5\u03bd\u22252+ Tr\u0012\n\u03a3\u03ba+ \u03a3v\u22122\u0010\n\u03a31\/2\n\u03ba\u03a3v\u03a31\/2\n\u03ba\u00111\/2\u0013\n\u2265 \u2225\u00b5\u03ba\u2212\u00b5\u03bd\u22252\nWith this, instead of quantifying the distance exactly, we can instead lower bound it. The only limitation is that we are\ngoing to have to specify a functional approximation model. In order to achieve a W2bound, we will be required to\nspecify how the mean changes between generations. In the scenario where we only have access to the sample mean, we\nwould approximate the mean of the next generation distribution as Equation (1). However, as more information arrives,\nor the model begins using it better, we may end up diverging from the sample mean. We would still require that the\nmodel have good performance, i.e.on average the mean estimate is the same. We will also have to specify expected\nbehaviour of the model over the the variance calculation, which once again will be chosen such that it averages out.\nThus, we will adopt the following evolution over generations:\n\u00b5i+1=1\nMiX\njXi\nj+\u03b5i+1=\u03a31\/2\ni\u221aMiTi+1+\u00b5i+\u03b5i+1;EXi\nj(\u03a3i+1) = \u03a3 i (6)\nwhere we define Ti+1to satisfy the equation above, i.e.Ti+1=\u03a3\u22121\/2\ni\u221aMiP\nj\u0000\nXi\nj\u2212\u00b5i\u0001\n. With this normalisation Thas\nmean 0and covariance INand by the central limit theorem (CLT) we would have Ti+1|\u00b5i,\u03a3iD\u2192 N (0, IN), however\nthe lower bound will not rely on this. To arrive at a lower bound for the risk, similar to that of Equation (4), we are\ngoing to have to make a few assumptions about the form of \u03b5i+1.\nAssumptions :\n8\nModel Collapse\n1. On average we can capture the mean to be the same as at the iteration prior:\nE[\u03b5i+1|\u00b5i,\u03a3i] = 0 (7)\n2. Given all of Xi\nj, epsilon must be constant, i.e.it is a function of only the data:\n\u03b5i+1=\u03b5i+1\u0000\nXi\nj\u0001\n(8)\nIn particular, it is dependent on \u00b5iand\u03a3ionly through the data.\n3.The extra noise is orthogonal to the sample mean in the sense of random variables. This is effectively assuming\nthat the noise does not contain any first moment information, i.e.we have:\nCov(\u03b5i+1, Ti+1|\u00b5i,\u03a3i) = 0 (9)\nThis may seem like a rather strong assumption, compared to the previous ones, however this property can\nbe shown to hold true when imposing CLT on Ti+1in the limit of large Mi(note here that Mican only be\nassumed to be large , and not infinite) and assuming that \u03b5is strictly a function of moments higher than first.\nFurthermore, a property of this type is necessary to actually provide any information, since prior to it there\nwould be no need to separate into an epsilon term and a sample mean term, since all could be absorbed into \u03b5.\nIn Appendix A.2, we further provide an alternative to Assumption 3, wherein by bounding the size of noise we are able\nto recover a similar bound, but only as an expansion in 1\/Mi.\nWith all the assumptions in place, we now have the following bound:\nE\u0002\nRi+1\nW2\u0003\n\u2265E\u0000\n\u2225\u00b5i+1\u2212\u00b5\u22252\u0001\n(10)\n=E\u0000\n\u2225\u00b5i\u2212\u00b5\u22252\u0001\n+E\u0000\n\u2225\u03b5i+1\u22252\u0001\n+1\nMiE\u0000\n(Ti+1)\u22a4\u03a3i(Ti+1)\u0001\n+ (11)\n+2\u221aMiE\u0010\n(\u03b5i+1)\u22a4\u03a31\/2\niTi+1+ (\u00b5i\u2212\u00b5)\u22a4\u03a31\/2\niTi+1\u0011\n(12)\n=E\u0000\n\u2225\u00b5i\u2212\u00b5\u22252\u0001\n+Tr \u03a3\nMi+E\u0000\n\u2225\u03b5i+1\u22252\u0001\n+2\u221aMiE\u0010\n(\u03b5i+1)\u22a4\u03a31\/2\niTi+1\u0011\n(13)\nNow the only quantity to evaluate is\n2\u221aMiE\u0010\n(\u03b5i+1)\u22a4\u03a31\/2\niTi+1\u0011\n=2\u221aMiZ\nd\u03a3ip(\u03a3i) Trh\n\u03a31\/2\niCov(\u03b5i+1, Ti+1|\u03a3i)i\n= 0, (14)\nby Assumption 3. Therefore, the overall bound would be similar to the Gaussian case, but with extra noise variance\nterms:\nE\u00b5n+1,\u03c32\nn+1\u0002\nRn+1\nW2\u0003\n\u2265Tr \u03a3\u00121\nM0+1\nM1+\u00b7\u00b7\u00b7+1\nMn\u0013\n+n+1X\ni=1E\u0000\n\u2225\u03b5i\u22252\u0001\n(15)\nAs a result, we have shown that the same superlinear scaling would be required to minimise the lower bound on model\ncollapse even in the case of more generic models of approximation, in which the mean at step i+ 1can be separated\northogonally into the sample mean and \u2019extra\u2019.\nOverall, the message of this section can be summarised as follows:\nWhen learning on generational data, due to finite sampling, we are only able to approximate the original distribution.\nWhile on average we should recover the original distribution, the variance arising from this is non-zero. As a result,\nover the generations, the average distance of n\u2019th generation from the original grows and can become infinite in the\nlimit since errors compound over time.\n5 Evaluation\n5.1 Training from scratch with GMMs and V AEs\nGaussian Mixture Models. In this subsection we evaluate the performance of Gaussian Mixture Models (GMM)\n[Reynolds et al., 2009]. The underlying task here is that a given GMM tries to separate two artificially-generated\nGaussians. Figure 7 shows the progression of the GMM fitting process over time. The left-most plot shows the\noriginal two Gaussians with the ground truth labels. The next plot shows the GMM fitted on the original data with\nno cross-generational data used i.e.\u03b1i=\u03b3i= 0, where the error is minimal. Yet, within 50 iterations of re-sampling\nwe arrive to a point where the underlying distribution is mis-perceived. The performance worsens over time and by\niteration 2000 we arrive at a point estimate of the distribution with very little variance. The L2 distance between the\noriginal GMM and its descendants is plotted in Figure 13.\n9\nModel Collapse\n2\n 0 23\n2\n1\n0123\nReal Data\n2\n 0 23\n2\n1\n0123\n0\n2\n 0 23\n2\n1\n0123\n50\n2\n 0 23\n2\n1\n0123\n100\n2\n 0 23\n2\n1\n0123\n150\n2\n 0 23\n2\n1\n0123\n200\n2\n 0 23\n2\n1\n0123\n350\n2\n 0 23\n2\n1\n0123\n2000\nFigure 7: An examples of GMM fitting data at iterations {0,50,100,150,200,350,2000}. At first the model fits data\nvery well as is shown on the left; yet even at generation 50 the perception of the underlying distribution completely\nchanges. At generation 2000 it converges to a state with very little variance. GMM is sampled a thousand times.\n(a) Original model\n (b) Generation 5\n (c) Generation 10\n (d) Generation 20\nFigure 9: Random latent reconstructions from V AEs. No training data comes from the original distribution. Over the\ngenerations, different modes of the original distribution get entangled and generated data starts looking unimodal.\n3\n 2\n 1\n 0 1 2 30.00.20.40.60.81.01.21.41.6DensityGeneration 0\nGeneration 1\nGeneration 2\nGeneration 3\nGeneration 4\nGeneration 5\nGeneration 6\nGeneration 7\nGeneration 8\nGeneration 9\nFigure 8: Changing distribution of latents over the learning\nprocess with generated data as perceived by the original\nencoder. Just as with the Gaussian case described above,\nthe tails get washed away and the model arrives at the mean\nrepresentation of the underlying data.Variational Autoencoders. In this subsection we turn to\nVariational Autoencoders (V AE). As before, we train an\nautoencoder on an original data source, which we later\nsample. Here, we generate latents from a Gaussian dis-\ntribution which are then used by the decoder to generate\ndata for the subsequent generation. Figure 9 on the left\nshows an example of generated data using the setting\ndescribed by Kingma and Welling.\nHaving performed the process a number of times we ar-\nrive at a representation that has very little resemblance of\nthe original classes learned from data. On the right, one\nsees the generated images from generation 20, which ap-\npear to be a mix of all of the different digits. Interestingly,\nthe original encoder perceives the generated data from its\ndescendant with ever-growing confidence \u2013 the encoder\nplaces such data closer and closer to the mean. Figure 8\nshows the density of the latent representation of the orig-\ninal model when presented with data generated by its\ndescendants. As with single-dimensional Gaussians, tails\ndisappear over time and all of the density shifts towards\nthe mean.\n5.2 Language Models\nBy now it is clear that Model Collapse is universal across\ndifferent families of ML models. Yet if small models such as GMMs and V AEs are normally trained from scratch,\nLLMs are different. They are so expensive to retrain from scratch that they are typically initialised with pre-trained\n10\nModel Collapse\nReal 123456789\nTrained on dataset from a given generation354045505560Perplexity \u00b1\nReal wikitext2 test dataset\nrun 1\nrun 2\nrun 3\nrun 4\nrun 5\n(a) No data preserved, 5 epochs\nReal 123456789\nTrained on dataset from a given generation323436384042Perplexity \u00b1\nReal wikitext2 test dataset\nrun 1\nrun 2\nrun 3\nrun 4\nrun 5 (b) 10% data preserved, 10 epochs\nFigure 10: Performance of OPT-125m models of different generations evaluated using the original wikitext2 test\ndataset. Perplexity is shown on the y-axis and for each independent run the graph of the mean and its standard deviation\nis shown with error bars. x-axis refers to the generation of the model \u2013 \u2018Real\u2019 refers to the \u2018model 0\u2019 trained on the\noriginal wikitext2 dataset; model 1 was trained on the data produced by model 0; model 2 was trained on data\nproduced by model 1 etc. with all generated datasets equal in size. We find that models trained on generated data are\nable to learn some of the original task, but with errors, as seen from the increase in perplexity.\nmodels such as BERT [Devlin et al., 2018], RoBERTa [Liu et al., 2019], or GPT2 [Brown et al., 2020], which are trained\non large text corpora. They are then fine-tuned to various downstream tasks [Bommasani et al., 2022].\nIn this subsection we explore what happens with language models when they are sequentially fine-tuned with data\ngenerated by other models4. We evaluate the most common setting of training a language model \u2013 a fine-tuning setting\nwhere each of the training cycles starts from a pre-trained model with recent data. Data here comes from another\nfine-tuned pre-trained model. Since training is restricted to produce models that are close to the original pre-trained\nmodel and datapoints generated by the models will generally produce very small gradients, the expectation here may be\nthat the model should only change moderately after fine-tuning. We fine-tune the OPT-125m causal language model\nmade available by Meta through Huggingface [Zhang et al., 2022].\nWe fine-tune the model on the wikitext2 dataset. For data generation from the trained models we use a 5-way\nbeam-search. We block training sequences to be 64 tokens long; then for each token sequence in the training set, we\nask the model to predict the next 64 tokens. We go through all of the original training dataset and produce an artificial\ndataset of the same size. Since we go though all of the original dataset and predict all of the blocks, if the model had\n0.0error it would produce the original wikitext2 dataset. Training for each of the generations starts with generation\nfrom the original training data. Each experiment is ran 5 times and the results are shown as 5 separate runs. The\noriginal model fine-tuned with real wikitext2 data gets 34mean perplexity, from the zero-shot baseline of 115,i.e.it\nsuccessfully learns the task. Finally, to be as realistic as possible, we use the best performing model on the original task,\nevaluated using the original wikitext2 validation set, as the base model for the subsequent generations, meaning in\npractice observed Model Collapse can be even more pronounced.\nHere we consider two different settings:\n5 epochs, no original training data \u2013 Here, the model is trained for 5 epochs on the original dataset and no original\ndata. The overall original task performance is presented in Figure 10.(a). We find that training with generated data\nallows one to adapt to the underlying task, losing some performance \u2013 from 20to28perplexity points.\n10 epochs, 10% of original training data preserved \u2013 Here the model is trained for 10 epochs on the original dataset\nand every new generation of training, a random 10% of the original data points are sampled. The overall original\n4One can easily replicate an experiment described in Section 5.1 with a language model to demonstrate model collapse . Given\nthat training a single moderately large model produces twice the American lifetime worth of CO 2[Strubell et al., 2019], we opted\nto not run such an experiment and instead focus on a more realistic setting for a proof-of-concept. Note that just the language\nexperiments described in the paper took weeks to run.\n11\nModel Collapse\n100101102\nPerplexity of generated datapoints0.00.20.40.6ProbabilityPerplexity of generated datapoints\nevaluated by model trained with\nreal wikitext2\nGeneration 0\nGeneration 1\nGeneration 2\nGeneration 3\nGeneration 5\nGeneration 9\n(a) No data preserved\n100101102\nPerplexity of generated datapoints0.00.10.20.3ProbabilityPerplexity of generated datapoints\nevaluated by model trained with\nreal wikitext2\nGeneration 0\nGeneration 1\nGeneration 2\nGeneration 3\nGeneration 5\nGeneration 9 (b) 10% data preserved\nFigure 11: Histograms of perplexities of each individual data training sequence produced by different generations as is\nevaluated by the very first model trained with the real data. Over the generations models tend to produce samples that\nthe original model trained with real data is more likely to produce. At the same time, a much longer tail appears for later\ngenerations \u2013 later generations start producing samples that would never be produced by the original model i.e.they\nstart misperceiving reality based on errors introduced by their ancestors. Same plots are shown in 3D in Figure 15.\ntask performance is presented in Figure 10.(b). We find that preservation of the original data allows for better model\nfine-tuning and leads to only minor degradation of performance.\nBoth training regimes lead to degraded performance in our models, yet we do find that learning with generated data\nis possible and models can successfully learn (some of) the underlying task. We now turn to consider the underlying\nperception of probable events for each generation of our models.\nFigure 11 shows histograms of individual datapoint perplexities generated by the models of different generations as\nis evaluated by the first model developed with real wikitext2 training data. Here over the generations models tend\nto produce more sequences that the original model would produce with the higher likelihood. The observed effect is\nsimilar to that described for V AEs and GMMs in Section 5.1, where over the generations models started to produce\nsamples that would be produced with higher probabilities by the original model. At the same time, we discover that\ngenerated data has much longer tails, suggesting that some of the data would never be produced by the original model \u2013\nthese are the errors that accumulate because of the learning with generational data .\nWe find that data generated by language models in our experiments end up containing a large number of repeating\nphrases. The repeating problem has been observed in nearly all text generation models [Keskar et al., 2019, Shumailov\net al., 2021] and to rule this out as the cause of Model Collapse , we further provide numerical experiments when models\nare explicitly encouraged to produce non-repeating sequences with repeating penalty of 2.0. We find that this causes the\nmodels to produce lower score continuations to avoid using repeats, which as a result causes the consequent models\nto perform even worse. Figure 14 show model perplexities shift across the generations towards more probable token\nsequences. In particular, enforcing this for the LLM experiments causes the perplexity to double, compared to the\noriginal. Models remain as susceptible to Model Collapse , if not more.\nThe described process demonstrates that fine-tuning of language models does not curb the effects of Model Collapse and\nmodels that are being fine-tuned are also vulnerable. We find that over the generations models tend to produce more\nprobable sequences from the original data and start introducing their own improbable sequences i.e.errors.\n6 Discussion and Conclusion\nWe now discuss the implications of Model Collapse on the underlying learning dynamics of LLMs. Long-term\npoisoning attacks on language models are not new. For example, we saw the creation of click, content, and troll farms \u2013\na form of human \u2018language models\u2019, whose job is to misguide social networks and search algorithms. The negative\neffect these poisoning attacks had on search results led to changes in search algorithms: e.g., Google downgraded\n12\nModel Collapse\nfarmed articles5, putting more emphasis on content produced by trustworthy sources e.g.education domains, while\nDuckDuckGo removed them altogether6.\nWhat is different with the arrival of LLMs is the scale at which such poisoning can happen once it is automated.\nPreserving the ability of LLMs to model low-probability events is essential to the fairness of their predictions:\nsuch events are often relevant to marginalised groups. Low-probability events are also vital to understand complex\nsystems [Taleb, 2007].\nOur evaluation suggests a \u201cfirst mover advantage\u201d when it comes to training models such as LLMs. In our work we\ndemonstrate that training on samples from another generative model can induce a distribution shift, which over time\ncauses Model Collapse . This in turn causes the model to mis-perceive the underlying learning task. To make sure that\nlearning is sustained over a long time period, one needs to make sure that access to the original data source is preserved\nand that additional data not generated by LLMs remain available over time. The need to distinguish data generated\nby LLMs from other data raises questions around the provenance of content that is crawled from the Internet: it is\nunclear how content generated by LLMs can be tracked at scale. One option is community-wide coordination to ensure\nthat different parties involved in LLM creation and deployment share the information needed to resolve questions of\nprovenance. Otherwise, it may become increasingly difficult to train newer versions of LLMs without access to data\nthat was crawled from the Internet prior to the mass adoption of the technology, or direct access to data generated by\nhumans at scale.\nAcknowledgements\nWe want to thank Anvith Thudi, David Glukhov, Peter Zaika, and Darija Barak for useful discussions and feedback.\nReferences\nRahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware\nsynapses: Learning what (not) to forget. In Proceedings of the European conference on computer vision (ECCV) ,\npages 139\u2013154, 2018.\nRahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition , pages 11254\u201311263, 2019.\nBattista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. arXiv preprint\narXiv:1206.6389 , 2012.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo\nCastellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue,\nMoussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn,\nTrevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan\nJurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh,\nMark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec,\nIsabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos\nNiebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris\nPiech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani,\nCamilo Ruiz, Jack Ryan, Christopher R\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan\nSrinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang, Bohan\nWu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang,\nTianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and\nrisks of foundation models, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems , 33:1877\u20131901, 2020.\nNicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. arXiv preprint arXiv:2106.09667 ,\n2021.\n5https:\/\/googleblog.blogspot.com\/2011\/02\/finding-more-high-quality-sites-in.html\n6https:\/\/www.technologyreview.com\/2010\/07\/26\/26327\/the-search-engine-backlash-against-content-mills\/\n13\nModel Collapse\nNicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson,\nAndreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. Poisoning web-scale training datasets is practical. arXiv preprint\narXiv:2302.10149 , 2023.\nW. G. Cochran. The distribution of quadratic forms in a normal system, with applications to the analysis of\ncovariance. Mathematical Proceedings of the Cambridge Philosophical Society , 30(2):178\u2013191, 1934. doi:\n10.1017\/S0305004100016595.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\nAdrian Fischer, Robert E. Gaunt, and Andrey Sarantsev. The variance-gamma distribution: A review, 2023.\nMatthias Gelbrich. On a formula for the l2 wasserstein metric between measures on euclidean and hilbert spaces.\nMathematische Nachrichten , 147(1):185\u2013203, 1990.\nShane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy shaping:\nIntegrating human feedback with reinforcement learning. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahra-\nmani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems , volume 26. Cur-\nran Associates, Inc., 2013. URL https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2013\/file\/\ne034fb6b66aacc1d48f445ddfb08da98-Paper.pdf .\nTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning\nmodel supply chain. arXiv preprint arXiv:1708.06733 , 2017.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional\ntransformer language model for controllable generation, 2019.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in\nneural networks. Proceedings of the national academy of sciences , 114(13):3521\u20133526, 2017.\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine\nintelligence , 40(12):2935\u20132947, 2017.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 ,\n2019.\nAnh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for\nunrecognizable images, 2015.\nOpenAI. Gpt-4 technical report, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In\nInternational conference on machine learning , pages 8748\u20138763. PMLR, 2021.\nDouglas A Reynolds et al. Gaussian mixture models. Encyclopedia of biometrics , 741(659-663), 2009.\nIlia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Papernot, Robert Mullins, and Ross Anderson. Sponge examples:\nEnergy-latency attacks on neural networks. In 2021 IEEE European Symposium on Security and Privacy (EuroS&P) ,\npages 212\u2013231. IEEE, 2021.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec Radford, Gretchen\nKrueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine\nWang. Release strategies and the social impacts of language models, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp.\narXiv preprint arXiv:1906.02243 , 2019.\nNassim Nicholas Taleb. Black swans and the domains of statistics. The American Statistician , 61(3):198\u2013200, 2007.\nGido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734 ,\n2019.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 ,\n2022.\n14\nModel Collapse\nA Appendix\nA.1 Absorbing Markov Chain\nThe subsection explains a well-known fact about absorbing Markov chains, that they converge to an absorbing state\nwith probability one. Assume that Xmform a Markov chain. In order to reason about this chain we need to consider\nthe transition probabilities. In general, these correspond to our functional approximation scheme. Due to the stochastic\nnature of the Markov chain, we expect to have the variance go up and down. But as the variance decreases, the newly\nsampled data, due to its finiteness, will be more concentrated, leading in the limit to a set of i.e.a delta functions. This\nargument assumes that the approximation scheme is good and can converge to delta functions. If not, the errors in\napproximation may prevent the propagation of errors in stochasticity.\nAs discussed in the previous section, we can model the process of repeated \u2018sampling\u2019 and \u2018fitting\u2019 as a Markov chain.\nIn this subsection, we explain how such a process can converge to a stationary state i.e.the absorbing state of a Markov\nChain. In this derivation we follow Allan Yashinski7. Suppose we have an absorbing Markov Chain with rtransient\nstates t1, . . . , t randsabsorbing states a1, . . . , a s. The whole Markov chain has r+sstates, ordered as follows:\nt1, . . . , t r, a1, . . . , a s. The transition matrix is then defined as\nT=\u0014\nQ0r\u00d7s\nR I s\u0015\n, (16)\nwhere\n\u2022Qis anr\u00d7rmatrix holds the probabilities of moving from a transient state to another transient state\n\u2022Ris ans\u00d7rmatrix which holds the probabilities of moving from a transient state to an absorbing state.\n\u20220r\u00d7sis the r\u00d7smatrix of all 0\u2019s. There 0\u2019s represent the probabilities of moving from an absorbing state to a\ntransient state (which is impossible by definition).\n\u2022Isholds the probabilities of transitioning between the absorbing states. As transition is impossible, this is just\nthes\u00d7sidentity matrix.\nWe are interested in limk\u2192\u221eTk(X0). For a given k, the matrix becomes\nTk=\u0014\nQk0r\u00d7s\nR+RQ+\u00b7\u00b7\u00b7+RQk\u22121Is\u0015\n=\u0014Qk0r\u00d7s\nRPk\u22121\ni=0QiIs\u0015\n. (17)\nFinally, for an absorbing Markov chain with T=\u0014\nQ0r\u00d7s\nR I s\u0015\n,\nwe have limk\u2192\u221eTk=\u0014\n0r\u00d7r 0r\u00d7s\nR(Ir\u2212Q)\u22121Is\u0015\n.\nSince in the limit the transition probabilities to transient states are zero, we end up converging to absorbing states and\nstaying there. In the case of discrete distributions, where we can perfectly approximate a zero-variance dataset ( i.e.a\ndelta function), the absorbing states are delta functions centered at any non-zero probability point from the original\ndistribution. In practice, we would like to know the expected number of steps before being absorbed, which may be\nlarge. But without knowing our fitting procedure it is impossible to calculate the matrix Qand therefore the average\nlength of time before collapse.\nA.2 Alternative assumption for noisy approximations\nThis subsection will cover an alternative assumption, which may be more realistic in some settings, in contrast to\nassumption 3 from Section 4.3, and this subsection mostly acts as an extension, rather than an alternative. In particular,\ninstead of imposing orthogonality, we can instead impose a certain size requirement on the noise term. This in turn\nallows us to arrive to a similar result.\nTo be more precise, we will consider the same setting as in Section 4.3, but we will now replace Assumption 3 with\nAssumption 3*:\n7www.math.umd.edu\/~immortal\/MATH401\/book\/ch_absorbing_markov_chains.pdf\n15\nModel Collapse\n101102103104105106107\nlog(number of samples)104\n103\n102\n101\nlog(| |)\n estimation of a (=0,=1)\nFigure 12: Approximation of a single-dimensional Gaussian N(0,1)as a function of number of points. The mean\nestimator and its standard deviation are calculated from running the procedure 10000 times.\n0 500 1000 1500 2000\nGeneration106\n104\n102\n100102log(||GMM0,GMMevolution||2)Distance between the original GMM and its approximation\n as function of a number of data samples\n500\n1000\n10000\n50000\n200000\nFigure 13: Progressive fitting of a GMM with different number of samples. On the y-axis is shown the logarithm of L2\ndistance between the two GMM distributions. Over the generations the distance begins to grow and can become quite\nlarge. The jumps in the distance for large sample sizes occur due to the fixed number of iterations and precision for the\nexpectation maximization algorithm.\n16\nModel Collapse\nAssumptions :\n3*.The extra noise is going to be assumed to be bounded and of the order larger than the sample mean deviation.\nTo be precise we will have a constant K(not dependent on generation i), such that for all i:\n\u2225\u03b5i+1\u2225 \u2264K\nMi(18)\nNow with the alternative assumption in place, we can follow the exact same calculations to arrive at\nE\u0002\nRi+1\nW2\u0003\n\u2265E\u0000\n\u2225\u00b5i\u2212\u00b5\u22252\u0001\n+Tr \u03a3\nMi+E\u0000\n\u2225\u03b5i+1\u22252\u0001\n+2\u221aMiE\u0010\n(\u03b5i+1)\u22a4\u03a31\/2\niTi+1\u0011\n(19)\nSimilar to before, we need to evaluate (which we instead bound this time):\n2\u221aMiE\u0010\n(\u03b5i+1)\u22a4\u03a31\/2\niTi+1\u0011\n=2\u221aMiZ\nd\u03a3ip(\u03a3i) Trh\n\u03a31\/2\niCov(\u03b5i+1, Ti+1|\u03a3i)i\n\u0338= 0 (20)\n\u2265 \u22122\u221a\nN\u221aMiZ\nd\u03a3ip(\u03a3i)q\nTr\u0002\n\u03a3i\u03a3\u03f5i+1\u0003\n(21)\n\u2265 \u22122\u221a\nN\u221aMiq\nE\u0000\n\u03b5\u22a4\ni+1\u03a3i\u03b5i+1\u0001\n, (22)\n\u2265 \u22122\u221a\nN\u221aMis\nK2Tr \u03a3\nM2\ni=\u22122K\u221a\nN\nMi\u221aMi\u221a\nTr \u03a3, (23)\nwhere we used the Cauchy-Schwarz and Jensen inequalities. Note that this is far from optimal inequality, since instead\nof using the expected value of the largest eigenvalue, we instead bounded it by Tr \u03a3 . In particular, the per step bound is\nthen:\nE\u0002\nRi+1\nW2\u0003\n\u2265E\u0000\n\u2225\u00b5i\u2212\u00b5\u22252\u0001\n+Tr \u03a3\nMi+E\u0000\n\u2225\u03b5i+1\u22252\u0001\n\u22122K\u221a\nN\nMi\u221aMi\u221a\nTr \u03a3. (24)\nWithout knowledge of the specific values of K,NorTr \u03a3 , the best we can do is consider what this means for the bound\nasMibecomes large. In particular, contribution from the last two terms will be of order at most 3\/2. As a result we\nrecover a bound similar to all of the ones observed so far:\nE\u00b5n+1,\u03c32\nn+1[RW2]\u2265Tr \u03a3\u00121\nM0+1\nM1+\u00b7\u00b7\u00b7+1\nMn\u0013\n+O(3\/2) (25)\nIn particular, we find in the same way, that superlinear scaling would be required to minimise the lower bound on model\ncollapse even in the case of more generic models of approximation, in which the mean at step i+ 1can be separated\ninto the sample mean and an extra bounded term of order at most 1\/Mi.\n17\nModel Collapse\n101\nPerplexity of generated datapoints0.00.10.20.30.40.5ProbabilityPerplexity of generated datapoints\nevaluated by model trained with\nreal wikitext2\nGeneration 0\nGeneration 1\nGeneration 2\nGeneration 3\nGeneration 5\nGeneration 9\n(a) Overlaid histograms\nGeneration\n0246810\nPerplexity\n0246810Probability (b) 3D view\nFigure 14: Histogram of perplexities of each individual data training sequence produced by different generations as is\nevaluated by the very first model trained with the real data. Over the generations models tend to produce samples that\nthe original model (trained with real data) is more likely to produce. At the same time, a much longer tail appears for\nlater generations \u2013 later generations start producing samples that would never be produced by the original model i.e.they\nstart misperceiving reality based on errors introduced by their ancestors. Models here are explicitly forced to not repeat\nsequences with a penalty of 2.0.\nGeneration\n0246810\nPerplexity\n0246810Probability\n(a) Figure 11.a in 3D. No data preserved.\nGeneration\n0246810\nPerplexity\n0246810Probability (b) Figure 11.b in 3D. 10% original data preserved.\nFigure 15: Histogram of perplexities of each individual data training sequence produced by different generations as is\nevaluated by the very first model trained with the real data. Over the generations models tend to produce samples that\nthe original model (trained with real data) is more likely to produce. At the same time, a much longer tail appears for\nlater generations \u2013 later generations start producing samples that would never be produced by the original model i.e.they\nstart misperceiving reality based on errors introduced by their ancestors.\n18","metadata":{"primary_category":"cs.LG","published":"20230527","title":"The Curse of Recursion: Training on Generated Data Makes Models Forget","updated":"20230531"}}
{"id":"2205.09712","source":"http:\/\/arxiv.org\/pdf\/2205.09712","text":"2022-5-20\nSelection-Inference: Exploiting Large\nLanguage Models for Interpretable Logical\nReasoning\nAntonia Creswell1, Murray Shanahan1and Irina Higgins1\n1DeepMind\nLarge language models (LLMs) have been shown to be capable of impressive few-shot generalisation\nto new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems.\nHere we carry out a comprehensive evaluation of LLMs on 50 tasks that probe di\ufb00erent aspects of\nlogical reasoning. We show that language models tend to perform fairly well at single step inference\nor entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex\nproblems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained\nLLMs as general processing modules, and alternates between selection and inference to generate a\nseriesofinterpretable,casualreasoningstepsleadingtothe\ufb01nalanswer. Weshowthata7Bparameter\nLLM used within the SI framework in a 5-shot generalisation setting, with no \ufb01ne-tuning, yields a\nperformance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10\nlogical reasoning tasks. The same model in the same setting even outperforms a signi\ufb01cantly larger\n280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework\nareaccompaniedbya causalnatural-language-basedreasoningtrace,whichhasimportantimplications\nfor the safety and trustworthiness of the system.\n1. Introduction\nLarge language models (LLMs) are powerful few-shot learners (Bommasani et al., 2021; Brown et al.,\n2020;Luetal.,2021). However, oneareawheretheytendtoperformpoorlyislogicalreasoning(Rae\net al., 2021). Yet the ability to perform multi-step, logically valid reasoning is fundamental for the\ndiscoveryofnewknowledgeandexplainability. Itunderpinsmanyadvancementsthathavebeenmade\nin science, medicine, maths and philosophy. It is also one of the most valued strengths of classical,\nsymbolic AI over contemporary deep learning methods (Bengio et al., 2021; Marcus, 2020; Marcus\nandDavis,2019),promptingtherecentincreaseintheuseofneurosymbolicapproachestobridgethis\ngap (Garcez and Lamb, 2020; Garnelo and Shanahan, 2019). Here we propose a Selection-Inference\n(SI) framework that takes inspiration from the neurosymbolic literature to improve the ability of\nLLMs to do logically valid reasoning.\nThere are many \ufb02avours of neurosymbolic models (Garcez and Lamb, 2020). Those from which\nwe draw inspiration tend to have a modular structure, where each module is specialised for one type\nof operation (Andreas et al., 2016; Mao et al., 2019). For example, such modules may be neural\nnetworksorhand-craftedfunctionsdesignedtoattendtoasingleobject,ortocomparethelocationor\nsizeoftwoinputs(Andreasetal.,2016;Yietal.,2018). Neurosymbolicmodelscanproduceananswer\nto a complex query by chaining these operations together, passing inputs from one module to another.\nThis has the bene\ufb01t of producing an interpretable trace of intermediate computations, in contrast\nto the \u201cblack-box\u201d computations common to end-to-end deep learning approaches. Importantly, the\nmodularity of neurosymbolic methods allows them to generalise to signi\ufb01cantly harder problems\nthat require long chains of reasoning (Hudson and Manning, 2019). However, the hand-crafted and\nspecialised nature of the modules often makes the resulting systems brittle, hard to optimise, and\nCorresponding author(s): tonicreswell@deepmind.comarXiv:2205.09712v1  [cs.AI]  19 May 2022\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nFigure 1jSelection-Inference (SI) framework (c) in comparison with the vanilla baseline (a) and\nChain-of-Thought, COT, (b) approach (Wei et al., 2022). (a): The vanilla large language model\nbaselinetakesinconcatenated[ context,question,answer]\u0002\ud835\udc58fork-shotprompting,followedby[ context,\nquestion] and is asked to generate the answer. All reasoning is done implicitly; (b): COT (Wei et al.,\n2022) inspired baseline takes in concatenated [ context,question,reasoning ,answer]\u0002\ud835\udc58for k-shot\nprompting, followed by [ context,question] and is asked to generate the [ reasoning ,answer];(c):\nSI framework consists of two steps. The selection step takes in concatenated [ context,question,\nselection]\u0002\ud835\udc58for k-shot prompting, followed by [ context,question] and is asked to select a subset of\nfacts from the context to support a single step of reasoning . The inference step takes in [ selection,\ninference]\u0002\ud835\udc58for k-shot prompting, followed by the selectionproduced by the Selection module to\nproduce a new fact (the inference) to be added to the context. Each combination of [selection +\ninference + add fact to context] makes up one step of reasoning. These can be chained together to\nanswer harder problems. The \ufb01nal inference is taken to be the answer.\ndi\ufb03cult to extend to new domains (Yi et al., 2018).\nOur SI framework, drawing on best practice from neurosymbolic approaches, decomposes logical\nreasoning into two modular stages: 1) selection, which involves choosing a subset of relevant infor-\nmation su\ufb03cient to make a single step of inference, and 2) inference, which only sees the limited\ninformationprovidedbytheselectionmodule,andusesittoinferanewintermediatepieceofevidence\non the way to the \ufb01nal answer (see Fig. 1c). We implement both stages using pre-trained LLMs which,\nthanks to their powerful few-shot generalisation capabilities, serve as more general alternatives to the\nhand-crafted, specialised modules typically used in neurosymbolic approaches. In the SI framework,\nmultiple steps of selection and inference are chained together to produce a sequence of reasoning\nsteps. As well as underpinning better performance on reasoning problems, this yields an interpretable\ntrace that justi\ufb01es the \ufb01nal answer.\nFurthermore, the reasoning trace produced by our system is causal, in the sense that each step\nfollowsfrom,anddependson,thepreviousstep. Eachinferencestepismadeinisolation,basedsolely\non the limited information provided by the Selection module, without direct access to the question or\nto previous steps of reasoning. This contrasts with the more common approach of obtaining post-hoc\nrationalisation ,wheretheanswerproducedbythemodelhasnodirectdependenceontheexplanation,\nsince the explanation is produced either in parallel to the answer or after the fact (Cobbe et al., 2021;\nLampinen et al., 2021; Saha et al., 2020). A notable example that sits in the grey area between\npost-hoc rationalisation approaches and the more causal explanation approaches is Chain-Of-Thought\n(COT) (Wei et al., 2022) (see Fig. 1b). In this approach LLMs are encouraged to produce a reasoning\ntrace before the answer. However the dependence of the answer on the reasoning is not explicitly\n2\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\n(a) Correct reasoning on bAbI deduction (left) and\ninduction (right) tasks.\n(b) SI can recover from an error (left) and justify an\nambiguous answer with a reasoning trace (right).\nFigure 2jQualitative results from the Selection-Inference (SI) model on bAbI tasks.\nencouragedtobecausal(asde\ufb01nedabove). Indeed,theauthorsshowthatwhiletheCOTexplanations\nhelp boost the \ufb01nal answer accuracy, the reasoning traces produced by the model are often wrong\neven when the \ufb01nal answer is correct (see the Supplementary Materials of (Wei et al., 2022) for\nexamples). Developing a system that can demonstrate how it reaches its answers using a causal\nreasoning trace has important bene\ufb01ts in terms of safety, explainability, interpretability, debugging,\nand trust. In this paper we make the following contributions:\n1.We provide a comprehensive evaluation of LLMs on a set of 50 tasks probing di\ufb00erent aspects\nof logical reasoning, and show that LLMs are good at simpler single step logical inference in\n5-shot generalisation settings, but struggle with harder problems (Sec. 3)\n2.We introduce the Selection-Inference (SI) framework, a modular, iterative approach to solving\nreasoning problems (Sec. 4).\n3.We demonstrate the utility of the SI framework by evaluating a 7B parameter LLM from the\nGopher family (Rae et al., 2021) on 10 logical reasoning tasks, showing overall that it almost\ntriples the performance of the same model used naively and almost doubles the performance of\nthe same model used in the COT framework. Moreover, it often outperforms a 40x larger 280B\nLLM baseline used both naively and in the COT framework.\n4.We illustrate further bene\ufb01ts of the SI framework in terms of the causal and interpretable\nreasoning traces produced (Sec. F.1). These traces can help humans understand how the model\nreached its \ufb01nal answer, which is useful for debugging and opens the system\u2019s decisions to\nhuman critique.\n2. Related Work\nOur Selection-Inference framework sits at the intersection of classical, symbolic AI and deep learning.\nA typical symbolic AI system might consist of a knowledge base, which is typically hand-curated by\nexperts, and an inference engine that allows the system to perform logic-based reasoning over its\nknowledge base. For example, such a system could apply step-by-step reasoning to answer a complex\nquestion such as \u201cWhat are the apothecary\u2019s incentives and disincentives for selling poison to Romeo\nin Romeo and Juliet?\u201d (Lenat, 2019) - something that even the best contemporary deep learning\nbased systems struggle to do.\nOneoftheprimarybene\ufb01tsofsymbolicAIsystemsoverdeeplearningmodelsistheirinterpretabil-\n3\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nity; we can look at the reasoning steps such a system has taken to see how the \ufb01nal conclusion was\nreached. However, unlike deep learning approaches, symbolic AI systems require knowledge to be\nhand-crafted and are in general hard to scale. Although some approaches have attempted to bridge\nthe gap between deep learning and symbolic AI by converting problems into formal logic (Nye et al.,\n2021b) and using existing solvers to help produce an answer, this process can be brittle and tends not\nto scale well. Another attempt to bridge the gap comes from the neurosymbolic perspective (Gupta\net al., 2019; Hudson and Manning, 2019; Mao et al., 2019; Yi et al., 2018). These models combine\nthe best parts of deep learning \u2013 learning knowledge from data \u2013 and symbolic AI \u2013 producing an\ninterpretable reasoning trace. However, they are typically quite brittle due to the hand-crafted (Gupta\net al., 2019), specialised nature (Mao et al., 2019) of the modules and optimisation di\ufb03culties.\nOn the deep learning side, recent work has attempted to adapt large pre-trained language models,\nLLMs, to the task of logical reasoning. At a high level these can be split into three groups: 1)\napproaches that try to \ufb01ne-tune LLMs to produce the \ufb01nal answer directly, keeping reasoning implicit\n(Betz et al., 2020; Clark et al., 2020) (e.g. Fig. 1a); 2) approaches that encourage LLMs to produce\nreasoning explicitly, but all reasoning steps are produced in one generative step (Cobbe et al., 2021;\nDalvi et al., 2021; Jhamtani and Clark, 2020; Nye et al., 2021a; Wei et al., 2022; Zelikman et al.,\n2022) (e.g. Fig. 1B); and 3) approaches that use LLMs to produce each reasoning step one at a time\n(Tafjord et al., 2020). The latter is where our Selection-Inference framework sits (Fig. 1C). In general\nit was found that the approaches that incorporate explicit reasoning work better than those that only\ntry to predict the \ufb01nal answer. However, although explicit reasoning helps improve the accuracy of\nthe models, encouraging the models to produce multiple steps of reasoning in a single generative\npass is not enough to make the models use reasoning in a causal manner. The authors found that\nthe generated reasoning traces often contain unrelated or incorrect steps while still resulting in\nthe correct answer (see examples in the appendices of (Wei et al., 2022; Zelikman et al., 2022)).\nEncouragingLLMsto generate eachreasoning steponeata time(Tafjord etal.,2020)is currentlythe\nmost promising direction for achieving causal reasoning, and it is the approach we take in our paper.\nWhile the model proposed by Tafjord et al. (2020) is very impressive, it only works for \u201cProve this\nstatement to be True\/False\u201d style questions, since it relies on enumerating all possible implications\nand checking whether the question statement or its negation are present in the inferred facts, which\nis also computationally expensive.\n3. How Well Do Large Language Models Reason?\nPastworkhasshownthatLLMsarepooratlogicalreasoning(Raeetal.,2021),howevertheevaluation\nwasdoneonarelativelysmallsetoftasks,andwasnotsystematic. Inparticular,hereweareinterested\nin 1) how LLMs perform on simple entailment tasks compared to multi-step reasoning problems and\n2) how scaling laws \u2013 suggested by Rae et al. (2021) \u2013 apply to logical reasoning. To this end, we\nevaluated LLMs from the Gopher family in a 5-shot1setting on a larger set of 50 tasks that touch on\ndi\ufb00erent aspects of logical reasoning and vary in terms of the number of reasoning steps required,\npresenceorabsenceofnegation,whethertherelevantcontextinformationwasprovided,andwhether\nthe model is required to evaluate the accuracy of multiple choices or generate the answer among\nothers. The additional tasks were collected from six sources: bAbI (Weston et al., 2015), BigBench\n(Ghazal et al., 2017), AAC (Betz et al., 2020), Jeopardy (Tunguz, 2019), ProofWriter (Tafjord et al.,\n2020) and 2WikiMultiHop (Welbl et al., 2018) (see Fig. S5a in Supplementary Information for raw\nresults).\nOuranalysisfoundthatLLMsaregoodatsomeaspectsoflogicalreasoning(Fig.3b). Inparticular,\n1We chose 5-shot setting because it was used in (Rae et al., 2021), and because (Min et al., 2022) have demonstrated\nthat additional shots beyond 4 result in limited increase in multi-choice accuracy\n4\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\n(a)Scalinglawsfornaturallanguagetasks(bigbench,\ndark purple line, squares, 56 tasks) and tasks involv-\ning logical reasoning (logic, light purple line, circles,\n38 tasks). All accuracy results are calculated relative\nto the random baseline (0% accuracy means chance\nlevel). Only multi-choice tasks are used.\n(b) Language models perform well for simple en-\ntailment tasks (AAC tasks, Entailed Polarity), their\nperformance starts to get worse on single step infer-\nence problems (bAbI task 1, ProofWriter tasks 0-1),\nand they struggle with more complex multi-step rea-\nsoning problems (2WikiMultiHop tasks, bAbI tasks\n2-3, ProofWriter tasks 2-5, StrategyQA).\nFigure 3jVanilla language models perform poorly on multi-step logical reasoning tasks.\nthey appear to be good at simple entailment and implication tasks (e.g. see AAC tasks and Entailed\nPolarity in Fig. S5a). This appears to hold when negation is present (AAC Split Extended tasks),\nand both in generative (AAC Split) and multiple-choice scoring settings (AAC Split Extended tasks,\nEntailedPolarity). However, theperformanceofvanillalanguagemodelstendstodecreasewhenthey\nget presented with irrelevant facts alongside the ones relevant for reasoning (e.g. see 2WikiMultiHop\nWith Context tasks, bAbI tasks 2-3 or ProofWriter tasks), when they have to infer the relevant facts\nfrom memory (e.g. 2WikiMultiHop or StrategyQA tasks), and as the questions start to require more\nsteps of reasoning (e.g. see the performance drop between bAbI tasks 1-3 or ProofWriter Tasks).\nIn line with Rae et al.\u2019s \ufb01ndings, our results con\ufb01rmed that LLMs of larger sizes do perform better\nthan the smaller models. However, we found that even the largest 280B model performed only 13.6%\nabove chance level on average across the 38 available multi-choice logic tasks (see Figs. S5a-S5b\nand Sec. H in Supplementary Information for more details). Furthermore, we found that logical\nreasoning tasks were qualitatively di\ufb00erent from other natural language tasks. The scaling law for\nlogic-based tasks within the Gopher family models was signi\ufb01cantly worse than for other language\ntasks measured here as the average performance on the subset of BigBench tasks from (Rae et al.,\n2021) with the logic tasks used in this paper removed (see Fig. 3a).\n4. The Selection-Inference (SI) Framework\nWe are interested in solving logical reasoning problems expressed in natural language. In this work\nwe assume that each question is accompanied by context information (see Figs. 1 and 2a), which\ncontainsalltheinformationnecessarytosolvetheproblem,aswellaspotentiallyirrelevantdistractors.\nIn the future this assumption can be relaxed, for example by extracting the necessary information\n5\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nthrough search (Lazaridou et al., 2022; Menick et al., 2022). We also assume that all questions are\nwell posed and de\ufb01nitively answerable given the context.\nLogical reasoning problems require using existing information to infer new relevant knowledge\nnecessary to answer the question. This can be done through deduction, induction or abduction,\nalthoughthedatasetsweuseherecontainmostlydeductiveandasmallnumberofinductiveproblems2.\nSome problems require multiple steps of inference, where later steps use the knowledge inferred in\nthe earlier steps. Hence, we use an iterative framework where at each step the SI uses information in\nthe existing context, C\ud835\udc61, to infer a new fact, \ud835\udc53\ud835\udc61, which is appended back to the context to create new\ncontext,C\ud835\udc61\u00b81=C\ud835\udc61[\ud835\udc53\ud835\udc61. This process can then iterate until the solution to the question is found. In the\ncurrent implementation of the SI framework, we repeat the process for a \ufb01xed number of steps and\ntake the \ufb01nal inference to be the answer. Addressing the issue of halting is left for future work.\nInspired by neurosymbolic methods, we additionally split each step of reasoning into further two\ncomponents: 1) Selection, which selects a subset of the information present in the context, \ud835\udc60\ud835\udc61, given\nthe context and the question, C\ud835\udc61[\ud835\udc5e. This selection, \ud835\udc60\ud835\udc61is fed to the next step, 2) inference, which\nproduces the new fact, \ud835\udc53\ud835\udc61, based on the information passed to it by the selection step. Examples of\nselection and inference are shown in Fig. 2a. This splitting of each step of reasoning into selection\nand inference is the main contribution of our paper, and is important for several reasons. First, and\nmost importantly, it makes the resulting reasoning causal, since both steps have limited capabilities\nby design, and are interdependent. The selection step is constrained to only use the information\navailable in the context, an the inference step only sees the subset of facts provided by the selection\nwithout access to the question. Hence, the model is unlikely to make up information to answer the\nquestion, and it cannot ignore reasoning when producing the \ufb01nal answer. The other bene\ufb01t of this\napproach is that each step of reasoning is broken down into even smaller sub-tasks, which are easier\nfor LLMs to adapt to, and which helps make the reasoning more generalisable to harder problems.\nIn this paper we use pre-trained, frozen language models from the Gopher family (Rae et al.,\n2021) in a 5-shot generalisation setting using prompt engineering to implement the Selection and\nInference modules. We settled on prompt engineering to evaluate the base utility of the SI framework,\nhowever it can also be used in the \ufb01ne-tuning setting which we explore brie\ufb02y in Sec. 6. We next\ndescribe the Selection and Inference modules in more detail.\n4.1. Selection Module\nWe use prompt engineering to encourage the model to output the correct selection, \ud835\udc60\ud835\udc61. The n-shot\nprompt is a string of the the following form:\n\"\"\"\n# n-shot prompt\n# First example.\n<context 1>\n<question 1>\n# Example selection\n<fact>. We know that <fact>[ and <fact>]*. Therefore,\n...\n# Problem to solve.\n<context>\n<question>\n\"\"\"\n2See Fig. 2 for an example of deduction and induction problems used in this paper.\n6\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nwhere<fact>sarecopieddirectlyfromthe context ,and[ and <fact>]* meansthatthemodule\nis allowed to select more than one fact for each step of inference, where the total number of facts is a\nhyper-parameter.\nThe simplest option to implement the selection is to feed this prompt directly to a pre-trained\nLLM and take the output generated by the language model. However, this unconstrained approach\nmay allow the model to make up facts, thus removing the causal aspect of the reasoning trace. Indeed\nduring experimentation this is what we often found. So instead we use the pre-trained LLM to score\neachofthefactsinthecontext, andselecttheonewiththehighestlog-likelihood. Wecanthenrepeat\nthis process by appending each new fact to the end of the previous prompt until the full selection\nis constructed. Note that for now we haltafter a \ufb01xed number of steps. See Algorithm 2 for more\ndetails.\n4.2. Inference module\nThe n-shot prompt for the Inference module has the following form (shown below, also see Fig. 1):\n\"\"\"\n#n-shot inference prompt\n# First example.\n<fact>. We know that <fact>[ and <fact>]*. Therefore, <new fact\n>.\n...\n# Problem to solve.\n<output of the Selection module>. Therefore,\n\"\"\"\nThe n-shot prompt and the output of the Selection module, are fed to the pre-trained LLM serving\nas the Inference module. The \ufb01rst generated sentence (extracted from the generated text as per\nBigBench evaluation (Ghazal et al., 2017) pipeline, see Supplementary Materials for details) is taken\nto be the newly inferred fact. This fact is added to the context, which concludes one reasoning step of\nthe SI framework. For now, we halt after a \ufb01xed number of steps. See Algorithm 1 for more details.\nAlgorithm 1 Selection-Inference\nRequire: An n-shot selection prompt, \ud835\udc5d\ud835\udc60\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61.\nRequire: An n-shot inference prompt, \ud835\udc5d\ud835\udc56\ud835\udc5b\ud835\udc53\ud835\udc52\ud835\udc5f.\nRequire: Initial Context, C0,made up of facts (and rules).\nRequire: The question, \ud835\udc5e.\nRequire: Language model, LLM.\nRequire: A halting function, halt, determines if the answer has been\nreached.\n\ud835\udc61=0 \u22b2Start at step 0.\nwhilenot halt() do\n\ud835\udc60\ud835\udc61 Selection_Module \u00b9\ud835\udc5d\ud835\udc60\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\u0094\ud835\udc36\ud835\udc61\u0094\ud835\udc5e\u0094LLM\u00ba \u22b2Do selection.\n\ud835\udc56\ud835\udc61 Inference_Module \u00b9\ud835\udc5d\ud835\udc56\ud835\udc5b\ud835\udc53\ud835\udc52\ud835\udc5f\u0094\ud835\udc60\ud835\udc61\u00ba \u22b2Do inference.\n\ud835\udc36\ud835\udc61\u00b81 \ud835\udc36\ud835\udc61[\ud835\udc56\ud835\udc61\u22b2Add the newly inferred fact to the context.\n\ud835\udc61 \ud835\udc61\u00b81 \u22b2Move onto the next step of reasoning\nend while\nreturn\ud835\udc60\ud835\udc61\n7\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\n5. Experiments and Results\nWe evaluate our SI framework on a subset of 10 \/50 logical reasoning tasks introduced in Sec. 3.\nThese tasks were chosen based on whether they include context information necessary to answer\nthe question, whether the questions have a de\ufb01nitive answer, and to ensure that they cover di\ufb00erent\nkinds of reasoning abilities. The tasks include bAbI (Weston et al., 2015) Tasks 1-3, which require\nthe model to use 1-3 supporting time-ordered facts respectively to answer a question, and Tasks\n15-16, which test deductive and inductive reasoning respectively. We also evaluate our model on the\nProofWriter OWA datasets (Tafjord et al., 2020) of depth 0, 1, 2, 3 and 5 (there is no depth 4 task).\nThese are language-based logical reasoning problems, where the depth is the number of reasoning\nsteps required to answer the question.\nTo baseline the performance of the SI framework, we consider a 7B (same size as the LLM used\nin the SI framework) and a 40x larger 280B parameter LLM evaluated in a 5-shot setting. There\nare two types of evaluation for these vanilla baselines that we consider: multi-choice andgenerative\nevaluation. In generative evaluation, we measure the exact string match (\ufb01rst sentence in lower case\nand ignoring any non-alphabetic characters) between the output generated by the LLM and the\nground truth answer. This is appropriate, since most of the tasks that we consider require either a\nsingle word answer, or the dataset is such that the answers are highly structured. In multi-choice\nevaluation the LLM is used to score each of the answer choices, as in Li et al. (Li et al., 2021). In\ngeneral LLMs perform signi\ufb01cantly better in a multi-choice vsgenerative evaluation setting, since the\nchance level in the multi-choice setting is signi\ufb01cantly higher.\nWe also consider a chain-of-thoughts (COT) (Wei et al., 2022) inspired baseline, where the k-shot\nprompts to the 7B and 280B models include reasoning traces for the same examples that we use\nto prompt the SI framework (although with selection and inference combined, see Supplementary\nInformation for example prompts). This tests whether providing the reasoning examples alone is\nsu\ufb03cient to improve performance, or whether the further breakdown into Selection and Inference\nsub-steps improves accuracy. Note that among all of the approaches outlined only the SI framework\nis explicitly set up to generate causalreasoning traces.\nFig.4ademonstratesthatoverallwhenevaluatedgeneratively,the7BparameterLLMwithintheSI\nframework performs better (58.75%) than the same model evaluated naively (2.94%) or in the COT\nframework (41.32%) (all \ud835\udc5d \u009d0\u009301, see Supplementary Information for details of the calculations).\nNot only that, the 7B parameter LLM within the SI framework also outperforms on average the 40x\nlarger 280B parameter LLM in both vanilla (31.19%) and COT framework (44.03%) (all \ud835\udc5d \u009d0\u009301).\nWhen evaluated in the easier multi-choice setting, we \ufb01nd that surprisingly3the vanilla 7B parameter\nLLM outperforms the 280B parameter LLM (57.31% vs 51.45%), while still performing signi\ufb01cantly\nworse than the 7B SI model ( \ud835\udc5d=0\u0093012). Note that the latter is evaluated in the harder generative\nsetting. Per task breakdown shown in Fig. 4b demonstrates that the SI framework solves the bAbI\n15 deduction task, the only model to achieve 100% accuracy (signi\ufb01cant di\ufb00erence from the other\nmodels,\ud835\udc5d \u009d0\u009301). Furthermore, it does so having seen only \ufb01ve examples in the prompt. The 7B SI\nmodelalsosigni\ufb01cantlyoutperformsallothermodelsonProofWriterDepth0( \ud835\udc5d \u009d0\u009301),ProofWriter\nDepth 1 (\ud835\udc5d=0\u0093034).\nAs well as improving upon most baselines quantitatively, the SI framework also has additional\nqualitative bene\ufb01ts: 1) it produces a causal, human interpretable reasoning trace that shows how the\nmodel reached its answer and 2) it is able to recover from errors. We will now discuss each of these\n3Thiscouldsuggest that the 280B LLM has stronger priors, than the 7B LLM, which it favours over logical reasoning. For\nexample, favouring sheep are afraid of wolves despite a context to the contrary (Min et al., 2022). However this\nrequires further investigation.\n8\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\n(a) Average accuracy over 11 datasets\ncomparing like-for-like generative per-\nformance of the 7B and 280B parameter\nlanguage models used in a 5-shot gener-\nalisationsettingtopredicttheanswerdi-\nrectly(vanilla),intheChain-Of-Thought\nframework, COT, and in the SI frame-\nwork.\n(b) Per task breakdown of the performance of LLMs used\nnaively, and within the COT and SI frameworks.\nFigure 4jQuantitative results for the Selection-Inference (SI) framework.\nin turn.\nSince the Selection module is only allowed to pick facts from the context and is separate from\nthe Inference module, and since the latter does not have access to the question, the model has to\nuse what is selected and cannot bypass the selection to compute its answer, thus creating a causal\nreasoning trace. Since the reasoning trace is in natural language and is causal, it can be audited and\ninspected by humans, which has signi\ufb01cant implications for safety and interpretability.\nExample reasoning traces produced by the SI model are shown in Fig. 2a. In the bAbI 16 example\nshown on the right the model is solving an inference problem, which requires the model to infer the\ncolourofananimalgivenfactsaboutthecoloursofotheranimals. Inthisexample, themodelisasked\n\"What colour is greg\" , and told that \"greg is a lion\" . This means \ufb01rst the model needs\nto use induction to infer a rule about the colour of lions. On the \ufb01rst step, we see that the model\ninduces a rule, \"lions are white\" , based on the fact that \"brian is a lion\" and\"brian\nis white\" ; we can see exactly what data underlies the model\u2019s decision to form a new rule. On\nthe second step, we see that the model applies this newly inferred rule to the fact that \"greg is a\nlion\"toreachthe\ufb01nalconclusionthat \"greg is white\" usingdeduction. Notethattheabilityof\nthe SI framework to produce inductions relies on its ability to deal with uncertainty and understand\nwhatis\u201creasonable\u201d-somethingthatLLMsarenaturallycapableof,whilealsobeingaknownstruggle\npoint for symbolic AI.\nSince the reasoning traces are output in natural language, they are easy for humans to interpret\nand potentially intervene. Consider a scenario where there may be both a white lion and a green lion\nmentioned in the context, in which case we could see which information the model used to make its\n\ufb01nal decision and decide whether we want to trust it (example in Fig. 2b). We could also imagine\nexamples where the model puts together two unrelated facts to come up with an incorrect inference,\nand this could also be easily be spotted by a human and recti\ufb01ed by replacing the wrongly inferred\nfact with a correct one, and re-running the consequent reasoning steps.\nAside from inspecting reasoning traces and using them to debug when something goes wrong, the\nadditive nature of our model - it accumulates new knowledge with each reasoning step, means that it\nalso has the ability to recover from errors. Fig. 2b demonstrates such an example. In the \ufb01rst step\n9\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nthe model inferred that \"swans are often gray\" , using the facts that \"julius is a swan\"\nand\"julius is gray\" . While this is correct, this new information is not useful for answering the\nquestion, which asks about lions. However, it is still possible for the model to make the more useful\ninference that \"lions are often white\" in a later step and recover from its original misstep.\n6. Fine-tuning Language Models for Selection and Inference\nIn Sec. 5 we have demonstrated signi\ufb01cant improvements in logical reasoning accuracy when using\nprompt-engineering to specialise LLMs for Selection and Inference in the SI framework. Prompt-\nengineering has the additional bene\ufb01t of not requiring large amounts of step-by-step reasoning data,\nwhich may be hard to obtain. In this section we investigate whether the accuracy of the SI framework\ncan be further improved by \ufb01ne-tuning the LLMs for Selection and Inference. We use the ProofWriter\ndataset for which ground truth reasoning traces exist.\nThe Selection LLM is \ufb01ne-tuned to select a subset of sentences (including facts and rules) from\nthe context by generating a string of the form \"sent 2. We know that sent 4 [and sent 7]*.\" given the\ncontext and the question. We ask the Selection LLM to generate sentence labels (e.g. \"sent 2\"or\"sent\n4\") instead of the sentences themselves, because this prevents the Selection LLM from cheating by\nmaking up facts to answer the question quicker. This preserves the dependency of the selection and\ntherefore subsequent reasoning steps on the context. The Inference LLM is \ufb01ne-tuned to compute an\nentailment given the selection. Both models are \ufb01ne-tuned on single steps of reasoning only. Example\ntraining data are shown in Fig. S2.\nThe Inference LLM converged very quickly to >99% test accuracy after only 300 \ufb01ne-tuning steps\nwith a batch size of 16, which is to be expected as we found that pre-trained LLMs are good at single\nstep entailment out of the box as shown in Fig. 3b. Examples of inferences made by the model are\nshown in Fig. S3. The Selection LLM was trained for 4\u0002104steps (with batch size 16 for 50 hours\non a TPU) with the exact string match accuracy reported in Fig. 5a. Although we notice that the\nmodel is much better at predicting selections for problems that require fewer steps of inference than\nthose that require more, ultimately the model still achieves high ( \u009f80%) accuracy across most of\nthe reasoning depths. Predicting early selections for deeper reasoning problems is hard, because it\nrequires planning. It is an important problem to address in future work.\nFig. 4b shows that \ufb01ne-tuning LLMs on single steps of reasoning within the SI framework leads to\nsigni\ufb01cant improvements in \ufb01nal reasoning accuracy (78.95%) over the prompt-engineered version\nof the SI framework (57.93%) and other prompt-engineered baselines (vanilla\/COT generative 7B:\n0.34\/15.73%, 280B: 31.58\/21.12%). We also found that the \ufb01ne-tuned 7B LLM used within the\nSI framework produces signi\ufb01cantly more accurate reasoning traces compared to the same LLM\n\ufb01ne-tuned to predict all reasoning steps in one go (Fig. 5b). We quanti\ufb01ed this using the Jaccard\nSimilarity, Jaccard Similarity =\u00b9\ud835\udc40\\\ud835\udc3a\ud835\udc47\u00ba\u009d\u00b9\ud835\udc40[\ud835\udc3a\ud835\udc47\u00ba, between the proof steps predicted by each\nmodel,\ud835\udc40, and the ground-truth reasoning steps, \ud835\udc3a\ud835\udc47, as shown in, calculated using exact string match\nover alphanumeric characters.\nQualitativelyweobservedthatwhilethebaselinemodelisgoodatpredictingmostofthereasoning\nsteps, they often appear in the wrong order, there are additional reasoning steps that are not on the\nminimal reasoning path, and some steps get repeated a number of times.\n7. Conclusion\nWehavepresentedtheSelection-Inferenceframeworkforimprovingtheabilityofpre-trainedlanguage\nmodels to solve logical reasoning problems expressed in natural language. Our approach borrows\n10\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\n(a) Average test \ufb01ne-tuning accuracy for the Selec-\ntionmoduletrainedonsingle-stepselectionacross\nall ProofWriter datasets (depth 1, 2, 3 and 5) and\ntested on problems of each depth separately.\n(b) Intersection over union between reasoning\ntraces produced by a model and the ground truth\nreasoning steps. Baseline, 7B parameter LLM \ufb01ne-\ntuned to predict all reasoning steps in one go; SI,\nusing same 7B LLM \ufb01ne-tuned for single step Se-\nlection and Inference.\nFigure 5jFine-tuning the SI framework on the ProofWriter dataset.\nfrom the best practices of neurosymbolic approaches to break down logical reasoning into a modular\nrecursive pipeline that not only signi\ufb01cantly improves the reasoning accuracy, but also produces a\ncausalinterpretable reasoning trace. We have demonstrated that prompt-engineered LLMs used in\nthe SI framework signi\ufb01cantly outperform both the vanilla and COT baselines evaluated in equivalent\nsettings,andeven40xlargerbaselines. TheperformanceoftheSIframeworkcanbefurtherimproved\nthrough \ufb01ne-tuning if step-by-step reasoning data is available.\nA model capable of casual, interpretable and logically valid multi-step reasoning has potential\napplications in law, medicine, science, maths, economics, and other areas where trustworthy and\nveri\ufb01able logical inference is important. At the same time we recognise that special care will need to\nbe taken to evaluate such a model before deployment in any of these settings. Further work is also\nneeded, for example, to improve the Selection module (e.g. by allowing the model search over and\nevaluate di\ufb00erent reasoning traces); to address the halting issue (both in terms of when to stop the\nselection and when to stop the overall reasoning); to incorporate veri\ufb01ers that would help avoid false\ninferences being added to the context; to enable the system to source its own relevant context rather\nthan relying on it being provided in the dataset; and to extend the ability of the system to deal with\nambiguous or unanswerable questions.\nReferences\nJ. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural module networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition , pages 39\u201348, 2016.\nY. Bengio, Y. Lecun, and G. Hinton. Deep learning for ai. Communications of the ACM , 64(7):58\u201365,\n2021.\nG. Betz, C. Voigt, and K. Richardson. Critical thinking for language models. arXiv preprint\narXiv:2009.07185 , 2020.\nR. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg,\n11\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nA. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258 , 2021.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\nprocessing systems , 33:1877\u20131901, 2020.\nP. Clark, O. Tafjord, and K. Richardson. Transformers as soft reasoners over language. arXiv preprint\narXiv:2002.05867 , 2020.\nK.Cobbe,V.Kosaraju,M.Bavarian,J.Hilton,R.Nakano,C.Hesse,andJ.Schulman. Trainingveri\ufb01ers\nto solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\nB. Dalvi, P. A. Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pipatanangkura, and P. Clark. Explaining\nanswers with entailment trees. ArXiv, abs\/2104.08661, 2021.\nA. d. Garcez and L. C. Lamb. Neurosymbolic ai: the 3rd wave. arXiv preprint arXiv:2012.05876 , 2020.\nM. Garnelo and M. Shanahan. Reconciling deep learning with symbolic arti\ufb01cial intelligence: repre-\nsenting objects and relations. Current Opinion in Behavioral Sciences , 29:17\u201323, 2019.\nA. Ghazal, T. Ivanov, P. Kostamaa, A. Crolotte, R. Voong, M. Al-Kateb, W. Ghazal, and R. V. Zicari.\nBigbench v2: The new and improved bigbench. In 2017 IEEE 33rd International Conference on Data\nEngineering (ICDE) , pages 1225\u20131236, 2017. doi: 10.1109\/ICDE.2017.167.\nN. Gupta, K. Lin, D. Roth, S. Singh, and M. Gardner. Neural module networks for reasoning over text.\narXiv preprint arXiv:1912.04971 , 2019.\nD. A. Hudson and C. D. Manning. Learning by abstraction: The neural state machine. arXiv preprint\narXiv:1907.03950 , 2019.\nH. Jhamtani and P. Clark. Learning to explain: Datasets and models for identifying valid reasoning\nchains in multihop question-answering. arXiv preprint arXiv:2010.03274 , 2020.\nA. K. Lampinen, N. A. Roy, I. Dasgupta, S. C. Chan, A. C. Tam, J. L. McClelland, C. Yan, A. Santoro,\nN. C. Rabinowitz, J. X. Wang, et al. Tell me why!\u2013explanations support learning of relational and\ncausal structure. arXiv preprint arXiv:2112.03753 , 2021.\nA. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev. Internet-augmented language models\nthroughfew-shotpromptingforopen-domainquestionanswering. arXivpreprintarXiv:2203.05115 ,\n2022.\nD. Lenat. What ai can learn from romeo & juliet, Jul 2019. URL https:\/\/www.forbes.com\/\nsites\/cognitiveworld\/2019\/07\/03\/what-ai-can-learn-from-romeo--juliet .\nX. L. Li, A. Kuncoro, C. de Masson d\u2019Autume, P. Blunsom, and A. Nematzadeh. A systematic investiga-\ntion of commonsense understanding in large language models. arXiv e-prints , pages arXiv\u20132111,\n2021.\nB. Y. Lin, S. Lee, R. Khanna, and X. Ren. Birds have four legs?! numersense: Probing numerical\ncommonsense knowledge of pre-trained language models. arXiv preprint arXiv:2005.00683 , 2020.\nK.Lu,A.Grover,P.Abbeel,andI.Mordatch. Pretrainedtransformersasuniversalcomputationengines.\narXiv preprint arXiv:2103.05247 , 2021.\n12\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nJ.Mao,C.Gan,P.Kohli,J.B.Tenenbaum,andJ.Wu. Theneuro-symbolicconceptlearner: Interpreting\nscenes, words, and sentences from natural supervision. arXiv preprint arXiv:1904.12584 , 2019.\nG. Marcus. The next decade in ai: four steps towards robust arti\ufb01cial intelligence. arXiv preprint\narXiv:2002.06177 , 2020.\nG. Marcus and E. Davis. Rebooting AI: Building Arti\ufb01cial Intelligence We Can Trust . Ballantine Books\nInc., 2019.\nJ.Menick,M.Trebacz,V.Mikulik,J.Aslanides,F.Song,M.Chadwick,M.Glaese,S.Young,L.Campbell-\nGillingham, G. Irving, et al. Teaching language models to support answers with veri\ufb01ed quotes.\narXiv preprint arXiv:2203.11147 , 2022.\nS. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the\nrole of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837 ,\n2022.\nR. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saun-\nders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint\narXiv:2112.09332 , 2021.\nM. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz,\nM. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with\nlanguage models. arXiv preprint arXiv:2112.00114 , 2021a.\nM. Nye, M. Tessler, J. Tenenbaum, and B. M. Lake. Improving coherence and consistency in neural\nsequence models with dual-system, neuro-symbolic reasoning. Advances in Neural Information\nProcessing Systems , 34, 2021b.\nJ. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Ho\ufb00mann, F. Song, J. Aslanides, S. Henderson, R. Ring,\nS. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv\npreprint arXiv:2112.11446 , 2021.\nS. Saha, S. Ghosh, S. Srivastava, and M. Bansal. Prover: Proof generation for interpretable reasoning\nover rules. arXiv preprint arXiv:2010.02830 , 2020.\nO. Tafjord, B. D. Mishra, and P. Clark. Proofwriter: Generating implications, proofs, and abductive\nstatements over natural language. arXiv preprint arXiv:2012.13048 , 2020.\nB.Tunguz.200,000+jeopardy! questions,Nov2019.URL https:\/\/www.kaggle.com\/datasets\/\ntunguz\/200000-jeopardy-questions .\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting\nelicits reasoning in large language models, 2022.\nJ. Welbl, P. Stenetorp, and S. Riedel. Constructing datasets for multi-hop reading comprehension\nacross documents. Transactions of the Association for Computational Linguistics , 6:287\u2013302, 2018.\nJ. Weston, A. Bordes, S. Chopra, A. M. Rush, B. Van Merri\u00ebnboer, A. Joulin, and T. Mikolov. Towards\nai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698 ,\n2015.\nK. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. B. Tenenbaum. Neural-symbolic vqa: Disentangling\nreasoning from vision and language understanding. arXiv preprint arXiv:1810.02338 , 2018.\nE.Zelikman,Y.Wu,andN.D.Goodman. Star: Bootstrappingreasoningwithreasoning. arXivpreprint\narXiv:2203.14465 , 2022.\n13\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nSupplementary Information\nA. Example prompts for vanilla baselines\nA.1. ProofWriter\n\"\"\"\nHere are some statements that describe a situation:\nBob is cold.\nCharlie is quiet.\nGary is cold.\nHarry is quiet.\nBig things are cold.\nAll blue things are not cold.\nIf something is quiet and blue then it is not cold.\nAll quiet things are cold.\nIf something is big and rough then it is round.\nIf something is cold and not rough then it is blue.\nIf something is quiet and not furry then it is not blue.\nRound things are big.\nBased on the above, the statement \"Charlie is cold\" is true.\n...\nHere are some statements that describe a situation:\nErin is not cold.\nErin is kind.\nErin is red.\nErin is smart.\nErin is not white.\nErin is young.\nGary is cold.\nGary is not furry.\nGary is kind.\nGary is red.\nGary is not smart.\nGary is young.\nAll cold, smart things are not furry.\nYoung, cold things are not furry.\nIf something is white and smart then it is furry.\nIf Gary is white then Gary is not furry.\nIf Erin is young then Erin is furry.\nIf Gary is not young then Gary is smart.\nIf Erin is cold then Erin is young.\nRed things are kind.\nBased on the above, the statement \"Erin is not furry\" is\n\"\"\"\nA.2. bAbI 1\n\"\"\"\nContext: daniel went to the bedroom\ndaniel journeyed to the office\ndaniel travelled to the bathroom\nmary went to the office\n14\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\njohn journeyed to the bedroom\ndaniel went back to the kitchen\njohn went to the garden\ndaniel travelled to the office\nQuestion: where is john?\nChoice: garden\nChoice: bathroom\nChoice: office\nChoice: kitchen\nChoice: bedroom\nChoice: hallway\nAnswer: garden\n...\nContext: sandra went to the kitchen\nsandra went to the office\nsandra travelled to the hallway\nsandra went back to the kitchen\nmary travelled to the hallway\nsandra went to the bedroom\njohn went to the garden\nsandra travelled to the office\nQuestion: where is sandra?\nChoice: garden\nChoice: bedroom\nChoice: kitchen\nChoice: bathroom\nChoice: hallway\nChoice: office\nAnswer:\n\"\"\"\nA.3. 2WikiMultiHop\nNew lines are added between facts to \ufb01t on the page.\n\"\"\"\nQ: When did Michael Baden-Powell\u2019s father die?\nHere are some relationships to help answer this question.\nMichael Baden-Powell::father::Peter Baden-Powell, 2nd Baron Baden-\nPowell,\nPeter Baden-Powell, 2nd Baron Baden-Powell::date of death::9\nDecember 1962\nA: 9 December 1962\n...\nQ: Where does Ekaterina Rybolovleva\u2019s father work at?\nHere are some relationships to help answer this question.\nEkaterina Dmitrievna Rybolovleva::father::Dmitry Rybolovlev,\nDmitry Rybolovlev::employer::Uralkali\nA:\n\"\"\"\n15\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nB. Example prompts for COT baselines\nB.1. ProofWriter 3\n\"\"\"\nGiven a set of rules and facts, you have to reason whether a\nstatement is true or false.\nHere are some facts and rules:\nIf someone is red then they are nice.\nIf someone is kind and red then they are white.\nIf someone is nice then they are kind.\nFiona is red.\nDoes it imply that the statement \"Fiona is not white\" is True?\nReasoning: If someone is red then they are nice. We know that Fiona\nis red. Therefore, Fiona is nice.\nIf someone is nice then they are kind. We know that Fiona is nice.\nTherefore, Fiona is kind.\nIf someone is kind and red then they are white. We know that Fiona\nis kind and Fiona is red. Therefore, Fiona is white.\n...\nHere are some facts and rules:\nIf someone chases the cow then they eat the cow.\nIf someone is big then they chase the cow.\nIf someone needs the bald eagle then the bald eagle is big.\nIf the bear is nice and the bear needs the cow then the bear eats\nthe lion.\nIf someone needs the lion and they eat the bald eagle then they are\nblue.\nIf someone eats the bear and they do not chase the cow then the cow\nis young.\nthe bald eagle eats the lion.\nthe bear is round.\nthe lion eats the bald eagle.\nthe bald eagle needs the cow.\nthe bear is young.\nthe cow is not nice.\nthe cow does not chase the bald eagle.\nthe bear does not eat the bald eagle.\nthe bear needs the bald eagle.\nthe bald eagle chases the bear.\nDoes it imply that the statement \"The bald eagle does not eat the\ncow\" is True?\nReasoning: If someone needs the bald eagle then the bald eagle is\nbig. We know that the bear needs the bald eagle. Therefore, the\nbald eagle is big.\nIf someone is big then they chase the cow. We know that the bald\neagle is big. Therefore, the bald eagle chases the cow.\nIf someone chases the cow then they eat the cow. We know that the\nbald eagle chases the cow. Therefore, the bald eagle eats the cow\n.\n\"\"\"\n16\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nB.2. bAbI 2\n\"\"\"\nBelow are some stories about people moving objects between rooms.\nAfter each story you have to answer a question about where a\nparticular object is.\nStory:\nat t=0 mary grabbed the football there\nat t=1 daniel got the apple there\nat t=2 mary went to the kitchen\nat t=3 daniel journeyed to the office\nat t=4 daniel went to the bedroom\nat t=5 mary moved to the garden\nQuestion: where is the apple?\nReason: at t=1 daniel got the apple there. We know that at t=4\ndaniel went to the bedroom. Therefore, the apple is in the\nbedroom\n...\nStory:\nat t=0 sandra went to the office\nat t=1 john took the milk there\nat t=2 sandra got the milk there\nat t=3 john dropped the milk\nQuestion: where is the milk?\nReason: at t=2 sandra got the milk there. We know that at t=0 sandra\nwent to the office. Therefore, the milk is in the office\n\"\"\"\nC. Example prompts for Selection-Inference\nC.1. bAbI 2\nTheselection prompt:\n\"\"\"\nHere are a collection of stories about people carrying objects from\none room to another. You will be asked where any object is. To\nanswer this question you need to figure out who last had the\nobject and which room they have the object in by the end of the\nstory. Here are some examples:\nStory:\nat t=0 mary grabbed the football there\nat t=1 daniel got the apple there\nat t=2 mary went to the kitchen\nat t=3 daniel journeyed to the office\nat t=4 daniel went to the bedroom\nat t=5 mary moved to the garden\nQuestion: where is the apple?\nReason: at t=1 daniel got the apple there. We know that at t=4\ndaniel went to the bedroom\n...\n17\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nat t=0 john moved to the bathroom\nat t=1 john travelled to the office\nat t=2 john picked up the football there\nat t=3 john journeyed to the bathroom\nQuestion: where is the football?\nReason:\"\"\"\nTheinference prompt:\n\"\"\"\nat t=1 daniel got the apple there. We know that at t=4 daniel went\nto the bedroom. Therefore, the apple is in the bedroom.\n...\nat t=2 john picked up the football there. We know at t=0 john moved\nto the bathroom. Therefore\"\"\"\nC.2. ProofWriter\nBelow is an example selection prompt. Note that this is for a depth-2 problem and so we show\nexamples of the \ufb01rst reasoning step where the conclusion would not directly prove or disprove the\nstatement and the last reasoning step, where the conclusion would directly prove or disprove the\nstatement.\n\"\"\"\nGiven a set of rules and facts, you have to reason whether a\nstatement is true or false.\nHere are some facts and rules:\nNice people are quiet.\nIf Dave is smart then Dave is nice.\nAll white people are smart.\nDave is smart.\nHarry is cold.\nDoes it imply that the statement \"Dave is not quiet\" is true?\nReasoning: If Dave is smart then Dave is nice. We know that Dave is\nsmart. Therefore,\nHere are some facts and rules:\nBlue things are green.\nAll blue things are white.\nIf Anne is not big then Anne is blue.\nBig things are white.\nAll kind things are round.\nIf something is white and big then it is not kind.\nIf something is big and not rough then it is green.\nIf something is white and blue then it is not green.\nErin is not white.\nAnne is big.\nBob is rough.\nAnne is white\n18\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nDoes it imply that the statement \"Anne is kind\" is True?\nReasoning: If something is white and big then it is not kind. We\nknow that Anne is white and Anne is big. Therefore,\n...\nHere are some facts and rules:\nIf something likes the squirrel and it is not young then it chases\nthe lion.\nIf something likes the squirrel then it is rough.\nIf something chases the rabbit and the rabbit is not young then it\nchases the lion.\nIf something eats the lion then it is young.\nIf something likes the rabbit then it chases the rabbit.\nAll rough things are nice.\nthe rabbit is young.\nthe squirrel likes the rabbit.\nthe lion likes the squirrel.\nDoes it imply that the statement \"The lion is not nice\" is True?\nReasoning:\"\"\"\nExample inference prompt:\n\"\"\"\nNice people are quiet. We know that Dave is nice. Therefore, Dave is\nquiet.\n...\nIf the cow chases the bald eagle then the cow eats the bald eagle.\nWe know that the cow chases the bald eagle. Therefore\"\"\"\nD. Selection-Inference evaluation details\nD.1. Selection module\nThe algorithm for the Scoring Selection module is shown in Algorithm 2.\nAlgorithm 2 ScoringSelection_Module\nRequire: An n-shot prompt, \ud835\udc5d.\nRequire: Initial Context, C0,made up of facts (and rules).\nRequire: The question, \ud835\udc5e.\nRequire: Language model, LLM.\nRequire: A halting function, halt, determines if the selection is complete.\n\ud835\udc60\ud835\udc61 empty string\nwhilenot halt() do\n\ud835\udc60temp arg max rule_or_fact2C\u00cd\ntoken2rule_or_fact LLM\u00b9tokenj\ud835\udc5d\u0094C\ud835\udc61\u0094\ud835\udc5e\u0094\ud835\udc60\ud835\udc61\u00ba\u22b2Choose the rule\nor fact with the maximum log-likelihood under the LLM model.\n\ud835\udc60\ud835\udc61 join\u00b9\ud835\udc60\ud835\udc61\u0094\ud835\udc60temp\u00ba \u22b2Join the selected fact or rule to the selection string.\nend whilereturn \ud835\udc60\ud835\udc61\n19\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nD.2. Inference module\nTo extract the new fact to be added to the context we \ufb01lter out the \ufb01rst sentence of the text generated\nby the LLM using the following regular expression: r\u2018[\u02c6.?!;nn]+\u2019.\nD.3. bAbI\nFor all bAbI tasks, the answer is a single word. For example, in bAbI 1-3 the answer is one of\n[\"hallway\", \"bathroom\", \"bedroom\", \"garden\", \"kitchen\", \"office\"] ; for bAbI 15\ntheanswerisoneof [\"sheep\", \"cat\", \"mouse\", \"wolf\"] andforbAbI16theanswerisoneof\n[\"yellow\", \"gray\", \"green\", \"white\"] . However, our model outputs a complete sentence,\nfor example \"emily is afraid of mice\" . Therefore, we take the answer to be the \ufb01nal word\noutput by the inference model on its last step.\nTo obtain the results for bAbI tasks 1-3, 15 and 16 shown in Fig. 4b we prompted the language\nmodel to solve the problem in a single step of reasoning. An example of such a prompt is shown in\nSec. C.1.\nWe run the SI model for only a single step of reasoning too. Additional steps may increase the\nchance of the model reaching the correct answer, however, we do not yet have a mechanism for\nhalting reasoning when the answer is reached.\nBAbI 16 is an inductive reasoning task that couldbe solved in two steps (rather than one). The\n\ufb01rst step requires a rule to be inferred and the second step requires the inferred rule to be applied\nto another fact. For this reason, we also apply SI for two steps to solve the bAbI 16 problems, \ufb01rst\ninferring a rule from a number of facts and then applying the rule to the correct fact. An example of\nthis is shown in Fig. 2. Using this two step approach, we can see exactly which facts contributed to\nthe formation of a new rule.\nD.4. ProofWriter\nWe use a subset of the ProofWriter Open World Assumption, OWA, dataset. In the Close World\nAssumption dataset, CWA, everything that can be proven is True otherwise it is False. This means\nthings are either True or False. This also means that reasoning traces are only provided when a\nstatement is True, but not when a statement is False. To \"show\" something is False one has to\nenumerate all possible facts (possibly up to a certain depth) and then if a statement is not shown to\nbe True it is assumed to be False. It is therefore not simple to generate meaningful reasoning traces\nfor these types of problems.\nOn the other hand, in the OWA data if it is not possible to prove something is True or False, then\nit is Unknown. This means that for True and False examples, where one may want to show p(x),\nreasoning traces are available that terminate in p(x)(for True) or not p(x) (for False). If one\ncannotshow p(x)ornot p(x) thentheanswerisUnknown,andagainthereisnotaclearreasoning\ntrace for this; it is necessary to enumerate all possible facts (possibly up to a certain depth) and then\nif one has not shown p(x)ornot p(x) it\u2019s considered Unknown. Note that here pis a predicate\nandxis a variable.\nIt is for this reason that we used the ProofWriter OWA dataset and removed the Unknowns; this\ngives us a dataset with reasoning traces concluding in either True or False. If we used CWA we would\nonly have traces that could conclude True.\nWe evaluate the SI on 5 tasks from the ProofWriter dataset, each requiring varying numbers of\nreasoning steps (1, 2, 3 and 5). This requires the model to learn to compute intermediate conclusions\n20\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nFigureS1jProofWriter: e\ufb00ectofadditionalreasoningsteps. Withadditionaliterationsofselection\nand inference the probability of the model producing the correct answer increases.\nthatmaynotdirectlyleadtothe\ufb01nalanswer,butmaybeneededtoreachthe\ufb01nalanswer. While,this\ncanbeveryhardtoachieveusingpromptengineeringalone,weendeavourtodoso,bydemonstrating\nexamples of intermediate and \ufb01nal steps of reasoning (for problems of depth >1). This means that\nthe language model sees (1) examples that both encourage the model to select rules and facts that\nmay not answer the problem in one-step but may help the model to obtain an intermediate output\nthat can be used in a future step and (2) examples of the \ufb01nal step, which takes the model to the\n\ufb01nal answer. See Sec. C.2 for an example prompt.\nThe ProofWriter tasks involve predicting if a given statement, for example \"Bob is nice.\" , is\nTrueorFalsegiven the context of facts and rules. Our SI model attempts to derive the statement\n\"Bob is nice.\" or the negation of the statement \"Bob is not nice.\" from the context. To\nassign a label TrueorFalsewe follow the procedure proposed in the original ProofWriter paper\n(Tafjord et al., 2020) and test if any of the implications matches the given statement. If there is a\nmatch, the statement is considered to be True, otherwise False.\nProofWriter results in Fig. 4b show that the Selection-Inference model outperforms the baselines\nfor problems of depth zero and one, however, with increasing depth, the gap between SI and the\nbaselines diminishes. This is in part because prompt engineering is not su\ufb03cient to obtain an optimal\nSelection module.\nAnother challenge with the ProofWriter dataset is deciding how many arguments should be\nselectedforeachrule. IntheProofWriterdataset, somerulestakeoneargument, otherstaketwo. We\nexperimented with various di\ufb00erent ways to encourage the model to stop selecting arguments. For\nexample, we append \". Therefore, \" as a choice to the context that the model can select. If the\nlanguage model selects \". Therefore, \" then the selection step ends. We allowed a maximum\nof two facts to be selected.\nTo obtain results in Fig. 4b we run SI model for the minimum number of steps needed to solve\nthe problem; a Depth \ud835\udc51problem is run for \ud835\udc51steps, with the exception of the depth-0 problem which\nis run for 1 step. However, models may perform better when allowed to run for additional steps, in\nthe case where the model makes a mistake on one step, but later recovers. Fig. S1 shows how the\nnumber of SI steps can lead to improved performance. There is greater improvement to performance\n21\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nfor depth-1 reasoning. For depth-2 reasoning, there was not enough variation in the selections at\neach step, so additional steps did not help as much as for depth-1.\nE. Reasoning Traces Output by SI\nE.1. bAbI 15\nBelow we show examples of reasoning traces output by our SI framework for the bAbI 15, deductive\nreasoning task. These examples are ones that the model got correct but are otherwise not cherry\npicked for their reasoning quality.\nbAbI 15 Example 1\nwolves are afraid of mice\nsheep are afraid of mice\nwinona is a sheep\nmice are afraid of cats\ncats are afraid of wolves\njessica is a mouse\nemily is a cat\ngertrude is a wolf\nQuestion: what is emily afraid of?\nSelection: emily is a cat. We know that cats are afraid of wolves\nInference: emily is afraid of wolves\nbAbI 15 Example 2\nwolves are afraid of mice\nsheep are afraid of mice\nwinona is a sheep\nmice are afraid of cats\ncats are afraid of wolves\njessica is a mouse\nemily is a cat\ngertrude is a wolf\nQuestion: what is winona afraid of?\nSelection: winona is a sheep. We know that sheep are afraid of mice\nInference: winona is afraid of mice\nbAbI 15 Example 3\nwolves are afraid of mice\nsheep are afraid of mice\nwinona is a sheep\nmice are afraid of cats\ncats are afraid of wolves\njessica is a mouse\nemily is a cat\ngertrude is a wolf\nQuestion: what is gertrude afraid of?\nSelection: gertrude is a wolf. We know that wolves are afraid of\nmice\nInference: gertrude is afraid of mice\n22\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nbAbI 15 Example 4\nwolves are afraid of mice\nsheep are afraid of mice\nwinona is a sheep\nmice are afraid of cats\ncats are afraid of wolves\njessica is a mouse\nemily is a cat\ngertrude is a wolf\nQuestion: what is jessica afraid of?\nSelection: jessica is a mouse. We know that mice are afraid of cats\nInference: jessica is afraid of cats\nE.2. bAbI 16\nIn the example below there are two possible conclusions that may be drawn. Since brian,bernard\nandgregare all rhions but brian and bernard are white, while greg is green the answer could be\ngreen or white. A black-box model may select one or the other answer without explaining why. Our\nmodel on the other hand shows that it use the facts that greg is green to infer that julius is\ngreen.\nbAbI 16 Example 1: Multiple \u201ccorrect\u201d answers\nbernhard is a rhino\nbrian is a rhino\nbernhard is white\nbrian is white\nlily is a lion\nlily is yellow\ngreg is a rhino\ngreg is green\njulius is a rhino\nQuestion: what color is julius?\nSelection: julius is a rhino. We know that greg is a rhino and greg\nis green\nInference: julius is green\nIn the example below the model must answer the question what color is greg? .gregis a\nlion, which means the model needs to infer a rule about lions. However, on the \ufb01rst step of reasoning\nthemodelinfersthat we inferred that swans are often gray . Whilethisisfactuallycorrect\ngiven the context, it does not help to answer the question. However, the model is able to recover\nand correctly infer that we inferred that lions are often white meaning that the model\nis still able to solve the problem despite making a mistake.\nbAbI 16 Example 2: Recovering from an error\njulius is a swan\njulius is gray\nbernhard is a swan\nbernhard is yellow\nbrian is a lion\ngreg is a lion\nbrian is white\n23\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nFigureS2jExamplesofinputsandtargetsusedto\ufb01ne-tuneLLMfortheSelectionandInference\nmodules. On the is an example of supervision for step-1 in a depth-2 problem. We create training\ndata for all steps in the dataset by appending the ground-truth inferences for the intermediate steps\nto the context. On the right, is an example of a data sample used to train the inference model. Both\nthe Selection and Inference LLM\u2019s are \ufb01ne-tuned on only single steps of reasoning.\nlily is a frog\nlily is gray\nQuestion: what color is greg?\nSelection: julius is a swan. We know that julius is gray\nInference: we inferred that swans are often gray\nSelection: brian is a lion. We know that brian is white\nInference: we inferred that lions are often white\nF. Fine-tuning Selection-Inference Details and Extra Results\nFig. S2 shows examples of the format of the data used to \ufb01ne-tune the Selection and Inference LLMs.\nThe Selection module is trained to predict sentence labels rather than the sentence strings. This\nprevents the model from making up facts and forces the model to use information in the context.\nDuringselection,ratherthanscoringeachsentencewecanappendsentencelabelstoeachelement\ninthecontext(asshowninFig.S2); usetheSelectionLLMtogenerateselectionstringsandsubstitute\ninelementsfromthecontextusingadictionary. Thisprocessismuchfasterthanscoringeachelement\nof the context, but still ensures that the selection consists only of samples from the context; the\nSelection module cannot make up facts to answer the question. Fig. S3 shows examples of entailment\ncomputed by the Inference LLM after \ufb01ne-tuning.\nFig. 4b compares SI models incorporating \ufb01ne-tuned vs. prompt-engineered LLMs. We see that\nthe model using LLMs \ufb01ne-tuned on single steps of reasoning signi\ufb01cantly outperform both the\nprompt-engineered LLMs and a Vanilla LLM prompt-engineered to predict the \ufb01nal answer directly.\nFig. S4 shows a reasoning trace output by the SI model on a challenging depth-5 problem.\nF.1. Selection-Inference Reasoning Traces\nF.1.1. Depth-2 reasoning traces for Depth-2 problems\nBelow we show examples of depth-2 reasoning traces produced via Selection-Inference using modules\n\ufb01ne-tuned on the ProofWriter dataset.\n24\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nFigure S3jInference Module \ufb01ne-tuning test examples\nExample 1:\nIf someone is cold then they eat the lion.\nIf someone is blue and they chase the dog then the dog chases the\nlion.\nIf someone eats the dog then the dog is young.\nIf someone is young and they eat the lion then they are red.\nIf someone is nice then they eat the dog.\nIf someone eats the lion and the lion eats the dog then the dog eats\nthe lion.\nIf someone sees the lion and the lion chases the dog then the lion\nis nice.\nIf the lion sees the dog and the dog sees the lion then the dog is\nnice.\nthe lion sees the dog.\nthe dog sees the lion.\nDoes it imply that the statement \"The dog eats the dog\" is True?\nstep 0:\nSelection: If the lion sees the dog and the dog sees the lion then\nthe dog is nice. We know that the lion sees the dog and the dog\nsees the lion.\nInference: The dog is nice.\nstep 1:\nSelection: If someone is nice then they eat the dog. We know that\nThe dog is nice.\nInference: The dog eats the dog.\nExample 2:\nIf something is cold and red then it likes the mouse.\nIf something needs the cat then the cat sees the dog.\n25\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nFigure S4jA ProofWriter depth-5 reasoning trace output by our model. The model produces an\ninterpretable reasoning trace that allows us to inspect how the model reached its answer.\nIf something needs the cow then the cow sees the mouse.\nIf something sees the dog then the dog likes the cat.\nIf the cat is not green then the cat does not see the mouse.\nIf something sees the mouse then it is cold.\nIf something likes the cat and the cat needs the cow then the cow is\nnice.\nIf something sees the cow then it needs the cow.\nthe mouse needs the dog.\nthe mouse needs the cat.\nthe dog is nice.\nthe cat is green.\nthe mouse is not nice.\nthe dog needs the cat.\nthe dog sees the cow.\nthe cow is not red.\nthe cat likes the dog.\nthe mouse sees the cow.\nthe mouse needs the cow.\nthe cow sees the dog.\nthe mouse is green.\nthe cow needs the dog.\nthe mouse is blue.\nDoes it imply that the statement \"The cow is cold\" is True?\nstep 0:\nSelection: If something needs the cow then the cow sees the mouse.\nWe know that the mouse needs the cow.\nInference: The cow sees the mouse.\nstep 1:\nSelection: If something sees the mouse then it is cold. We know\nthat The cow sees the mouse.\nInference: The cow is cold.\n26\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nExample 3:\nRough things are white.\nIf Erin is smart and Erin is rough then Erin is white.\nIf something is round then it is rough.\nIf Erin is round and Erin is not smart then Erin is white.\nAll quiet things are not white.\nIf something is blue and white then it is not quiet.\nErin is green.\nErin is rough.\nErin is blue.\nDoes it imply that the statement \"Erin is not quiet\" is True?\nstep 0:\nSelection: If Erin is smart and Erin is rough then Erin is white.\nWe know that Erin is green and Erin is rough.\nInference: Erin is white.\nstep 1:\nSelection: If something is blue and white then it is not quiet. We\nknow that Erin is blue and Erin is white.\nInference: Erin is not quiet.\nExample 4:\nIf something chases the squirrel then the squirrel is big.\nIf something is big then it is not kind.\nIf something chases the bald eagle and it sees the bald eagle then\nthe bald eagle sees the lion.\nthe cow does not like the squirrel.\nthe cow sees the lion.\nthe bald eagle likes the lion.\nthe cow chases the squirrel.\nthe lion chases the cow.\nthe bald eagle is not round.\nthe squirrel likes the cow.\nthe cow likes the lion.\nthe cow chases the bald eagle.\nthe squirrel likes the bald eagle.\nthe cow is kind.\nthe lion chases the squirrel.\nthe cow does not see the squirrel.\nthe lion chases the bald eagle.\nthe squirrel likes the lion.\nDoes it imply that the statement \"The squirrel is kind\" is True?\nstep 0:\nSelection: If something chases the squirrel then the squirrel is\nbig. We know that the cow chases the squirrel.\nInference: The squirrel is big.\nstep 1:\nSelection: If something is big then it is not kind. We know that\nThe squirrel is big.\nInference: The squirrel is not kind.\nF.1.2. Depth-3 reasoning traces for Depth-3 problems\nBelow we show examples of depth-3 reasoning traces produced via Selection-Inference using modules\n\ufb01ne-tuned on the ProofWriter dataset.\n27\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nExample 1:\nIf something eats the bald eagle then it eats the squirrel.\nIf something eats the squirrel then the squirrel does not eat the\nbald eagle.\nIf the bald eagle is not red then the bald eagle likes the squirrel.\nIf the squirrel likes the bald eagle then the squirrel visits the\nbald eagle.\nIf something likes the bald eagle then the bald eagle is young.\nIf something is young then it eats the squirrel.\nthe squirrel visits the bald eagle.\nthe bald eagle visits the squirrel.\nthe squirrel likes the bald eagle.\nDoes it imply that the statement \"The squirrel does not eat the bald\neagle\" is True?\nstep 0:\nSelection: If something likes the bald eagle then the bald eagle is\nyoung. We know that the squirrel likes the bald eagle.\nInference: The bald eagle is young.\nstep 1:\nSelection: If something is young then it eats the squirrel. We know\nthat The bald eagle is young.\nInference: The bald eagle eats the squirrel.\nstep 2:\nSelection: If something eats the squirrel then the squirrel does\nnot eat the bald eagle. We know that The bald eagle eats the\nsquirrel.\nInference: The squirrel does not eat the bald eagle.\nExample 2:\nIf someone is quiet then they are white.\nIf someone is young and red then they are white.\nYoung people are nice.\nIf someone is nice then they are round.\nAll quiet people are young.\nRed, big people are nice.\nRound, red people are white.\nIf someone is round then they are quiet.\nHarry is young.\nBob is red.\nBob is big.\nDoes it imply that the statement \"Harry is not quiet\" is True?\nstep 0:\nSelection: Young people are nice. We know that Harry is young.\nInference: Harry is nice.\nstep 1:\nSelection: If someone is nice then they are round. We know that\nHarry is nice.\nInference: Harry is round.\nstep 2:\nSelection: If someone is round then they are quiet. We know that\nHarry is round.\nInference: Harry is quiet.\nExample 3:\n28\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nHerethemodelmakethesameselectionon step 1andstep 2andthereforedoesnotcorrectly\nshowthatthe\u201cThelionvisitsthemouse\u201d. Althoughthemodelhasfailedhereitis(a)possiblethatthe\nmodel can still recover (if ran for additional steps) and compute the correct answer and (b) we can\nsee how the model reached its answer and interpret its failings because the reasoning trace is causal.\nIf something is green then it visits the mouse.\nIf something chases the bear then it is green.\nIf something chases the mouse then the mouse sees the bear.\nIf something sees the bear and the bear visits the mouse then the\nmouse chases the lion.\nIf something chases the mouse then it is green.\nIf something visits the bear then it chases the mouse.\nthe mouse visits the lion.\nthe lion visits the bear.\nthe bear chases the lion.\nDoes it imply that the statement \"The lion does not visit the mouse\"\nis True?\nstep 0:\nSelection: If something visits the bear then it chases the mouse.\nWe know that the lion visits the bear.\nInference: The lion chases the mouse.\nstep 1:\nSelection: If something chases the mouse then it is green. We know\nthat The lion chases the mouse.\nInference: The lion is green.\nstep 2:\nSelection: If something chases the mouse then it is green. We know\nthat The lion chases the mouse.\nInference: The lion is green.\nG. Limitation Details\nWehaveseenthatourapproachtosolvingreasoningproblems,usingSI,hasmanydesirableproperties\nand indeed this model is intended to be a proof of concept to demonstrate that it is possible to build a\nmodel with these properties. However, as a proof of concept, this model has several limitations, which\nwe we now discuss in detail.\nWhen observing the outputs of our model, the main point of failure tends to be the Selection\nmodule. This is hard to quantify since we do not have labelled data for the intermediate reasoning\nsteps. One reason for this is that we us prompt-engineering to encourage language models to produce\nthe correct outputs, rather than \ufb01ne-tuning. While our current results are good, and do not require\n(large amounts of) task speci\ufb01c data, they can be signi\ufb01cantly improved by \ufb01ne-tuning our models\nfor speci\ufb01c tasks, as demonstrated in Section 6.\nPrompt engineering works best for single modality cases (Nakano et al., 2021). It is more di\ufb03cult\nto get the model to do multi-step reasoning since the distribution or patterns for the intermediate\nsteps di\ufb00er to the \ufb01nal step. It is also di\ufb03cult to get the model to \ufb01gure out how many arguments to\nselect or how many arguments a rule takes, using only prompt engineering, again because there are\nmultiple di\ufb00erent patterns that the model needs to learn how and when to apply.\nOther limitations of our work include the assumption that a database of facts or rules are given.\nIn many practical settings we would need to be able to retrieve relevant knowledge from an existing\nknowledge base. There is exciting progress being made in this area (Lazaridou et al., 2022) and we\nhope in the future to combine these approaches with the SI model.\n29\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nFinally, whileourmodelhasthebene\ufb01tthatperformancescaleswithcomputetime; thelongerwe\nrun our model the more likely it is to reach a correct answer, we do not have a good way of deciding\nwhen to halt the reasoning process or to \ufb01lter the reasoning traces. In our current approach we have\n\ufb01xed budget, and tend to report results based on the \ufb01nal inference. Results in Fig. S1 suggests that\naccuracy could be improved if we had a mechanism for \ufb01ltering the reasoning steps and selecting the\nbest answer.\nH. Baseline Datasets\nIn this paper we used tasks from six sources: bAbi (Weston et al., 2015), BigBench (Ghazal et al.,\n2017), AAC (Betz et al., 2020), Jeopardy (Tunguz, 2019), ProofWriter (Tafjord et al., 2020) and\n2WikiMultiHop (Welbl et al., 2018). All of these dataset are publicly available and our use of the data\nwas in accordance to their respective license permissions. As far as we are aware, none of the datasets\ncontain personally identi\ufb01able information or o\ufb00ensive content.\nTask decomposition From bAbI dataset (Weston et al., 2015) used tasks 1-3 to measure the ability\nof LLMs to cope with progressively larger numbers of reasoning steps on the same type of problem;\ntask 6 to compare to task 1 whether yes\/no questions are easier for the models to answer compared\nto more free-form answers; task 9 to compare to task 1 to test how well the models can deal with\nnegation; task 10 to test whether LLMs know that the facts they are given are not su\ufb03cient to answer\na question; tasks 15 and 16 to test basic deduction and induction abilities respectively; task 18 to\ncompare to task 2 for two step reasoning on a di\ufb00erent kind of task (based on size).\nJeopardy (Tunguz, 2019) and 2WikiMultiHop (Welbl et al., 2018) tasks measure the ability of\nLLMs to do reasoning in less structured settings, where the answer has to be generated in free form,\nand the level of available context varies between none (2WikiMultiHop, Jeopardy), to relevant and\nirrelevant unstructured context (2WikiMultiHop With Context and 2WikiMultiHop With Context &\nFacts), to relevant structured context only (2WikiMultiHop With Evidence).\nAAC (Betz et al., 2020) measures the ability of LLMs to do relatively shallow formal reasoning\n(1-2 steps) over a relatively large set of syllogistic argument schemes, both with (AAC Split Extended)\nand without (AAC Split) dealing with negation.\nProofWriter (Tafjord et al., 2020) tasks measure the ability of models to do formal reasoning over\na progressively more di\ufb03cult tasks that require more steps of reasoning.\nFrom BigBench (Ghazal et al., 2017) we imported the following tasks: Analytic Entailment,\nEpistemic Reasoning and measure the ability of LLMs to decide implicit entailment relationship given\nfacts.\nEntailed Polarity, Presuppositions as NLI and Logical Arguments measure the ability of LLMs to\nunderstand implied information from vague language.\nEvaluating Information Essentiality and Su\ufb03cient Information measure how well LLMs can\nevaluate which context information is relevant and su\ufb03cient to answer a question.\nFormal Fallacies Syllogisms Negation, Logic Grid Puzzle, Logical Fallacy Detection, and Logical\nDeduction test the ability of LLMs to do formal deductive reasoning.\nSequenceProblemsTasksandTrackingShu\ufb04edObjectsaresimilartobAbItasks1-3andevaluates\nthe ability of LLMs to do multi-hop reasoning based on sequenced facts.\nPhysics Questions and Unit Interpretation measure the ability of LLMs to reason about grade\n30\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nschool science problems.\nStrategyQA measures the ability of LLMs to do multi-hop reasoning based on general knowledge\nthat is not explicitly provided as context.\nMultiple choice normalisation We evaluated whether normalising log probability of the choices\nunder the model by the token length resulted in better accuracy to avoid potential bias as reported\nin (Lin et al., 2020). We found signi\ufb01cant ( \ud835\udc5d=0\u00930002, two-tailed t-test with equal variance) but\nminimal di\ufb00erence between the average accuracy across all evaluated tasks, when evaluated with\n(67\u009392\u000646\u009368%)andwithoutnormalisation( 68\u00933\u000646\u009353%). Forthisreasonweusetheunnormalised\nmeasures in the paper.\nEvaluating dataset bias To check whether the datasets we use are biased, we compared how much\nthebaselinemultiplechoiceaccuracyofLLMswhenpresentedwithoptionsallbythemselves,without\nany context or question, deviates from the expected random performance calculated as 1\u009d\ud835\udc41, where\ud835\udc41\nis the number of choices. We found that the two di\ufb00ered by a very small amount 0\u009308\u00068\u009374%on\naverage across all models and all multiple-choice datasets ( \ud835\udc5d=5\ud835\udc52\u000016, two-tailed t-test with equal\nvariance). Since the e\ufb00ect size was so small, we concluded that the datasets were not biased and\npresent the expected random baseline where appropriate.\nAppending choices to bAbI tasks We evaluated whether adding choices to the prompt improved\nthe multiple choice accuracy of LLMs on bAbI tasks. We found that this was not the case, with\nthe average accuracy across all bAbI tasks being 37\u009386\u000648\u00935%when choices were appended, and\n44\u009386\u000649\u009374%when choices were not appended ( \ud835\udc5d=2\ud835\udc52\u000061, two-tailed t-test with equal variance).\nFor this reason, we report the latter results in the paper.\n2WikiMultiHop results We evaluated the performance of LLMs on 2WikiMultiHop (Welbl et al.,\n2018) dev subset using exact string match between the generated and the ground truth answers.\nIn particular, the generated answer was truncated at the \ufb01rst sentence up to \".\", \"?\", \"!\", \";\" or\nnewline characters following the BigBench generative evaluation protocol (Ghazal et al., 2017). The\ntwo answers were then both stripped of all punctuation, white space and special characters before\ncomparison is made. Dataset examples receive a score of 1 if the post-processed answers match\nexactly, and 0 otherwise.\nWe found that the models scored 13\u009362\u000634\u00933%on average on the original dataset, consisting\nof questions only. When the context of Wikipedia paragraphs with relevant and irrelevant facts to\nanswerthequestionwasaddedtothecontext,theperformancedroppedto 1\u009347\u000612\u009302%onaverage.\nAdding information about the relevant facts within these context paragraphs did not help much,\nresulting in 1\u009397\u000613\u00939%accuracy. On the other hand, adding only the relevant facts extracted from\nthe underlying knowledge graph triples has more than doubled the models\u2019 performance, resulting in\n35\u009355\u000647\u009387%average accuracy.\nGeneral insights LMs get progressively worse as more steps are needed for reasoning (see bAbI\ntasks 1-3 and 18, ProofWriter tasks; although not the case for Logical Deduction, Sequence Problems\nTasks and Tracking Shu\ufb04ed Objects is at chance).\nLMs \ufb01nd yes\/no questions harder to answer than freeform questions (see bAbI task 6 vs 1).\n31\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\nLMs struggle to deal with negation (bAbi task 9 vs 1) unless it is in a very well formalised limited\nsetup (AAC Split Extended).\nLMs struggle with deciding when they do not have su\ufb03cient information (bAbI task 10 vs 1;\nEvaluating Information Essentiality and Su\ufb03cient Information are both at chance).\nLMs are average at formal deduction and induction tasks, although their performance very much\ndepends on the di\ufb03culty of the task and the evaluation protocol (see bAbI task 15 and Logical\nDeduction, although Proof Writer, Formal Fallacies Syllogisms Negation, Logic Grid Puzzle, and\nLogical Fallacy Detection results are close to chance, while AAC results, where the models are\nevaluated in a very structured setting are very good).\nIn less formal mutli-hop question answering scenarios, LLMs are close to chance when no context\nis provided (2WikiMultiHop, StrategyQA; although Jeopardy is an outlier) or when the provided\ninformation is unstructured (e.g. whole Wikipedia paragraphs as in 2WikiMultiHop With Facts and\n2WikiMultiHop With Facts & Rules), but do better when minimal structured context information is\nprovided (2WikiMultiHop With Evidence).\nLMs also perform poorly in solving grade school science problems (Physics Questions and Unit\nInterpretation) although this ability is better in multiple choice compared to generative evaluation\nsettings.\nLMs are, however, reasonable at doing simple implication, entailment and induction tasks (see\nbAbI task 16, Analytic Entailment, Entailed Polarity, and Logical Arguments; although on Epistemic\nReasoning and Presuppositions as NLI the models perform around chance level).\nI. Tests of statistical signi\ufb01cance\nTo calculate statistical signi\ufb01cance of di\ufb00erences between di\ufb00erent models in Fig. 4 we used propor-\ntion hypothesis test for binary data. In particular, we used two-sided proportions_ztest from\nstatsmodels.stats.proportion .\n32\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\n(a)Absolutemulti-choiceandgenerative(markedwith\nan asterix *) accuracy.\n(b) Relative accuracy compared to chance level for\nmulti-choice tasks. Red line - chance performance.\nFigure S5jAverage accuracy of LLMs from the Gopher family (Rae et al., 2021) evaluated on logical\nreasoning tasks in a 5-shot generalisation setting.\n33","metadata":{"primary_category":"cs.AI","published":"20220519","title":"Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning","updated":"20220519"}}
{"id":"2104.06001","source":"http:\/\/arxiv.org\/pdf\/2104.06001","text":"Gender Bias in Machine Translation\nBeatrice Savoldi1,2, Marco Gaido1,2, Luisa Bentivogli2, Matteo Negri2, Marco Turchi2\n1University of Trento\n2Fondazione Bruno Kessler\nfbsavoldi,mgaido,bentivo,negri,turchi g@fbk.eu\nAbstract\nMachine translation (MT) technology has\nfacilitated our daily tasks by providing ac-\ncessible shortcuts for gathering, processing\nand communicating information. However,\nit can suffer from biases that harm users and\nsociety at large. As a relatively new \ufb01eld of\ninquiry, studies of gender bias in MT still\nlack cohesion. This advocates for a uni\ufb01ed\nframework to ease future research. To this\nend, we: i)critically review current concep-\ntualizations of bias in light of theoretical in-\nsights from related disciplines, ii)summarize\nprevious analyses aimed at assessing gender\nbias in MT, iii)discuss the mitigating strate-\ngies proposed so far, and iv)point toward\npotential directions for future work.\n1 Introduction\nInterest in understanding, assessing, and mitigating\ngender bias is steadily growing within the natu-\nral language processing (NLP) community, with\nrecent studies showing how gender disparities af-\nfect language technologies. Sometimes, for exam-\nple, coreference resolution systems fail to recog-\nnize women doctors (Zhao et al., 2017; Rudinger\net al., 2018), image captioning models do not detect\nwomen sitting next to a computer (Hendricks et al.,\n2018), and automatic speech recognition works\nbetter with male voices (Tatman, 2017). Despite a\nprior disregard for such phenomena within research\nagendas (Cislak et al., 2018), it is now widely rec-\nognized that NLP tools encode and re\ufb02ect con-\ntroversial social asymmetries for many seemingly\nneutral tasks, machine translation (MT) included.\nAdmittedly, the problem is not new (Frank et al.,\n2004). A few years ago, Schiebinger (2014) crit-\nicized the phenomenon of \u201cmasculine default\u201d in\nMT after running one of her interviews through a\ncommercial translation system. In spite of several\nfeminine mentions in the text, she was repeatedlyreferred to by masculine pronouns. Gender-related\nconcerns have also been voiced by online MT users,\nwho noticed how commercial systems entrench so-\ncial gender expectations, e.g., translating engineers\nas masculine and nurses as feminine (Olson, 2018).\nWith language technologies entering widespread\nuse and being deployed at a massive scale, their so-\ncietal impact has raised concern both within (Hovy\nand Spruit, 2016; Bender et al., 2021) and outside\n(Dastin, 2018) the scienti\ufb01c community. To take\nstock of the situation, Sun et al. (2019) reviewed\nNLP studies on the topic. However, their survey\nis based on monolingual applications, whose un-\nderlying assumptions and solutions may not be\ndirectly applicable to languages other than English\n(Zhou et al., 2019; Zhao et al., 2020; Takeshita\net al., 2020) and cross-lingual settings. Moreover,\nMT is a multifaceted task, which requires resolving\nmultiple gender-related subtasks at the same time\n(e.g., coreference resolution, named entity recogni-\ntion). Hence, depending on the languages involved\nand the factors accounted for, gender bias has been\nconceptualized differently across studies. To date,\ngender bias in MT has been tackled by means of a\nnarrow, problem-solving oriented approach. While\ntechnical countermeasures are needed, failing to\nadopt a wider perspective and engage with related\nliterature outside of NLP can be detrimental to the\nadvancement of the \ufb01eld (Blodgett et al., 2020).\nIn this paper, we intend to put such literature to\nuse for the study of gender bias in MT. We go be-\nyond surveys restricted to monolingual NLP (Sun\net al., 2019) or more limited in scope (Costa-juss `a,\n2019; Monti, 2020), and present the \ufb01rst compre-\nhensive review of gender bias in MT. In particular,\nwe1)offer a uni\ufb01ed framework that introduces\nthe concepts, sources, and effects of bias in MT,\nclari\ufb01ed in light of relevant notions on the relation\nbetween gender and different languages; 2)criti-\ncally discuss the state of the research by identifying\nblind spots and key challenges.arXiv:2104.06001v3  [cs.CL]  7 May 2021\n2 Bias statement\nBias is a fraught term with partially overlapping, or\neven competing, de\ufb01nitions (Campolo et al., 2017).\nIn cognitive science, bias refers to the possible\noutcome of heuristics, i.e., mental shortcuts that\ncan be critical to support prompt reactions (Tver-\nsky and Kahneman, 1973, 1974). AI research bor-\nrowed from such a tradition (Rich and Gureckis,\n2019; Rahwan et al., 2019) and conceived bias as\nthe divergence from an ideal or expected value\n(Glymour and Herington, 2019; Shah et al., 2020),\nwhich can occur if models rely on spurious cues\nand unintended shortcut strategies to predict out-\nputs (Schuster et al., 2019; McCoy et al., 2019;\nGeirhos et al., 2020). Since this can lead to sys-\ntematic errors and\/or adverse social effects, bias\ninvestigation is not only a scienti\ufb01c and techni-\ncal endeavour but also an ethical one, given the\ngrowing societal role of NLP applications (Bender\nand Friedman, 2018). As Blodgett et al. (2020)\nrecently called out, and has been endorsed in other\nvenues (Hardmeier et al., 2021), analysing bias\nis an inherently normative process which requires\nidentifying what is deemed as harmful behavior,\nhow, and to whom . Hereby, we stress a human-\ncentered, sociolinguistically-motivated framing of\nbias. By drawing on the de\ufb01nition by Friedman\nand Nissenbaum (1996), we consider as biased an\nMT model that systematically andunfairly discrim-\ninates against certain individuals or groups in favor\nof others. We identify bias per speci\ufb01c model\u2019s\nbehaviors, which are assessed by envisaging their\npotential risks when the model is deployed (Bender\net al., 2021) and the harms that could ensue (Craw-\nford, 2017), with people in focus (Bender, 2019).\nSince MT systems are daily employed by millions\nof individuals, they could impact a wide array of\npeople in different ways.\nAs a guide, we rely on Crawford (2017), who\nde\ufb01nes two main categories of harms produced by\na biased system: i)Representational harms (R)\n\u2013 i.e., detraction from the representation of social\ngroups and their identity, which, in turn, affects\nattitudes and beliefs; ii)Allocational harms (A)\n\u2013 i.e., a system allocates or withholds opportuni-\nties or resources to certain groups. Considering\nthe so far reported real-world instances of gender\nbias (Schiebinger, 2014; Olson, 2018) and those\naddressed in the MT literature reviewed in this\npaper, (R)can be further distinguished into under-\nrepresentation andstereotyping .Under-representation refers to the reduction of\nthe visibility of certain social groups through lan-\nguage by i)producing a disproportionately low rep-\nresentation of women (e.g., most feminine entities\nin a text are misrepresented as male in translation);\norii)not recognizing the existence of non-binary\nindividuals (e.g., when a system does not account\nfor gender neutral forms). For such cases, the mis-\nrepresentation occurs in the language employed to\ntalk \u201cabout\u201d such groups.1Also, this harm can\nimply the reduced visibility of the language used\n\u201cby\u201d speakers of such groups by iii)failing to re-\n\ufb02ect their identity and communicative repertoires.\nIn these cases, an MT \ufb02attens their communica-\ntion and produces an output that indexes unwanted\ngender identities and social meanings (e.g. women\nand non-binary speakers are not referred to by their\npreferred linguistic expressions of gender).\nStereotyping regards the propagation of negative\ngeneralizations of a social group, e.g., belittling\nfeminine representation to less prestigious occu-\npations (teacher (Feminine) vs. lecturer (Mascu-\nline)), or in association with attractiveness judg-\nments (pretty lecturer (Feminine)).\nSuch behaviors are harmful as they can directly\naffect the self-esteem of members of the target\ngroup (Bourguignon et al., 2015). Additionally,\nthey can propagate to indirect stakeholders. For\ninstance, if a system fosters the visibility of the\nway of speaking of the dominant group, MT users\ncan presume that such a language represents the\nmost appropriate or prestigious variant2\u2013 at the\nexpense of other groups and communicative reper-\ntoires. These harms can aggregate, and the ubiq-\nuitous embedding of MT in web applications pro-\nvides us with paradigmatic examples of how the\ntwo types of (R)can interplay. For example, if\nwomen or non-binary3scientists are the subjects\nof a query, automatically translated pages run the\nrisk of referring to them via masculine-in\ufb02ected\njob quali\ufb01cations. Such misrepresentations can\nlead to experience feelings of identity invalidation\n(Zimman et al., 2017). Also, users may not be\naware of being exposed to MT mistakes due to the\ndeceptively \ufb02uent output of a system (Martindale\nand Carpuat, 2018). In the long run, stereotypi-\n1See also the classi\ufb01cations by Dinan et al. (2020).\n2For an analogy on how technology shaped the perception\nof feminine voices as shrill and immature, see Tallon (2019).\n3Throughout the paper, we use non-binary as an umbrella\nterm for referring to all gender identities between or outside\nthe masculine\/feminine binary categories.\ncal assumptions and prejudices (e.g., only men are\nquali\ufb01ed for high-level positions) will be reinforced\n(Levesque, 2011; R \u00b4egner et al., 2019).\nRegarding (A), MT services are consumed by\nthe general public and can thus be regarded as re-\nsources in their own right. Hence, (R)can directly\nimply (A)as a performance disparity across users\nin the quality of service , i.e., the overall ef\ufb01ciency\nof the service. Accordingly, a woman attempting\nto translate her biography by relying on an MT sys-\ntem requires additional energy and time to revise\nwrong masculine references. If such disparities are\nnot accounted for, the MT \ufb01eld runs the risk of\nproducing systems that prevent certain groups from\nfully bene\ufb01ting from such technological resources.\nIn the following, we operationalize such cate-\ngories to map studies on gender bias to their moti-\nvations and societal implications (Table 1 and 2).\n3 Understanding Bias\nTo confront bias in MT, it is vital to reach out to\nother disciplines that foregrounded how the socio-\ncultural notions of gender interact with language(s),\ntranslation, and implicit biases. Only then can\nwe discuss the multiple factors that concur to en-\ncode and amplify gender inequalities in language\ntechnology. Note that, except for Saunders et al.\n(2020), current studies on gender bias in MT have\nassumed an (often implicit) binary vision of gender.\nAs such, our discussion is largely forced into this\nclassi\ufb01cation. Although we reiterate on bimodal\nfeminine\/masculine linguistic forms and social cat-\negories, we emphasize that gender encompasses\nmultiple biosocial elements not to be con\ufb02ated with\nsex (Risman, 2018; Fausto-Sterling, 2019), and that\nsome individuals do not experience gender, at all,\nor in binary terms (Glen and Hurrell, 2012).\n3.1 Gender and Language\nThe relation between language and gender is not\nstraightforward. First, the linguistic structures used\nto refer to the extra-linguistic reality of gender vary\nacross languages (x3.1.1). Moreover, how gender\nis assigned and perceived in our verbal practices de-\npends on contextual factors as well as assumptions\nabout social roles, traits, and attributes ( x3.1.2). At\nlast, language is conceived as a tool for articulating\nand constructing personal identities ( x3.1.3).\n3.1.1 Linguistic Encoding of Gender\nDrawing on (Corbett, 1991; Craig, 1994; Comrie,\n1999; Hellinger and Bu\u00dfman, 2001, 2002, 2003;Corbett, 2013; Gygax et al., 2019) we hereby de-\nscribe the linguistic forms (lexical, pronominal,\ngrammatical) that bear a relation with the extra-\nlinguistic reality of gender. Following Stahlberg\net al. (2007), we identify three language groups:\nGenderless languages (e.g., Finnish, Turkish).\nIn such languages, the gender-speci\ufb01c repertoire\nis at its minimum, only expressed for basic lexical\npairs, usually kinship or address terms (e.g., in\nFinnish sisko \/sister vs. veli\/brother).\nNotional gender languages4(e.g., Danish, En-\nglish). On top of lexical gender ( mom \/dad), such\nlanguages display a system of pronominal gender\n(she\/he ,her\/him ). English also hosts some marked\nderivative nouns ( actor \/actress ) and compounds\n(chairman \/chairwoman ).\nGrammatical gender languages (e.g., Arabic,\nSpanish). In these languages, each noun pertains\nto a class such as masculine, feminine, and neuter\n(if present). Although for most inanimate objects\ngender assignment is only formal,5for human ref-\nerents masculine\/feminine markings are assigned\non a semantic basis. Grammatical gender is de\ufb01ned\nby a system of morphosyntactic agreement, where\nseveral parts of speech beside the noun (e.g., verbs,\ndeterminers, adjectives) carry gender in\ufb02ections.\nIn light of the above, the English sentence\n\u201cHe\/She is a good friend\u201d has no overt expression\nof gender in a genderless language like Turkish (\u201c O\niyi bir arkada s \u00b8\u201d), whereas Spanish spreads several\nmasculine or feminine markings (\u201c El\/laesun\/a\nbuen\/ aamig o\/a\u201d). Although general, such macro-\ncategories allow us to highlight typological differ-\nences across languages. These are crucial to frame\ngender issues in both human and machine transla-\ntion. Also, they exhibit to what extent speakers of\neach group are led to think and communicate via bi-\nnary distinctions,6as well as underline the relative\ncomplexity in carving out a space for lexical in-\nnovations which encode non-binary gender (Hord,\n2016; Conrod, 2020). In this sense, while English\nis bringing the singular they in common use and\ndeveloping neo-pronouns (Bradley et al., 2019), for\ngrammatical gender languages like Spanish neu-\n4Also referred to as natural gender languages. Following\nMcConnell-Ginet (2013), we prefer notional to avoid termino-\nlogical overlapping with \u201cnatural\u201d, i.e., biological\/anatomical\nsexual categories. For a wider discussion on the topic, see\nNevalainen and Raumolin-Brunberg (1993); Curzan (2003).\n5E.g., \u201cmoon\u201d is masculine in German, feminine in French.\n6Outside of the Western paradigm, there are cultures whose\nlanguages traditionally encode gender outside of the binary\n(Epple, 1998; Murray, 2003; Hall and O\u2019Donovan, 2014).\ntrality requires the development of neo-morphemes\n(\u201cElleesunebuene amigue \u201d).\n3.1.2 Social Gender Connotations\nTo understand gender bias, we have to grasp not\nonly the structure of different languages, but also\nhow linguistic expressions are connoted, deployed,\nand perceived (Hellinger and Motschenbacher,\n2015). In grammatical gender languages, feminine\nforms are often subject to a so-called semantic dero-\ngation (Schulz, 1975), e.g., in French, couturier\n(fashion designer) vs. couturi `ere(seamstress). En-\nglish is no exception (e.g., governor \/governess ).\nMoreover, bias can lurk underneath seemingly\nneutral forms. Such is the case of epicene (i.e., gen-\nder neutral) nouns where gender is not grammati-\ncally marked. Here, gender assignment is linked\nto (typically binary) social gender, i.e., \u201cthe so-\ncially imposed dichotomy of masculine and femi-\nnine role and character traits\u201d (Kramarae and Tre-\nichler, 1985). As an illustration, Danish speakers\ntend to pronominalize dommer (judge) with han\n(he) when referring to the whole occupational cate-\ngory (Gomard, 1995; Nissen, 2002). Social gender\nassignment varies across time and space (Lyons,\n1977; Romaine, 1999; Cameron, 2003) and regards\nstereotypical assumptions about what is typical or\nappropriate for men and women. Such assumptions\nimpact our perceptions (Hamilton, 1988; Gygax\net al., 2008; Kreiner et al., 2008) and in\ufb02uence our\nbehavior \u2013 e.g., leading individuals to identify with\nand ful\ufb01ll stereotypical expectations (Wolter and\nHannover, 2016; Sczesny et al., 2018) \u2013 and verbal\ncommunication, e.g., women are often misquoted\nin the academic community (Krawczyk, 2017).\nTranslation studies highlight how social gender\nassignment in\ufb02uences translation choices (Jakob-\nson, 1959; Chamberlain, 1988; Comrie, 1999;\nDi Sabato and Perri, 2020). Primarily, the prob-\nlem arises from typological differences across lan-\nguages and their gender systems. Nonetheless,\nsocio-cultural factors also in\ufb02uence how transla-\ntors deal with such differences. Consider the char-\nacter of the cook in Daphne du Maurier\u2019s \u201cRe-\nbecca\u201d, whose gender is never explicitly stated in\nthe whole book. In the lack of any available in-\nformation, translators of \ufb01ve grammatical gender\nlanguages represented the character as either a man\nor a woman (Wandruszka, 1969; Nissen, 2002).\nAlthough extreme, this case can illustrate the sit-\nuation of uncertainty faced by MT: the mapping\nof one-to-many forms in gender prediction. But,as discussed inx4.1, mistranslations occur when\ncontextual gender information is available as well.\n3.1.3 Gender and Language Use\nLanguage use varies between demographic groups\nand re\ufb02ects their backgrounds, personalities, and\nsocial identities (Labov, 1972; Trudgill, 2000; Pen-\nnebaker and Stone, 2003). In this light, the study of\ngender and language variation has received much\nattention in socio- and corpus linguistics (Holmes\nand Meyerhoff, 2003; Eckert and McConnell-Ginet,\n2013). Research conducted in speech and text\nanalysis highlighted several gender differences,\nwhich are exhibited at the phonological and lexical-\nsyntactic level. For example, women rely more\non hedging strategies (\u201cit seems that\u201d), purpose\nclauses (\u201cin order to\u201d), \ufb01rst-person pronouns, and\nprosodic exclamations (Mulac et al., 2001; Mon-\ndorf, 2002; Brownlow et al., 2003). Although some\ncorrespondences between gender and linguistic fea-\ntures hold across cultures and languages (Smith,\n2003; Johannsen et al., 2015), it should be kept in\nmind that they are far from universal7and should\nnot be intended in a stereotyped and oversimpli\ufb01ed\nmanner (Bergvall et al., 1996; Nguyen et al., 2016;\nKoolen and van Cranenburgh, 2017).\nDrawing on gender-related features proved use-\nful to build demographically informed NLP tools\n(Garimella et al., 2019) and personalized MT mod-\nels (Mirkin et al., 2015; Bawden et al., 2016; Ra-\nbinovich et al., 2017). However, using personal\ngender as a variable requires a prior understanding\nof which categories may be salient, and a critical\nre\ufb02ection on how gender is intended and ascribed\n(Larson, 2017). Otherwise, if we assume that the\nonly relevant (sexual) categories are \u201cmale\u201d and\n\u201cfemale\u201d, our models will inevitably ful\ufb01ll such a\nreductionist expectation (Bamman et al., 2014).\n3.2 Gender Bias in MT\nTo date, an overview of how several factors may\ncontribute to gender bias in MT does not exist. We\nidentify and clarify concurring problematic causes,\naccounting for the context in which systems are\ndeveloped and used ( x2). To this aim, we rely on\nthe three overarching categories of bias described\nby Friedman and Nissenbaum (1996), which fore-\n7It has been largely debated whether gender-related differ-\nences are inherently biological or cultural and social products\n(Mulac et al., 2001). Currently, the idea that they depend on\nbiological reasons is largely rejected (Hyde, 2005) in favor of\na socio-cultural or performative perspective (Butler, 1990).\nground different sources that can lead to machine\nbias. These are: pre-existing bias \u2013 rooted in our\ninstitutions, practices and attitudes ( x3.2.1), techni-\ncal bias \u2013 due to technical constraints and decisions\n(x3.2.2), and emergent bias \u2013 arising from the in-\nteraction between systems and users ( x3.2.3). We\nconsider such categories as placed along a contin-\nuum, rather than being discrete.\n3.2.1 Pre-existing Bias\nMT models are known to re\ufb02ect gender dispari-\nties present in the data. However, re\ufb02ections on\nsuch generally invoked disparities are often over-\nlooked. Treating data as an abstract, monolithic\nentity (Gitelman, 2013) \u2013 or relying on \u201coverly\nbroad\/overloaded terms like training data bias \u201d8\n(Suresh and Guttag, 2019) \u2013 do not encourage rea-\nsoning on the many factors of which data are the\nproduct. First and foremost, the historical, socio-\ncultural context in which they are generated.\nA starting point to tackle these issues is the Eu-\nroparl corpus (Koehn, 2005), where only 30% of\nsentences are uttered by women (Vanmassenhove\net al., 2018). Such an imbalance is a direct window\ninto the glass ceiling that has hampered women\u2019s\naccess to parliamentary positions. This case exem-\npli\ufb01es how data might be \u201ctainted with historical\nbias\u201d, mirroring an \u201cunequal ground truth\u201d (Hacker,\n2018). However, other gender variables are harder\nto spot and quantify.\nEmpirical linguistics research pointed out that\nsubtle gender asymmetries are rooted in languages\u2019\nuse and structure. For instance, an important aspect\nregards how women are referred to. Femaleness is\noften explicitly invoked when there is no textual\nneed to do so, even in languages that do not require\novert gender marking. A case in point regards\nTurkish, which differentiates cocuk (child) and kiz\ncocugu (female child) (Braun, 2000). Similarly, in\na corpus search, Romaine (2001) found 155 explicit\nfemale markings for doctor (female, woman or lady\ndoctor), compared to only 14 male doctor . Feminist\nlanguage critique provided extensive analysis of\nsuch a phenomenon by highlighting how referents\nin discourse are considered men by default unless\nexplicitly stated (Silveira, 1980; Hamilton, 1991).\nFinally, prescriptive top-down guidelines limit the\nlinguistic visibility of gender diversity, e.g., the\nReal Academia de la Lengua Espa \u02dcnola recently\ndiscarded the of\ufb01cial use of non-binary innovations\n8See (Johnson, 2020a; Samar, 2020) for a discussion on\nhow such narrative can be counterproductive for tackling bias.and claimed the functionality of masculine generics\n(Mundo, 2018; L \u00b4opez et al., 2020).\nBy stressing such issues, we are not condoning\nthe reproduction of pre-existing bias in MT. Rather,\nthe above-mentioned concerns are the starting point\nto account for when dealing with gender bias.\n3.2.2 Technical Bias\nTechnical bias comprises aspects related to data\ncreation, models design, training and testing pro-\ncedures. If present in training and testing samples,\nasymmetries in the semantics of language use and\ngender distribution are respectively learnt by MT\nsystems and rewarded in their evaluation. However,\nas just discussed, biased representations are not\nmerely quantitative, but also qualitative. Accord-\ningly, straightforward procedures \u2013 e.g., balancing\nthe number of speakers in existing datasets \u2013 do\nnot ensure a fairer representation of gender in MT\noutputs. Since datasets are a crucial source of bias,\nit is also crucial to advocate for a careful data cu-\nration (Mehrabi et al., 2019; Paullada et al., 2020;\nHanna et al., 2021; Bender et al., 2021), guided\nby pragmatically- and socially-informed analyses\n(Hitti et al., 2019; Sap et al., 2020; Devinney et al.,\n2020) and annotation practices (Gaido et al., 2020).\nOverall, while data can mirror gender inequali-\nties and offer adverse shortcut learning opportuni-\nties, it is \u201cquite clear that data alone rarely constrain\na model suf\ufb01ciently\u201d (Geirhos et al., 2020) nor ex-\nplain the fact that models overamplify (Shah et al.,\n2020) such inequalities in their outputs. Focusing\non models\u2019 components, Costa-juss `a et al. (2020b)\ndemonstrate that architectural choices in multilin-\ngual MT impact the systems\u2019 behavior: shared\nencoder-decoders retain less gender information\nin the source embeddings and less diversion in the\nattention than language-speci\ufb01c encoder-decoders\n(Escolano et al., 2021), thus disfavoring the gen-\neration of feminine forms. While discussing the\nloss and decay of certain words in translation, Van-\nmassenhove et al. (2019, 2021) attest to the ex-\nistence of an algorithmic bias that leads under-\nrepresented forms in the training data \u2013 as it may\nbe the case for feminine references \u2013 to further\ndecrease in the MT output. Speci\ufb01cally, Roberts\net al. (2020) prove that beam search \u2013 unlike sam-\npling \u2013 is skewed toward the generation of more\nfrequent (masculine) pronouns, as it leads models\nto an extreme operating point that exhibits zero\nvariability.\nThus, efforts towards understating and mitigat-\ning gender bias should also account for the model\nfront. To date, this remains largely unexplored.\n3.2.3 Emergent Bias\nEmergent bias may arise when a system is used\nin a different context than the one it was designed\nfor, e.g., when it is applied to another demographic\ngroup. From car crash dummies to clinical trials,\nwe have evidence of how not accounting for gender\ndifferences brings to the creation of male-grounded\nproducts with dire consequences (Liu and Dipi-\netro Mager, 2016; Criado-Perez, 2019), such as\nhigher death and injury risks in vehicle crash and\nless effective medical treatments for women. Simi-\nlarly, unbeknownst to their creators, MT systems\nthat are not intentionally envisioned for a diverse\nrange of users will not generalize for the feminine\nsegment of the population. Hence, in the interac-\ntion with an MT system, a woman will likely be\nmisgendered or not have her linguistic style pre-\nserved (Hovy et al., 2020). Other conditions of\nusers\/system mismatch may be the result of chang-\ning societal knowledge and values. A case in point\nregards Google Translate\u2019s historical decision to\nadjust its system for instances of gender ambigu-\nity. Since its launch twenty years ago, Google\nhad provided only one translation for single-word\ngender-ambiguous queries (e.g., professor trans-\nlated in Italian with the masculine professore ). In\na community increasingly conscious of the power\nof language to hardwire stereotypical beliefs and\nwomen\u2019s invisibility (Lindqvist et al., 2019; Beuke-\nboom and Burgers, 2019), the bias exhibited by\nthe system was confronted with a new sensitivity.\nThe service\u2019s decision (Kuczmarski, 2018) to pro-\nvide a double feminine\/masculine output ( profes-\nsor!professoressajprofessore ) stems from current\ndemands for gender-inclusive resolutions. For the\nrecognition of non-binary groups (Richards et al.,\n2016), we invite studies on how such modeling\ncould be integrated with neutral strategies ( x6).\n4 Assessing Bias\nFirst accounts on gender bias in MT date back to\nFrank et al. (2004). Their manual analysis pointed\nout how English-German MT suffers from a dearth\nof linguistic competence, as it shows severe dif\ufb01-\nculties in recovering syntactic and semantic infor-\nmation to correctly produce gender agreement.\nSimilar inquiries were conducted on other tar-\nget grammatical gender languages for several com-\nmercial MT systems (Abu-Ayyash, 2017; Monti,2017; Rescigno et al., 2020). While these stud-\nies focused on contrastive phenomena, Schiebinger\n(2014)9went beyond linguistic insights, calling for\na deeper understanding of gender bias. Her article\non Google Translate\u2019s \u201cmasculine default\u201d behav-\nior emphasized how such a phenomenon is related\nto the larger issue of gender inequalities, also per-\npetuated by socio-technical artifacts (Selbst et al.,\n2019). All in all, these qualitative analyses demon-\nstrated that gender problems encompass all three\nMT paradigms (neural, statistical, and rule-based),\npreparing the ground for quantitative work.\nTo attest the existence and scale of gender bias\nacross several languages, dedicated benchmarks,\nevaluations, and experiments have been designed.\nWe \ufb01rst discuss large scale analyses aimed at as-\nsessing gender bias in MT, grouped according to\ntwo main conceptualizations: i)works focusing\non the weight of prejudices and stereotypes in MT\n(x4.1); ii)studies assessing whether gender is prop-\nerly preserved in translation ( x4.2). In accordance\nwith the human-centered approach embraced in this\nsurvey, in Table 1 we map each work to the harms\n(seex2) ensuing from the biased behaviors they\nassess. Finally, we review existing benchmarks for\ncomparing MT performance across genders ( x4.3).\n4.1 MT and Gender Stereotypes\nIn MT, we record prior studies concerned with pro-\nnoun translation and coreference resolution across\ntypologically different languages accounting for\nboth animate and inanimate referents (Hardmeier\nand Federico, 2010; Le Nagard and Koehn, 2010;\nGuillou, 2012). For the speci\ufb01c analysis on gender\nbias, instead, such tasks are exclusively studied in\nrelation to human entities.\nPrates et al. (2018) and Cho et al. (2019) design\na similar setting to assess gender bias. Prates et al.\n(2018) investigate pronoun translation from 12 gen-\nderless languages into English. Retrieving \u00181,000\njob positions from the U.S. Bureau of Labor Statis-\ntics, they build simple constructions like the Hun-\ngarian \u201c \u02ddoegym\u00b4ern\u00a8ok\u201d (\u201che\/she is an engineer \u201d).\nFollowing the same template, Cho et al. (2019)\nextend the analysis to Korean-English including\nboth occupations and sentiment words (e.g., kind).\nAs their samples are ambiguous by design, the ob-\nserved predictions of he\/she pronouns should be\n9See also Schiebinger\u2019s project\nGendered Innovations : http:\/\/\ngenderedinnovations :stanford :edu\/case-\nstudies\/nlp :html\nrandom, yet they show a strong masculine skew.10\nTo further analyze the under-representation of\nshepronouns, Prates et al. (2018) focus on 22\nmacro-categories of occupation areas and compare\nthe proportion of pronoun predictions against the\nreal-world proportion of men and women employed\nin such sectors. In this way, they \ufb01nd that MT\nnot only yields a masculine default, but it also un-\nderestimates feminine frequency at a greater rate\nthan occupation data alone suggest. Such an analy-\nsis starts by acknowledging pre-existing bias (see\nx3.2.1) \u2013 e.g., low rates of women in STEM \u2013 to\nattest the existence of machine bias, and de\ufb01nes it\nas the exacerbation of actual gender disparities.\nGoing beyond word lists and simple synthetic\nconstructions, Gonen and Webster (2020) inspect\nthe translation into Russian, Spanish, German, and\nFrench of natural yet ambiguous English sentences.\nTheir analysis on the ratio and type of generated\nmasculine\/feminine job titles consistently exhibits\nsocial asymmetries for target grammatical gender\nlanguages (e.g., lecturer masculine vs. teacher\nfeminine). Finally, Stanovsky et al. (2019) assess\nthat MT is skewed to the point of actually ignoring\nexplicit feminine gender information in source En-\nglish sentences. For instance, MT systems yield a\nwrong masculine translation of the job title baker ,\nalthough it is referred to by the pronoun she. Beside\nthe overlook of overt gender mentions, the model\u2019s\nreliance on unintended (and irrelevant) cues for gen-\nder assignment is further con\ufb01rmed by the fact that\nadding a socially connoted \u2013 but formally epicene \u2013\nadjective (the pretty baker) pushes models toward\nfeminine in\ufb02ections in translation.\nWe observe that the propagation of stereotypes\nis a widely researched form of gender asymmetries\nin MT, one that so far has been largely narrowed\ndown to occupational stereotyping. After all, occu-\npational stereotyping has been studied by different\ndisciplines (Greenwald et al., 1998) attested across\ncultures (Lewis and Lupyan, 2020), and it can be\neasily detected in MT across multiple language di-\nrections with consistent results. Current research\nshould not neglect other stereotyping dynamics, as\nin the case of Stanovsky et al. (2019) and Cho et al.\n10Cho et al. (2019) highlight that a higher frequency of fem-\ninine references in the MT output does not necessarily imply a\nbias reduction. Rather, it may re\ufb02ect gender stereotypes, as for\nhairdresser that is skewed toward feminine. This observation\npoints to the tension between frequency count, suitable for\ntesting under-representation, and qualitative-oriented analysis\non bias conceptualized in terms of stereotyping.(2019), who include associations to physical char-\nacteristics or psychological traits. Also, the intrinsi-\ncally contextual nature of societal expectations ad-\nvocates for the study of culture-speci\ufb01c dimensions\nof bias. Finally, we signal that the BERT-based\nperturbation method by Webster et al. (2019) iden-\nti\ufb01es other bias-susceptible nouns that tend to be\nassigned to a speci\ufb01c gender (e.g., \ufb01ghter as mas-\nculine). As Blodgett (2021) underscores, however,\n\u201cthe existence of these undesirable correlations is\nnot suf\ufb01cient to identify them as normatively un-\ndesirable\u201d. It should thus be investigated whether\nsuch statistical preferences can cause harms, e.g.,\nby checking if they map to existing harmful associ-\nations or quality of service disparities.\n4.2 MT and Gender Preservation\nVanmassenhove et al. (2018) and Hovy et al. (2020)\ninvestigate whether speakers\u2019 gender11is properly\nre\ufb02ected in MT. This line of research is preceded\nby \ufb01ndings on gender personalization of statisti-\ncal MT (Mirkin et al., 2015; Bawden et al., 2016;\nRabinovich et al., 2017), which claim that gender\n\u201csignals\u201d are weakened in translation.\nHovy et al. (2020) conjecture the existence of\nage and gender stylistic bias due to models\u2019 under-\nexposure to the writings of women and younger\nsegments of the population. To test this hypoth-\nesis, they automatically translate a corpus of on-\nline reviews with available metadata about users\n(Hovy et al., 2015). Then, they compare such demo-\ngraphic information with the prediction of age and\ngender classi\ufb01ers run on the MT output. Results\nindicate that different commercial MT models sys-\ntematically make authors \u201csound\u201d older and male.\nTheir study thus concerns the under-representation\nof the language used \u201cby\u201d certain speakers and\nhow it is perceived (Blodgett, 2021). However,\nthe authors do not inspect which linguistic choices\nMT overproduces, nor which stylistic features may\ncharacterize different socio-demographic groups.\nStill starting from the assumption that demo-\ngraphic factors in\ufb02uence language use, Vanmassen-\nhove et al. (2018) probe MT\u2019s ability to preserve\nspeaker\u2019s gender translating from English into ten\nlanguages. To this aim, they develop gender-\ninformed MT models (see x5.1), whose outputs\nare compared with those obtained by their base-\nline counterparts. Tested on a set for spoken lan-\n11Note that these studies distinguish speakers into fe-\nmale\/male. As discussed in x3.1.3, we invite a re\ufb02ection\non the appropriateness and use of such categories.\nguage translation (Koehn, 2005), their enhanced\nmodels show consistent gains in terms of overall\nquality when translating into grammatical gender\nlanguages, where speaker\u2019s references are often\nmarked. For instance, the French translation of\n\u201cI\u2019m happy \u201d is either \u201cJe suis heureuse \u201c or \u201cJe\nsuishereux \u201d for a female\/male speaker respectively.\nThrough a focused cross-gender analysis \u2013 carried\nout by splitting their English-French test set into\n1st person male vs. female data \u2013 they assess that\nthe largest margin of improvement for their gender-\ninformed approach concerns sentences uttered by\nwomen, since the results of their baseline disclose a\nquality of service disparity in favor of male speak-\ners. Besides morphological agreement, they also\nattribute such improvement to the fact that their\nenhanced model produces gendered preferences in\nother word choices. For instance, it opts for think\nrather than believe , which is in concordance with\ncorpus studies claiming a tendency for women to\nuse less assertive speech (Newman et al., 2008).\nNote that the authors rely on manual analysis to\nascribe performance differences to gender-related\nfeatures. In fact, global evaluations on generic test\nsets alone are inadequate to pointedly measure gen-\nder bias.\n4.3 Existing Benchmarks\nMT outputs are typically evaluated against refer-\nence translations employing standard metrics such\nas BLEU (Papineni et al., 2002) or TER (Snover\net al., 2006). This procedure poses two chal-\nlenges. First, these metrics provide coarse-grained\nscores for translation quality, as they treat all er-\nrors equally and are rather insensitive to speci\ufb01c\nlinguistic phenomena (Sennrich, 2017). Second,\ngeneric test sets containing the same gender imbal-\nance present in the training data can reward biased\npredictions. Hereby, we describe the publicly avail-\nable MT Gender Bias Evaluation Testsets (GBETs)\n(Sun et al., 2019), i.e., benchmarks designed to\nprobe gender bias by isolating the impact of gender\nfrom other factors that may affect systems\u2019 perfor-\nmance. Note that different benchmarks and met-\nrics respond to different conceptualizations of bias\n(Barocas et al., 2019). Common to them all in MT,\nhowever, is that biased behaviors are formalized\nby using some variants of averaged performance12\n12This is a value-laden option (Birhane et al., 2020), and\nnot the only possible one (Mitchell et al., 2020). For a broader\ndiscussion on measurement and bias we refer the reader also\nto (Jacobs, 2021; Jacobs et al., 2020).disparities across gender groups, comparing the ac-\ncuracy of gender predictions on an equal number\nof masculine, feminine, and neutral references.\nEscud \u00b4e Font and Costa-juss `a (2019) developed\nthe bilingual English-Spanish Occupations test\nset. It consists of 1,000 sentences equally dis-\ntributed across genders. The phrasal structure\nenvisioned for their sentences is \u201cI\u2019ve known\nfherjhimj<proper noun >gfor a long time, my\nfriend works asfajang<occupation >\u201d. The evalu-\nation focuses on the translation of the noun friend\ninto Spanish ( amig o\/a). Since gender information\nis present in the source context and sentences are\nthe same for both masculine\/feminine participants,\nan MT system exhibits gender bias if it disregards\nrelevant context and cannot provide the correct\ntranslation of friend at the same rate across genders.\nStanovsky et al. (2019) created WinoMT by\nconcatenating two existing English GBETs for\ncoreference resolution (Rudinger et al., 2018; Zhao\net al., 2018a). The corpus consists of 3,888 Wino-\ngradesque sentences presenting two human entities\nde\ufb01ned by their role and a subsequent pronoun that\nneeds to be correctly resolved to one of the entities\n(e.g., \u201cThe lawyer yelled at the hairdresser because\nhedid a bad job\u201d). For each sentence, there are two\nvariants with either heorshepronouns, so as to\ncast the referred annotated entity ( hairdresser ) into\na proto- or anti-stereotypical gender role. By trans-\nlating WinoMT into grammatical gender languages,\none can thus measure systems\u2019 ability to resolve\nthe anaphoric relation and pick the correct femi-\nnine\/masculine in\ufb02ection for the occupational noun.\nOn top of quantifying under-representation as the\ndifference between the total amount of translated\nfeminine and masculine references, the subdivision\nof the corpus into proto- and anti-stereotypical sets\nalso allows verifying if MT predictions correlate\nwith occupational stereotyping.\nFinally, Saunders et al. (2020) enriched the origi-\nnal version of WinoMT in two different ways. First,\nthey included a third gender-neutral case based on\nthe singular they pronoun, thus paving the way\nto account for non-binary referents. Second, they\nlabeled the entity in the sentence which is not coref-\nerent with the pronoun ( lawyer ). The latter anno-\ntation is used to verify the shortcomings of some\nmitigating approaches as discussed in x5.\nThe above-mentioned corpora are known as chal-\nlenge sets , consisting of sentences created ad hoc\nfor diagnostic purposes. In this way, they can\nStudy Benchmark Gender Harms\n(Prates et al., 2018) Synthetic, U.S. Bureau of Labor Statistics b R: under-rep, stereotyping\n(Cho et al., 2019) Synthetic equity evaluation corpus (EEC) b R: under-rep, stereotyping\n(Gonen and Webster, 2020) BERT-based perturbations on natural sentences b R: under-rep, stereotyping\n(Stanovsky et al., 2019) WinoMT b R: under-rep, stereotyping\n(Vanmassenhove et al., 2018) Europarl (generic) b A: quality\n(Hovy et al., 2020) Trustpilot (reviews with gender and age) b R: under-rep\nTable 1: For each Study , the Table shows on which Benchmark gender bias is assessed, how Gender is intended (here only\nin binary (b) terms). Finally, we indicate which (R)epresentational \u2013 under-representation andstereotyping \u2013 or (A)llocational\nHarm \u2013 as reduced quality of service \u2013 is addressed in the study.\nbe used to quantify bias related to stereotyping\nand under-representation in a sound environment.\nHowever, since they consist of a limited variety of\nsynthetic gender-related phenomena, they hardly\naddress the variety of challenges posed by real-\nworld language and are relatively easy to over\ufb01t.\nAs recognized by Rudinger et al. (2018) \u201cthey may\ndemonstrate the presence of gender bias in a sys-\ntem, but not prove its absence\u201d.\nThe Arabic Parallel Gender Corpus (Habash\net al., 2019) includes an English-Arabic test set13\nretrieved from OpenSubtitles natural language data\n(Lison and Tiedemann, 2016). Each of the 2,448\nsentences in the set exhibits a \ufb01rst person sin-\ngular reference to the speaker (e.g., \u201cI\u2019m rich\u201d).\nAmong them,\u0018200 English sentences require gen-\nder agreement to be assigned in translation. These\nwere translated into Arabic in both gender forms,\nobtaining a quantitatively and qualitatively equal\namount of sentence pairs with annotated mascu-\nline\/feminine references. This natural corpus thus\nallows for cross-gender evaluations on MT produc-\ntion of correct speaker\u2019s gender agreement.\nMuST-SHE (Bentivogli et al., 2020) is a natu-\nral benchmark for three language pairs (English-\nFrench\/Italian\/Spanish). Built on TED talks data\n(Cattoni et al., 2021), for each language pair it\ncomprises\u00181,000 ( audio ,transcript ,translation )\ntriplets, thus allowing evaluation for both MT and\nspeech translation (ST). Its samples are balanced\nbetween masculine and feminine phenomena, and\nincorporate two types of constructions: i)sentences\nreferring to the speaker (e.g., \u201c Iwasborn in Mum-\nbai\u201d), and ii)sentences that present contextual in-\nformation to disambiguate gender (e.g., \u201cMy mum\nwasborn in Mumbai\u201d). Since every gender-marked\nword in the target language is annotated in the cor-\npus, MuST-SHE grants the advantage of comple-\nmenting BLEU- and accuracy-based evaluations on\n13Overall, the corpus comprises over 12,000 annotated sen-\ntences and 200,000 synthetic sentences.gender translation for a great variety of phenomena.\nUnlike challenge sets, natural corpora quantify\nwhether MT yields reduced feminine representa-\ntion in authentic conditions and whether the quality\nof service varies across speakers of different gen-\nders. However, as they treat all gender-marked\nwords equally, it is not possible to identify if the\nmodel is propagating stereotypical representations.\nAll in all, we stress that each test set and metric\nis only a proxy for framing a phenomenon or an\nability (e.g., anaphora resolution), and an approxi-\nmation of what we truly intend to gauge. Thus, as\nwe discuss inx6, advances in MT should account\nfor the observation of gender bias in real-world\nconditions to avoid that achieving high scores on\na mathematically formalized esteem could lead to\na false sense of security. Still, benchmarks remain\nvaluable tools to monitor models\u2019 behavior. As\nsuch, we remark that evaluation procedures ought\nto cover both models\u2019 general performance and\ngender-related issues. This is crucial to establish\nthe capabilities and limits of mitigating strategies.\n5 Mitigating Bias\nTo attenuate gender bias in MT, different strategies\ndealing with input data, learning algorithms, and\nmodel outputs have been proposed. As attested\nby Birhane et al. (2020), since advancements are\noftentimes exclusively reported in terms of values\ninternal to the machine learning \ufb01eld (e.g ef\ufb01ciency,\nperformance), it is not clear how such strategies\nare meeting societal needs by reducing MT-related\nharms. In order to conciliate technical perspectives\nwith the intended social purpose, in Table 2 we map\neach mitigating approach to the harms (see x2) they\nare meant to alleviate, as well as to the benchmark\ntheir effectiveness is evaluated against. Comple-\nmentarily, we hereby describe each approach by\nmeans of two categories: model debiasing ( x5.1)\nand debiasing through external components ( x5.2).\nApproach Authors Benchmark Gender Harms\nGender tagging\n(sentence-level)Vanmassenhove et al. Europarl (generic) b R: under-rep, A: quality\nElaraby et al. Open subtitles (generic) b R: under-rep, A: quality\nGender tagging\n(word-level)Saunders et al. expanded WinoMT nb R: under-rep, stereotyping\nStafanovi \u02c7cs et al. WinoMT b R: under-rep, stereotyping\nAdding context Basta et al. WinoMT b R: under-rep, stereotyping\nWord-embeddings Escud \u00b4e Font and Costa-juss `aOccupation test set b R: under-rep\nFine-tuning Costa-juss `a and de Jorge WinoMT b R: under-rep, stereotyping\nBlack-box injection Moryossef et al. Open subtitles (selected sample) b R: under-rep, A: quality\nLattice-rescoring Saunders and Byrne WinoMT b R: under-rep, steretoyping\nRe-in\ufb02ection Habash et al.; Alhafni et al. Arabic Parallel Gender Corpus b R: under-rep, A: quality\nTable 2: For each Approach and related Authors , the Table shows on which Benchmark it is tested, if Gender is intended\nin binary terms (b), or including non-binary (nb) identities. Finally, we indicate which (R)epresentational \u2013 under-representation\nandstereotyping \u2013 or (A)llocational Harm \u2013 as reduced quality of service \u2013 the approach attempts to mitigate.\n5.1 Model Debiasing\nThis line of work focuses on mitigating gender bias\nthrough architectural changes of general-purpose\nMT models or via dedicated training procedures.\nGender tagging. To improve the generation\nof speaker\u2019s referential markings, Vanmassenhove\net al. (2018) prepend a gender tag (M or F) to each\nsource sentence, both at training and inference time.\nAs their model is able to leverage this additional\ninformation, the approach proves useful to handle\nmorphological agreement when translating from\nEnglish into French. However, this solution re-\nquires additional metadata regarding the speakers\u2019\ngender that might not always be feasible to ac-\nquire. Automatic annotation of speakers\u2019 gender\n(e.g., based on \ufb01rst names) is not advisable, as it\nruns the risk of introducing additional bias by mak-\ning unlicensed assumptions about one\u2019s identity.\nElaraby et al. (2018) bypass this risk by de\ufb01ning\na comprehensive set of cross-lingual gender agree-\nment rules based on POS tagging. In this way, they\nidentify speakers\u2019 and listeners\u2019 gender references\nin an English-Arabic parallel corpus, which is con-\nsequently labeled and used for training. The idea,\noriginally developed for spoken language transla-\ntion in a two-way conversational setting, can be\nadapted for other languages and scenarios by creat-\ning new dedicated rules. However, in realistic de-\nployment conditions where reference translations\nare not available, gender information still has to be\nexternally supplied as metadata at inference time.\nStafanovi \u02c7cs et al. (2020) and Saunders et al.\n(2020) explore the use of word-level gender tags.\nWhile Stafanovi \u02c7cs et al. (2020) just report a gen-\nder translation improvement, Saunders et al. (2020)\nrely on the expanded version of WinoMT to iden-\ntify a problem concerning gender tagging: it intro-duces noise if applied to sentences with references\nto multiple participants, as it pushes their transla-\ntion toward the same gender. Saunders et al. (2020)\nalso include a \ufb01rst non-binary exploration of neu-\ntral translation by exploiting an arti\ufb01cial dataset,\nwhere neutral tags are added and gendered in\ufb02ec-\ntions are replaced by placeholders. The results are\nhowever inconclusive, most likely due to the small\nsize and synthetic nature of their dataset.\nAdding context. Without further information\nneeded for training or inference, Basta et al. (2020)\nadopt a generic approach and concatenate each sen-\ntence with its preceding one. By providing more\ncontext, they attest a slight improvement in gender\ntranslations requiring anaphorical coreference to be\nsolved in English-Spanish. This \ufb01nding motivates\nexploration at the document level, but it should be\nvalidated with manual (Castilho et al., 2020) and in-\nterpretability analyses since the added context can\nbe bene\ufb01cial for gender-unrelated reasons, such as\nacting as a regularization factor (Kim et al., 2019).\nDebiased word embeddings. The two above-\nmentioned mitigations share the same intent: sup-\nply the model with additional gender knowledge.\nInstead, Escud \u00b4e Font and Costa-juss `a (2019) lever-\nage pre-trained word embeddings, which are debi-\nased by using the hard-debiasing method proposed\nby Bolukbasi et al. (2016) or the GN-GloVe algo-\nrithm (Zhao et al., 2018b). These methods respec-\ntively remove gender associations or isolate them\nfrom the representations of English gender-neutral\nwords. Escud \u00b4e Font and Costa-juss `a (2019) employ\nsuch embeddings on the decoder side, the encoder\nside, and both sides of an English-Spanish model.\nThe best results are obtained by leveraging GN-\nGloVe embeddings on both encoder and decoder\nsides, increasing BLEU scores and gender accuracy.\nThe authors generically apply debiasing methods\ndeveloped for English also to their target language.\nHowever, being Spanish a grammatical gender lan-\nguage, other language-speci\ufb01c approaches should\nbe considered to preserve the quality of the original\nembeddings (Zhou et al., 2019; Zhao et al., 2020).\nWe also stress that it is debated whether depriving\nsystems of some knowledge and \u201cblind\u201d their per-\nceptions is the right path toward fairer language\nmodels (Dwork et al., 2012; Caliskan et al., 2017;\nGonen and Goldberg, 2019; Nissim and van der\nGoot, 2020). Also, Goldfarb-Tarrant et al. (2020)\n\ufb01nd that there is no reliable correlation between in-\ntrinsic evaluations of bias in word-embeddings and\ncascaded effects on MT models\u2019 biased behavior.\nBalanced \ufb01ne-tuning. Costa-juss `a and de Jorge\n(2020) rely on Gebiotoolkit (Costa-juss `a et al.,\n2020c) to build gender-balanced datasets (i.e., fea-\nturing an equal amount of masculine\/feminine ref-\nerences) based on Wikipedia biographies. By \ufb01ne-\ntuning their models on such natural and more even\ndata, the generation of feminine forms is overall\nimproved. However, the approach is not as effec-\ntive for gender translation on the anti-stereotypical\nWinoMT set. As discussed in x3.2.2, they employ\na straightforward method that aims to increase the\namount of feminine Wikipedia pages in their train-\ning data. However, such coverage increase does not\nmitigate stereotyping harms, as it does not account\nfor the qualitative different ways in which men and\nwomen are portrayed (Wagner et al., 2015).\n5.2 Debiasing through External Components\nInstead of directly debiasing the MT model, these\nmitigating strategies intervene in the inference\nphase with external dedicated components. Such\napproaches do not imply retraining, but introduce\nthe additional cost of maintaining separate modules\nand handling their integration with the MT model.\nBlack-box injection. Moryossef et al. (2019)\nattempt to control the production of feminine refer-\nences to the speaker and numeral in\ufb02ections (plural\nor singular) for the listener(s) in an English-Hebrew\nspoken language setting. To this aim, they rely on\na short construction, such as \u201c shesaid to them \u201d,\nwhich is prepended to the source sentence and then\nremoved from the MT output. Their approach is\nsimple, it can handle two types of information (gen-\nder and number) for multiple entities (speaker and\nlistener), and improves systems\u2019 ability to gener-\nate feminine target forms. However, as in the case\nof Vanmassenhove et al. (2018) and Elaraby et al.(2018), it requires metadata about speakers and\nlisteners.\nLattice re-scoring. Saunders and Byrne (2020)\npropose to post-process the MT output with a lat-\ntice re-scoring module. This module exploits a\ntransducer to create a lattice by mapping gender\nmarked words in the MT output to all their possi-\nble in\ufb02ectional variants. Developed for German,\nSpanish, and Hebrew, all the sentences correspond-\ning to the paths in the lattice are re-scored with\nanother model, which has been gender-debiased\nbut at the cost of lower generic translation quality.\nThen, the sentence with the highest probability is\npicked as the \ufb01nal output. When tested on WinoMT,\nsuch an approach leads to an increase in the ac-\ncuracy of gender forms selection. Note that the\ngender-debiased system is created by \ufb01ne-tuning\nthe model on an ad hoc built tiny set containing\na balanced amount of masculine\/feminine forms.\nSuch an approach, also known as counterfactual\ndata augmentation (Lu et al., 2020), requires to\ncreate identical pairs of sentences differing only\nin terms of gender references. In fact, Saunders\nand Byrne (2020) compile English sentences fol-\nlowing this schema: \u201cThe <profession >\ufb01nished\n<hisjher>work\u201d. Then, the sentences are auto-\nmatically translated and manually checked. In this\nway, they obtain gender-balanced parallel corpus.\nThus, to implement their method for other language\npairs, the generation of new data is necessary. For\nthe \ufb01ne-tuning set, the effort required is limited\nas the goal is to alleviate stereotypes by focusing\non a pre-de\ufb01ned occupational lexicon. However,\ndata augmentation is very demanding for complex\nsentences that represent a rich variety of gender\nagreement phenomena14such as those occurring in\nnatural language scenarios.\nGender re-in\ufb02ection. Habash et al. (2019)\nand Alhafni et al. (2020) confront the problem\nof speaker\u2019s gender agreement in Arabic with a\npost-processing component that re-in\ufb02ects 1st per-\nson references into masculine\/feminine forms. In\nAlhafni et al. (2020), the preferred gender of the\nspeaker and the translated Arabic sentence are fed\nto the component, which re-in\ufb02ects the sentence\nin the desired form. In Habash et al. (2019) the\ncomponent can be: i) a two-step system that \ufb01rst\nidenti\ufb01es the gender of 1st person references in\n14Zmigrod et al. (2019) proposed an automatic approach for\naugmenting data into morphologically-rich languages, but it\nis only viable for simple constructions with one single entity.\nan MT output, and then re-in\ufb02ects them in the op-\nposite form; ii) a single-step system that always\nproduces both forms from an MT output. Their\nmethod does not necessarily require speakers\u2019 gen-\nder information: if metadata are supplied, the MT\noutput is re-in\ufb02ected accordingly; differently, both\nfeminine\/masculine in\ufb02ections are offered (leaving\nto the user the choice of the appropriate one). The\nimplementation of the re-in\ufb02ection component was\nmade possible by the Arabic Parallel Gender Cor-\npus (seex4.3), which demanded an expensive work\nof manual data creation. However, such corpus\ngrants research on English-Arabic the bene\ufb01ts of\na wealth of gender-informed natural language data\nthat have been curated to avoid hetero-centrist inter-\npretations and preconceptions (e.g., proper names\nand speakers of sentences like \u201cthat\u2019s my wife\u201d are\n\ufb02agged as gender-ambiguous). Along the same\nline, Google Translate also delivers two outputs for\nshort gender-ambiguous queries (Johnson, 2020b).\nAmong languages with grammatical gender, the ser-\nvice is currently available only for English-Spanish.\nIn light of the above, we remark that there is no\nconclusive state-of-the-art method for mitigating\nbias. The discussed interventions in MT tend to re-\nspond to speci\ufb01c aspects of the problem with mod-\nular solutions, but if and how they can be integrated\nwithin the same MT system remains unexplored.\nAs we have discussed through the survey, the um-\nbrella term \u201cgender bias\u201d refers to a wide array of\nundesirable phenomena. Thus, it is unlikely that a\none-size-\ufb01ts-all solution will be able tackle prob-\nlems that differ from one another, as they depend\non e.g., how bias is conceptualized, the language\ncombinations, the kinds of corpora used. As a re-\nsult, we believe that generalization and scalability\nshould not be the only criteria against which miti-\ngating strategies are valued. Conversely, we should\nmake room for openly context-aware interventions.\nFinally, gender bias in MT is a socio-technical\nproblem. We thus highlight that engineering in-\nterventions alone are not a panacea (Chang, 2019)\nand should be integrated with long-term multidisci-\nplinary commitment and practices (D\u2019Ignazio and\nKlein, 2020; Gebru, 2020) necessary to address\nbias in our community, hence in its artifacts, too.\n6 Conclusion and Key Challenges\nAs studies confronting gender bias in MT are\nrapidly emerging, in this paper we presented them\nwithin a uni\ufb01ed framework to critically overviewcurrent conceptualizations and approaches to the\nproblem. Since gender bias is a multifaceted and\ninterdisciplinary issue, in our discussion we inte-\ngrated knowledge from related disciplines, which\ncan be instrumental to guide future research and\nmake it thrive. We conclude by suggesting several\ndirections that can help this \ufb01eld going forward.\nModel de-biasing. Neural networks rely on\neasy-to-learn shortcuts or \u201ccheap tricks\u201d (Levesque,\n2014), as picking up on spurious correlations of-\nfered by training data can be easier for machines\nthan learning to actually solve a speci\ufb01c task. What\nis \u201ceasy to learn\u201d for a model depends on the induc-\ntive bias (Sinz et al., 2019; Geirhos et al., 2020) re-\nsulting from architectural choices, training data and\nlearning rules. We think that explainability tech-\nniques (Belinkov et al., 2020) represent a useful\ntool to identify spurious cues (features) exploited\nby the model during inference. Discerning them\ncan provide the research community with guid-\nance on how to improve models\u2019 generalization\nby working on data, architectures, loss functions\nand optimizations. For instance, data responsi-\nble for spurious features (e.g., stereotypical cor-\nrelations) might be recognized and their weight\nat training time might be lowered (Karimi Ma-\nhabadi et al., 2020). Besides, state-of-the-art ar-\nchitectural choices and algorithms in MT have\nmostly been studied in terms of overall transla-\ntion quality without speci\ufb01c analyses regarding\ngender translation. For instance, current systems\nsegment text into subword units with statistical\nmethods that can break the morphological struc-\nture of words, thus losing relevant semantic and\nsyntactic information in morphologically-rich lan-\nguages (Niehues et al., 2016; Ataman et al., 2017).\nSeveral languages show complex feminine forms,\ntypically derivative and created by adding a suf-\n\ufb01x to the masculine form, such as Lehrer\/Lehrer in\n(de), studente\/studente ssa(it). It would be relevant\nto investigate whether, compared to other segmenta-\ntion techniques, statistical approaches disadvantage\n(rarer and more complex) feminine forms. The MT\ncommunity should not overlook focused hypothe-\nses of such kind, as they can deepen our compre-\nhension of the gender bias conundrum.\nNon-textual modalities. Gender bias for non-\ntextual automatic translations (e.g., audiovisual)\nhas been largely neglected. In this sense, ST repre-\nsents a small niche (Costa-juss `a et al., 2020a). For\nthe translation of speaker-related gender phenom-\nena, Bentivogli et al. (2020) prove that direct ST\nsystems exploit speaker\u2019s vocal characteristics as a\ngender cue to improve feminine translation. How-\never, as addressed by Gaido et al. (2020), relying\non physical gender cues (e.g., pitch) for such task\nimplies reductionist gender classi\ufb01cations (Zim-\nman, 2020) making systems potentially harmful\nfor a diverse range of users. Similarly, although\nimage-guided translation has been claimed useful\nfor gender translation since it relies on visual in-\nputs for disambiguation (Frank et al., 2018; Ive\net al., 2019), it could bend toward stereotypical\nassumptions about appearance. Further research\nshould explore such directions to identify potential\nchallenges and risks, by drawing on bias in im-\nage captioning (van Miltenburg, 2019) and consoli-\ndated studies from the \ufb01elds of automatic gender\nrecognition and human-computer interaction (HCI)\n(Hamidi et al., 2018; Keyes, 2018; May, 2019).\nBeyond Dichotomies. Besides a few notable\nexceptions for English NLP tasks (Manzini et al.,\n2019; Cao and Daum \u00b4e III, 2020; Sun et al., 2021)\nand one in MT (Saunders et al., 2020), the discus-\nsion around gender bias has been reduced to the\nbinary masculine\/feminine dichotomy. Although\nresearch in this direction is currently hampered by\nthe absence of data, we invite considering inclu-\nsive solutions and exploring nuanced dimensions\nof gender. Starting from language practices, Indi-\nrect Non-binary Language (INL) overcomes gen-\nder speci\ufb01cations (e.g., using service, humankind\nrather than waiter\/waitress ormankind ).15Whilst\nmore challenging, INL can be achieved also for\ngrammatical gender languages (Motschenbacher,\n2014; Lindqvist et al., 2019), and it is endorsed\nfor of\ufb01cial EU documents (Papadimoulis, 2018).\nAccordingly, MT models could be brought to avoid\nbinary forms and move toward gender-unspeci\ufb01ed\nsolutions, e.g., adversarial networks including a\ndiscriminator that classi\ufb01es speaker\u2019s linguistic ex-\npression of gender (masculine or feminine) could\nbe employed to \u201cneutralize\u201d speaker-related forms\n(Li et al., 2018; Delobelle et al., 2020). Conversely,\nDirect Non-binary Language (DNL) aims at in-\ncreasing the visibility of non-binary individuals\nvia neologisms and neomorphemes (Bradley et al.,\n2019; Papadopoulos, 2019; Knisely, 2020). With\nDNL starting to circulate (Shroy, 2016; Santiago,\n2018; L \u00b4opez, 2019), the community is presented\n15INL suggestions have also been recently implemented\nwithin Microsoft text editors (Langston, 2020).with the opportunity to promote the creation of\ninclusive data.\nFinally, as already highlighted in legal and so-\ncial science theory, discrimination can arise from\nthe intersection of multiple identity categories (e.g.,\nrace and gender) (Crenshaw, 1989) which are not\nadditive and cannot always be detected in isolation\n(Schlesinger et al., 2017). Following the MT work\nby Hovy et al. (2020), as well as other intersec-\ntional analyses from NLP (Herbelot et al., 2012;\nJiang and Fellbaum, 2020) and AI-related \ufb01elds\n(Buolamwini and Gebru, 2018), future studies may\naccount for the interaction of gender attributes with\nother sociodemographic classes.\nHuman-in-the-loop. Research on gender bias\nin MT is still restricted to lab tests. As such, un-\nlike other studies that rely on participatory design\n(Turner et al., 2015; Cercas Curry et al., 2020;\nLiebling et al., 2020), the advancement of the \ufb01eld\nis not measured with people\u2019s experience in fo-\ncus or in relation to speci\ufb01c deployment contexts.\nHowever, these are fundamental considerations to\nguide the \ufb01eld forward and, as HCI studies show\n(V orvoreanu et al., 2019), to propel the creation of\ngender-inclusive technology. In particular, repre-\nsentational harms are intrinsically dif\ufb01cult to es-\ntimate and available benchmarks only provide a\nrough idea of their extent. This advocates for fo-\ncused studies16on their individual or aggregate\neffects in everyday life. Also, we invite the whole\ndevelopment process to be paired with bias-aware\nresearch methodology (Havens et al., 2020) and\nHCI approaches (Stumpf et al., 2020), which can\nhelp to operationalize sensitive attributes like gen-\nder (Keyes et al., 2021). Finally, MT is not only\nbuilt for people, but also by people. Thus, it is vital\nto re\ufb02ect on the implicit biases and backgrounds of\nthe people involved in MT pipelines at all stages\nand how they could be re\ufb02ected in the model. This\nmeans starting from bottom-level countermeasures,\nengaging with translators (De Marco and Toto,\n2019; Lessinger, 2020), annotators (Waseem, 2016;\nGeva et al., 2019), considering everyone\u2019s subjec-\ntive positionality and, crucially, also the lack of\ndiversity within technology teams (Schluter, 2018;\nWaseem et al., 2020).\n16To the best of our knowledge, the Gender-Inclusive\nLanguage Models Survey is the \ufb01rst project of this kind that\nincludes MT. At time of writing it is available at: https:\/\/\ndocs.google.com\/forms\/d\/e\/1FAIpQLSfKenp4RKtDhKA0W\nLqP\ufb02GSBV2VdBA9h3F8MwqRex 4kiCf9Q\/viewform\nAcknowledgments\nWe would like to thank the anonymous reviewers\nand the TACL Action Editors. Their insightful\ncomments helped us improve on the current version\nof the paper.\nReferences\nEmad A. S. Abu-Ayyash. 2017. Errors and Non-\nerrors in English-Arabic Machine Translation\nof Gender-Bound Constructs in Technical Texts.\nProcedia Computer Science , 117:73\u201380.\nBashar Alhafni, Nizar Habash, and Houda\nBouamor. 2020. Gender-Aware Rein\ufb02ection us-\ning Linguistically Enhanced Neural Models. In\nProceedings of the Second Workshop on Gen-\nder Bias in Natural Language Processing , pages\n139\u2013150, Online. Association for Computational\nLinguistics.\nDuygu Ataman, Matteo Negri, Marco Turchi, and\nMarcello Federico. 2017. Linguistically Mo-\ntivated V ocabulary Reduction for Neural Ma-\nchine Translation from Turkish to English. The\nPrague Bulletin of Mathematical Linguistics ,\n108(1):331\u2013342.\nDavid Bamman, Jacob Eisenstein, and Tyler Sch-\nnoebelen. 2014. Gender identity and lexical vari-\nation in social media. Journal of Sociolinguis-\ntics, 18(2):135\u2013160.\nSolon Barocas, Moritz Hardt, and Arvind\nNarayanan. 2019. Fairness and Ma-\nchine Learning . fairmlbook.org.\nhttp:\/\/www :fairmlbook :org.\nChristine Basta, Marta R. Costa-juss `a, and Jos \u00b4e\nA. R. Fonollosa. 2020. Towards Mitigating Gen-\nder Bias in a Decoder-based Neural Machine\nTranslation model by Adding Contextual In-\nformation. In Proceedings of the The Fourth\nWidening Natural Language Processing Work-\nshop , pages 99\u2013102, Seattle, USA. Association\nfor Computational Linguistics.\nRachel Bawden, Guillaume Wisniewski, and\nH\u00b4el`ene Maynard. 2016. Investigating Gender\nAdaptation for Speech Translation. In Proceed-\nings of the 23 `eme Conf \u00b4erence sur le Traitement\nAutomatique des Langues Naturelles , volume 2,\npages 490\u2013497, Paris, FR.Yonatan Belinkov, Nadir Durrani, Fahim Dalvi,\nHassan Sajjad, and James Glass. 2020. On the\nLinguistic Representational Power of Neural Ma-\nchine Translation Models. Computational Lin-\nguistics , 46(1):1\u201352.\nEmily M. Bender. 2019. A Typology of Ethical\nRisks in Language Technology with an Eye To-\nwards where Transparent Documentation might\nhelp. In CRAASH. The future of Arti\ufb01cial In-\ntelligence: Language, Ethics, Technology , Cam-\nbridge, UK.\nEmily M. Bender and Batya Friedman. 2018. Data\nStatements for Natural Language Processing: To-\nward Mitigating System Bias and Enabling Bet-\nter Science. Transactions of the Association for\nComputational Linguistics , 6:587\u2013604.\nEmily M. Bender, Timnit Gebru, Angelina\nMcMillan-Major, and Shmargaret Shmitchell.\n2021. On the Dangers of Stochastic Parrots: Can\nLanguage Models be too Big? In Proceedings\nof the Conference on Fairness, Accountability,\nand Transparency (FAccT \u201921) , pages 610\u2013623,\nOnline. ACM.\nLuisa Bentivogli, Beatrice Savoldi, Matteo Negri,\nMattia A. Di Gangi, Roldano Cattoni, and Marco\nTurchi. 2020. Gender in Danger? Evaluating\nSpeech Translation Technology on the MuST-\nSHE Corpus. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics , pages 6923\u20136933, Online. Associa-\ntion for Computational Linguistics.\nVictoria L. Bergvall, Janet M. Bing, and Alice F.\nFreed. 1996. Rethinking Language and Gen-\nder Research: Theory and Practice . Addison\nWesley Longman, London, UK.\nCamiel J. Beukeboom and Christian Burgers. 2019.\nHow Stereotypes are shared through Language:\nA Review and Introduction of the Social Cate-\ngories and Stereotypes Communication (SCSC)\nFramework. Review of Communication Re-\nsearch , 7:1\u201337.\nAbeba Birhane, Pratyusha Kalluri, Dallas Card,\nWilliam Agnew, Ravit Dotan, and Michelle Bao.\n2020. The Underlying Values of Machine Learn-\ning Research. In Resistance AI Workshop @\nNeurIPS , Online.\nSu Lin Blodgett. 2021. Sociolinguistically Driven\nApproaches for Just Natural Language Process-\ning. Doctoral Dissertations. 2092.\nSu Lin Blodgett, Solon Barocas, Hal Daum \u00b4e III,\nand Hanna Wallach. 2020. Language (Technol-\nogy) is Power: A Critical Survey of \u201cBias\u201d in\nNLP. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Lin-\nguistics , pages 5454\u20135476, Online. Association\nfor Computational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam T. Kalai. 2016.\nMan is to Computer Programmer as Woman is to\nHomemaker? Debiasing Word Embeddings. In\nProceedings of the 30th Conference on Neural\nInformation Processing Systems (NIPS 2016) ,\nvolume 29, pages 4349\u20134357, Barcelona, ES.\nCurran Associates, Inc.\nDavid Bourguignon, Vincent Y . Yzerbyt, Catia P.\nTeixeira, and Ginette Herman. 2015. When does\nit hurt? Intergroup permeability moderates the\nlink between discrimination and self-esteem. Eu-\nropean Journal of Social Psychology , 45(1):3\u20139.\nEvan D. Bradley, Julia Salkind, Ally Moore, and\nSo\ufb01 Teitsort. 2019. Singular \u2018they\u2019 and novel\npronouns: gender-neutral, nonbinary, or both?\nProceedings of the Linguistic Society of America ,\n4(1):36\u20131.\nFriederike Braun. 2000. Geschlecht im T \u00a8urkischen:\nUntersuchungen zum sprachlichen Umgang mit\neiner sozialen Kategorie . Turcologica Series.\nOtto Harrassowitz Verlag, Wiesbaden, DE.\nSheila Brownlow, Julie A. Rosamond, and Jen-\nnifer A. Parker. 2003. Gender-linked Linguistic\nBehavior in Television Interviews. Sex Roles ,\n49(3-4):121\u2013132.\nJoy Buolamwini and Timnit Gebru. 2018. Gender\nShades: Intersectional Accuracy Disparities in\nCommercial Gender Classi\ufb01cation. In Proceed-\nings of the 1st Conference on Fairness, Account-\nability and Transparency , volume 81 of Proceed-\nings of Machine Learning Research , pages 77\u2013\n91, New York, USA. PMLR.\nJudith Butler. 1990. Gender Trouble: Feminism\nand the Subversion of Identity . Routledge, New\nYork, USA.Aylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics Derived Automat-\nically from Language Corpora contain Human-\nlike Biases. Science , 356(6334):183\u2013186.\nDeborah Cameron. 2003. Gender Issues in Lan-\nguage Change. Annual Review of Applied Lin-\nguistics , 23:187\u2013201.\nAlex Campolo, Madelyn R. San\ufb01lippo, Meredith\nWhittaker, and Kate Crawford. 2017. AI Now\nReport 2017. New York: AI Now Institute .\nYang T. Cao and Hal Daum \u00b4e III. 2020. Toward\nGender-Inclusive Coreference Resolution. In\nProceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics ,\npages 4568\u20134595, Online. Association for Com-\nputational Linguistics.\nSheila Castilho, Maja Popovi \u00b4c, and Andy Way.\n2020. On Context Span Needed for Machine\nTranslation Evaluation. In Proceedings of the\n12th Language Resources and Evaluation Con-\nference , pages 3735\u20133742, Marseille, FR. Euro-\npean Language Resources Association.\nRoldano Cattoni, Mattia A. Di Gangi, Luisa Ben-\ntivogli, Matteo Negri, and Marco Turchi. 2021.\nMuST-C: A multilingual corpus for end-to-end\nspeech translation. Computer Speech & Lan-\nguage , 66:101155.\nAmanda Cercas Curry, Judy Robertson, and Ver-\nena Rieser. 2020. Conversational Assistants\nand Gender Stereotypes: Public Perceptions and\nDesiderata for V oice Personas. In Proceedings\nof the Second Workshop on Gender Bias in Nat-\nural Language Processing , pages 72\u201378, Online.\nAssociation for Computational Linguistics.\nLori Chamberlain. 1988. Gender and the\nMetaphorics of Translation. Signs: Journal of\nWomen in Culture and Society , 13(3):454\u2013472.\nKai-Wei Chang. 2019. Bias and Fairness in Nat-\nural Language Processing. Tutorial at the 2019\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nWon Ik Cho, Ji Won Kim, Seok Min Kim, and\nNam Soo Kim. 2019. On Measuring Gender\nbias in Translation of Gender-neutral Pronouns.\nInProceedings of the First Workshop on Gen-\nder Bias in Natural Language Processing , pages\n173\u2013181, Florence, IT. Association for Compu-\ntational Linguistics.\nAleksandra Cislak, Magdalena Formanowicz, and\nTamar Saguy. 2018. Bias Against Research on\nGender Bias. Scientometrics , 115(1):189\u2013200.\nBernard Comrie. 1999. Grammatical Gender Sys-\ntems: A Linguist\u2019s Assessment. Journal of Psy-\ncholinguistic Research , 28:457\u2013466.\nKirby Conrod. 2020. Pronouns and Gender in Lan-\nguage. The Oxford Handbook of Language and\nSexuality .\nGreville G. Corbett. 1991. Gender . Cambridge\nTextbooks in Linguistics. Cambridge University\nPress, Cambridge, UK.\nGreville G. Corbett. 2013. The Expression of Gen-\nder. De Gruyter Mouton, Berlin, DE.\nMarta R. Costa-juss `a. 2019. An Analysis of Gen-\nder Bias Studies in Natural Language Processing.\nNature Machine Intelligence , 1:495\u2013496.\nMarta R. Costa-juss `a, Christine Basta, and Ger-\nard I. G \u00b4allego. 2020a. Evaluating gender\nbias in speech translation. arXiv preprint\narXiv:2010.14465 .\nMarta R. Costa-juss `a, Carlos Escolano, Christine\nBasta, Javier Ferrando, Roser Batlle, and Ksenia\nKharitonova. 2020b. Gender Bias in Multilin-\ngual Neural Machine Translation: The Architec-\nture Matters. arXiv preprint arXiv:2012.13176 .\nMarta R. Costa-juss `a and Adri `a de Jorge. 2020.\nFine-tuning Neural Machine Translation on\nGender-Balanced Datasets. In Proceedings of\nthe Second Workshop on Gender Bias in Natu-\nral Language Processing , pages 26\u201334, Online.\nAssociation for Computational Linguistics.\nMarta R. Costa-juss `a, Pau Li Lin, and Cristina\nEspa \u02dcna-Bonet. 2020c. GeBioToolkit: Auto-\nmatic Extraction of Gender-Balanced Multilin-\ngual Corpus of Wikipedia Biographies. In Pro-\nceedings of the 12th Language Resources and\nEvaluation Conference , pages 4081\u20134088, Mar-\nseille, FR. European Language Resources Asso-\nciation.\nColette G. Craig. 1994. Classi\ufb01er Languages. In\nRonald E. Asher & James M. Y . Simpson, editor,The Encyclopedia of Language and Linguistics ,\nvolume 2, pages 565\u2013569. Pergamon Press, Ox-\nford, UK.\nKate Crawford. 2017. The Trouble with Bias. In\nConference on Neural Information Processing\nSystems (NIPS) \u2013 Keynote , Long Beach, USA.\nKimberl \u00b4e Crenshaw. 1989. Demarginalizing the\nIntersection of Race and Sex: A Black Feminist\nCritique of Antidiscrimination Doctrine, Femi-\nnist Theory and Antiracist Politics. University\nof Chicago Legal Forum , 1989:139\u2013167.\nCaroline Criado-Perez. 2019. Invisible Women:\nExposing Data Bias in a World Designed for\nMen. Chatto & Windus, London, UK.\nAnne Curzan. 2003. Gender Shifts in the History\nof English . Cambridge University Press, Cam-\nbridge, UK.\nJeffrey Dastin. 2018. Amazon scraps secret AI\nrecruiting tool that showed bias against women.\nhttps:\/\/www :reuters :com\/article\/\nus-amazon-com-jobs-automation-\ninsight-idUSKCN1MK08G . Accessed:\n2021-02-25.\nMarcella De Marco and Piero Toto. 2019. Intro-\nduction: The Potential of Gender Training in the\nTranslation Classroom. In Gender Approaches\nin the Translation Classroom: Training the Do-\ners, pages 1\u20137. Palgrave Macmillan, Cham, CH.\nPieter Delobelle, Paul Temple, Gilles Perrouin,\nBeno \u02c6\u0131t Fr \u00b4enay, Patrick Heymans, and Bettina\nBerendt. 2020. Ethical Adversaries: Towards\nMitigating Unfairness with Adversarial Machine\nLearning. In Informal Proceedings of the Bias\nand Fairness in AI Workshop at ECML-PKDD\n(BIAS 2020) . BIAS 2020.\nHannah Devinney, Jenny Bj \u00a8orklund, and Henrik\nBj\u00a8orklund. 2020. Semi-Supervised Topic Mod-\neling for Gender Bias Discovery in English and\nSwedish. In Proceedings of the Second Work-\nshop on Gender Bias in Natural Language Pro-\ncessing , pages 79\u201392, Online. Association for\nComputational Linguistics.\nBruna Di Sabato and Antonio Perri. 2020. Gram-\nmatical gender and translation: A cross-\nlinguistic overview. In Luise von Flotow and\nHala Kamal, editors, The Routledge Handbook\nof Translation, Feminism and Gender . Rout-\nledge, New York, USA.\nCatherine D\u2019Ignazio and Lauren F. Klein. 2020.\nData feminism . MIT Press, London, UK.\nEmily Dinan, Angela Fan, Ledell Wu, Jason We-\nston, Douwe Kiela, and Adina Williams. 2020.\nMulti-Dimensional Gender Bias Classi\ufb01cation.\nInProceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP) , pages 314\u2013331, Online. Association\nfor Computational Linguistics.\nCynthia Dwork, Moritz Hardt, Toniann Pitassi,\nOmer Reingold, and Richard Zemel. 2012. Fair-\nness through Awareness. In Proceedings of the\n3rd Innovations in Theoretical Computer Science\nConference , ITCS \u201912, pages 214\u2013226, New\nYork, USA. Association for Computing Machin-\nery.\nPenelope Eckert and Sally McConnell-Ginet. 2013.\nLanguage and Gender . Cambridge University\nPress, Cambridge, UK.\nMostafa Elaraby, Ahmed Y . Taw\ufb01k, Mahmoud\nKhaled, Hany Hassan, and Aly Osama. 2018.\nGender Aware Spoken Language Translation Ap-\nplied to English-Arabic. In Proceedings of the\n2nd International Conference on Natural Lan-\nguage and Speech Processing (ICNLSP) , pages\n1\u20136, Algiers, DZ.\nCarolyn Epple. 1998. Coming to Terms with\nNavajo N \u00b4adleeh \u00b4\u0131: A Critique of Berdache,\n\u201cGay\u201d, \u201cAlternate Gender\u201d, and \u201cTwo-spirit\u201d.\nAmerican Ethnologist , 25(2):267\u2013290.\nCarlos Escolano, Marta R. Costa-juss `a, Jos \u00b4e A. R.\nFonollosa, and Mikel Artetxe. 2021. Multilin-\ngual Machine Translation: Closing the Gap be-\ntween Shared and Language-speci\ufb01c Encoder-\nDecoders. In Proceedings of the 16th conference\nof the European Chapter of the Association for\nComputational Linguistics (EACL) , Online.\nJoel Escud \u00b4e Font and Marta R. Costa-juss `a. 2019.\nEqualizing Gender Bias in Neural Machine\nTranslation with Word Embeddings Techniques.\nInProceedings of the First Workshop on Gen-\nder Bias in Natural Language Processing , pages\n147\u2013154, Florence, IT. Association for Compu-\ntational Linguistics.Anne Fausto-Sterling. 2019. Gender\/Sex, Sexual\nOrientation, and Identity Are in the Body: How\nDid They Get There? The Journal of Sex Re-\nsearch , 56(4-5):529\u2013555.\nAnke Frank, Christiane Hoffmann, and Maria Stro-\nbel. 2004. Gender Issues in Machine Translation.\nUniversity of Bremen .\nStella Frank, Desmond Elliott, and Lucia Specia.\n2018. Assessing multilingual multimodal image\ndescription: Studies of native speaker prefer-\nences and translator choices. Natural Language\nEngineering , 24(3):393\u2013413.\nBatya Friedman and Helen Nissenbaum. 1996.\nBias in Computer Systems. ACM Transactions\non Information Systems (TOIS) , 14(3):330\u2013347.\nMarco Gaido, Beatrice Savoldi, Luisa Bentivogli,\nMatteo Negri, and Marco Turchi. 2020. Breed-\ning Gender-aware Direct Speech Translation Sys-\ntems. In Proceedings of the 28th International\nConference on Computational Linguistics , pages\n3951\u20133964, Online. International Committee on\nComputational Linguistics.\nAparna Garimella, Carmen Banea, Dirk Hovy, and\nRada Mihalcea. 2019. Women\u2019s Syntactic Re-\nsilience and Men\u2019s Grammatical Luck: Gender-\nBias in Part-Of-Speech Tagging and Dependency\nParsing. In Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics , pages 3493\u20133498, Florence, IT. As-\nsociation for Computational Linguistics.\nTimnit Gebru. 2020. Race and gender. In\nMarkus D. Dubber, Frank Pasquale, and Sunit\nDas, editors, The Oxford Handbook of Ethics of\nAI. Oxford Handbook Online.\nRobert Geirhos, J \u00a8orn-Henrik Jacobsen, Claudio\nMichaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A. Wichmann. 2020.\nShortcut Learning in Deep Neural Networks.\nNature Machine Intelligence , 2(11):665\u2013673.\nMor Geva, Yoav Goldberg, and Jonathan Berant.\n2019. Are We Modeling the Task or the Annota-\ntor? An Investigation of Annotator Bias in Nat-\nural Language Understanding Datasets. In Pro-\nceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and\nthe 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP) ,\npages 1161\u20131166, Hong Kong, CN. Association\nfor Computational Linguistics.\nLisa Gitelman. 2013. Raw Data is an Oxymoron .\nMIT press.\nFiona Glen and Karen Hurrell. 2012.\nMeasuring gender identity. https:\n\/\/www :equalityhumanrights :com\/\nsites\/default\/files\/\ntechnical note final :pdf. Accessed:\n2021-02-25.\nBruce Glymour and Jonathan Herington. 2019.\nMeasuring the Biases That Matter: The Ethical\nand Casual Foundations for Measures of Fair-\nness in Algorithms. In Proceedings of the Con-\nference on Fairness, Accountability, and Trans-\nparency , FAT* \u201919, pages 269\u2013278, New York,\nUSA. Association for Computing Machinery.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant,\nRicardo Mu \u02dcnoz Sanchez, Mugdha Pandya, and\nAdam Lopez. 2020. Intrinsic Bias Metrics Do\nNot Correlate with Application Bias. arXiv\npreprint arXiv:2012.15859 .\nKirsten Gomard. 1995. The (Un)equal Treatment\nof Women in Language: a Comparative Study of\nDanish, English, and German. Working Papers\non Language, Gender and Sexism , 5(1):5\u201325.\nHila Gonen and Yoav Goldberg. 2019. Lipstick\non a Pig: Debiasing Methods Cover up System-\natic Gender Biases in Word Embeddings But do\nnot Remove Them. In Proceedings of the 2019\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers) , pages 609\u2013614, Minneapolis,\nMinnesota, USA. Association for Computational\nLinguistics.\nHila Gonen and Kellie Webster. 2020. Automat-\nically Identifying Gender Issues in Machine\nTranslation using Perturbations. In Findings of\nthe Association for Computational Linguistics:\nEMNLP 2020 , pages 1991\u20131995, Online. Asso-\nciation for Computational Linguistics.\nAnthony G. Greenwald, Debbie E. McGhee, and\nJordan L. K. Schwartz. 1998. Measuring in-\ndividual differences in implicit cognition: The\nImplicit Association Test. Journal of personality\nand social psychology , 74(6):1464.Liane Guillou. 2012. Improving Pronoun Trans-\nlation for Statistical Machine Translation. In\nProceedings of the Student Research Workshop\nat the 13th Conference of the European Chapter\nof the Association for Computational Linguis-\ntics, pages 1\u201310, Avignon, FR. Association for\nComputational Linguistics.\nPascal M. Gygax, Daniel Elmiger, Sandrine Zuf-\nferey, Alan Garnham, Sabine Sczesny, Lisa von\nStockhausen, Friederike Braun, and Jane Oakhill.\n2019. A Language Index of Grammatical Gen-\nder Dimensions to Study the Impact of Gram-\nmatical Gender on the Way We Perceive Women\nand Men. Frontiers in Psychology , 10:1604.\nPascal M. Gygax, Ute Gabriel, Oriane Sarrasin,\nJane Oakhill, and Alan Garnham. 2008. Gener-\nically Intended, but Speci\ufb01cally Interpreted:\nWhen Beauticians, Musicians and Mechanics\nare all Men. Language and Cognitive Processes ,\n23:464\u2013485.\nNizar Habash, Houda Bouamor, and Christine\nChung. 2019. Automatic Gender Identi\ufb01cation\nand Rein\ufb02ection in Arabic. In Proceedings of\nthe First Workshop on Gender Bias in Natural\nLanguage Processing , pages 155\u2013165, Florence,\nIT. Association for Computational Linguistics.\nPhilipp Hacker. 2018. Teaching Fairness to Arti\ufb01-\ncial Intelligence: Existing and Novel Strategies\nagainst Algorithmic Discrimination under EU\nLaw. Common market law review , 55(4):1143\u2013\n1185.\nKira Hall and Veronica O\u2019Donovan. 2014. Shifting\ngender positions among Hindi-speaking hijras.\nRethinking language and gender research: The-\nory and practice , pages 228\u2013266.\nFoad Hamidi, Morgan K. Scheuerman, and\nStacy M. Branham. 2018. Gender Recognition\nor Gender Reductionism? The Social Implica-\ntions of Embedded Gender Recognition Systems.\nInProceedings of the 2018 CHI Conference on\nHuman Factors in Computing Systems , CHI \u201918,\npages 1\u201313, New York, USA. Association for\nComputing Machinery.\nMykol C. Hamilton. 1988. Using masculine gener-\nics: Does generic he increase male bias in the\nuser\u2019s imagery? Sex roles , 19(11-12):785\u2013799.\nMykol C. Hamilton. 1991. Masculine Bias in\nthe Attribution of Personhood: People = Male,\nMale = People. Psychology of Women Quarterly ,\n15(3):393\u2013402.\nAlex Hanna, Andrew Smart, Ben Hutchinson,\nChristina Greer, Emily Denton, Margaret\nMitchell, Oddur Kjartansson, and Parker Barnes.\n2021. Towards Accountability for Machine\nLearning Datasets. In Proceedings of the Con-\nference on Fairness, Accountability, and Trans-\nparency (FAccT \u201921) , pages 560\u2013575, Online.\nACM.\nChristian Hardmeier, Marta R. Costa-juss `a, Kel-\nlie Webster, Will Radford, and Su Lin Blod-\ngett. 2021. How to Write a Bias Statement:\nRecommendations for Submissions to the Work-\nshop on Gender Bias in NLP. arXiv preprint\narXiv:2104.03026 .\nChristian Hardmeier and Marcello Federico. 2010.\nModelling Pronominal Anaphora in Statistical\nMachine Translation. In Proceedings of the sev-\nenth International Workshop on Spoken Lan-\nguage Translation (IWSLT) , pages 283\u2013289,\nParis, FR.\nLucy Havens, Melissa Terras, Benjamin Bach, and\nBeatrice Alex. 2020. Situated Data, Situated\nSystems: A Methodology to Engage with Power\nRelations in Natural Language Processing Re-\nsearch. In Proceedings of the Second Workshop\non Gender Bias in Natural Language Processing ,\npages 107\u2013124, Online. Association for Compu-\ntational Linguistics.\nMarlis Hellinger and Hadumond Bu\u00dfman. 2001.\nGender across Languages: The linguistic repre-\nsentation of women and men , volume 1. John\nBenjamins Publishing, Amsterdam, NL.\nMarlis Hellinger and Hadumond Bu\u00dfman. 2002.\nGender across Languages: The linguistic repre-\nsentation of women and men , volume 2. John\nBenjamins Publishing, Amsterdam, NL.\nMarlis Hellinger and Hadumond Bu\u00dfman. 2003.\nGender across Languages: The linguistic repre-\nsentation of women and men , volume 3. John\nBenjamins Publishing, Amsterdam, NL.\nMarlis Hellinger and Heiko Motschenbacher. 2015.\nGender Across Languages. The Linguistic Rep-resentation of Women and Men , volume 4. John\nBenjamins, Amsterdam, NL.\nLisa A. Hendricks, Kaylee Burns, Kate Saenko,\nTrevor Darrell, and Anna Rohrbach. 2018.\nWomen also Snowboard: Overcoming Bias in\nCaptioning Model. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV) ,\npages 740\u2013755, Munich, DE.\nAur\u00b4elie Herbelot, Eva von Redecker, and Johanna\nM\u00a8uller. 2012. Distributional Techniques for\nPhilosophical Enquiry. In Proceedings of the\n6th Workshop on Language Technology for Cul-\ntural Heritage, Social Sciences, and Humanities ,\npages 45\u201354, Avignon, FR. Association for Com-\nputational Linguistics.\nYasmeen Hitti, Eunbee Jang, Ines Moreno, and\nCarolyne Pelletier. 2019. Proposed Taxonomy\nfor Gender Bias in Text; A Filtering Methodol-\nogy for the Gender Generalization Subtype. In\nProceedings of the First Workshop on Gender\nBias in Natural Language Processing , pages 8\u2013\n17, Florence, IT. Association for Computational\nLinguistics.\nJanet Holmes and Miriam Meyerhoff. 2003. The\nHandbook of Language and Gender . Blackwell\nPublishing Ltd, Malden, USA.\nLevi C. R. Hord. 2016. Bucking the linguistic\nbinary: Gender neutral language in English,\nSwedish, French, and German. Western Papers\nin Linguistics \/ Cahiers linguistiques de Western ,\n3(1):4.\nDirk Hovy, Federico Bianchi, and Tommaso Forna-\nciari. 2020. \u201cYou Sound Just Like Your Father\u201d\nCommercial Machine Translation Systems In-\nclude Stylistic Biases. In Proceedings of the\n58th Annual Meeting of the Association for Com-\nputational Linguistics , pages 1686\u20131690, Online.\nAssociation for Computational Linguistics.\nDirk Hovy, Anders Johannsen, and Anders S\u00f8gaard.\n2015. User Review Sites as a Resource for\nLarge-Scale Sociolinguistic Studies. In Proceed-\nings of the 24th International Conference on\nWorld Wide Web , WWW \u201915, pages 452\u2013461,\nGeneva, CH. International World Wide Web\nConferences Steering Committee.\nDirk Hovy and Shannon L. Spruit. 2016. The So-\ncial Impact of Natural Language Processing. In\nProceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Vol-\nume 2: Short Papers) , pages 591\u2013598, Berlin,\nDE. Association for Computational Linguistics.\nJanet S. Hyde. 2005. The Gender Similarities\nHypothesis. American psychologist , 60(6):581\u2013\n592.\nJulia Ive, Pranava Madhyastha, and Lucia Specia.\n2019. Distilling Translations with Visual Aware-\nness. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 6525\u20136538, Florence, IT. Association for\nComputational Linguistics.\nAbigail Z. Jacobs. 2021. Measurement and Fair-\nness. In Proceedings of the 2021 ACM Con-\nference on Fairness, Accountability, and Trans-\nparency , FAccT \u201921, pages 375\u2013385, New York,\nUSA. Association for Computing Machinery.\nAbigail Z. Jacobs, Su Lin Blodgett, Solon Baro-\ncas, Hal Daum \u00b4e III, and Hanna Wallach. 2020.\nThe Meaning and Measurement of Bias: Lessons\nfrom Natural Language Processing. In Proceed-\nings of the 2020 Conference on Fairness, Ac-\ncountability, and Transparency , FAT* \u201920, page\n706, New York, USA. Association for Comput-\ning Machinery.\nRoman Jakobson. 1959. On Linguistic Aspects of\nTranslation. In Reuben A. Brower, editor, On\ntranslation , pages 232\u2013239. Harvard University\nPress, Cambridge, USA.\nMay Jiang and Christiane Fellbaum. 2020. Inter-\ndependencies of Gender and Race in Contex-\ntualized Word Embeddings. In Proceedings of\nthe Second Workshop on Gender Bias in Natu-\nral Language Processing , pages 17\u201325, Online.\nAssociation for Computational Linguistics.\nAnders Johannsen, Dirk Hovy, and Anders S\u00f8gaard.\n2015. Cross-lingual Syntactic Variation over\nAge and Gender. In Proceedings of the Nine-\nteenth Conference on Computational Natural\nLanguage Learning , pages 103\u2013112, Beijing,\nCN.\nKari Johnson. 2020a. AI Weekly: A deep learning\npioneer\u2019s teachable moment on AI bias. https:\n\/\/venturebeat :com\/2020\/06\/26\/ai-\nweekly-a-deep-learning-pioneers-teachable-moment-on-ai-bias\/ . Ac-\ncessed: 2021-02-25.\nMelvin Johnson. 2020b. A Scalable Approach\nto Reducing Gender Bias in Google Translate.\nhttps:\/\/ai :googleblog :com\/2020\/04\/\na-scalable-approach-to-reducing-\ngender :html . Accessed: 2021-02-25.\nRabeeh Karimi Mahabadi, Yonatan Belinkov, and\nJames Henderson. 2020. End-to-End Bias Miti-\ngation by Modelling Biases in Corpora. In Pro-\nceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics , pages\n8706\u20138716, Online. Association for Computa-\ntional Linguistics.\nOs Keyes. 2018. The Misgendering Machines:\nTrans\/HCI Implications of Automatic Gender\nRecognition. Proceedings of the ACM on\nHuman-Computer Interaction , 2(CSCW).\nOs Keyes, Chandler May, and Annabelle Carrell.\n2021. You Keep Using That Word: Ways of\nThinking about Gender in Computing Research.\nProceedings of the ACM on Human-Computer\nInteraction , 5(CSCW).\nYunsu Kim, Duc Thanh Tran, and Hermann Ney.\n2019. When and Why is Document-level Con-\ntext Useful in Neural Machine Translation? In\nProceedings of the Fourth Workshop on Dis-\ncourse in Machine Translation (DiscoMT 2019) ,\npages 24\u201334, Hong Kong, CN. Association for\nComputational Linguistics.\nKris Aric Knisely. 2020. Le fran c \u00b8ais non-binaire:\nLinguistic forms used by non-binary speakers of\nFrench. Foreign Language Annals , 53(4):850\u2013\n876.\nPhilipp Koehn. 2005. Europarl: A Parallel Corpus\nfor Statistical Machine Translation. In Proceed-\nings of the tenth Machine Translation Summit ,\npages 79\u201386, Phuket, TH. AAMT.\nCorina Koolen and Andreas van Cranenburgh.\n2017. These are not the Stereotypes You are\nLooking for: Bias and Fairness in Authorial Gen-\nder Attribution. In Proceedings of the First ACL\nWorkshop on Ethics in Natural Language Pro-\ncessing , pages 12\u201322, Valencia, ES. Association\nfor Computational Linguistics.\nCheris Kramarae and Paula A. Treichler. 1985. A\nFeminist Dictionary . Pandora Press, London,\nUK.\nMicha\u0142 Krawczyk. 2017. Are all Researchers\nMale? Gender Misattributions in Citations. Sci-\nentometrics , 110(3):1397\u20131402.\nHamutal Kreiner, Patrick Sturt, and Simon Garrod.\n2008. Processing De\ufb01nitional and Stereotypi-\ncal Gender in Reference Resolution: Evidence\nfrom Eye-Movements. Journal of Memory and\nLanguage , 58:239\u2013261.\nJames Kuczmarski. 2018. Reducing\nGender Bias in Google Translate.\nhttps:\/\/www :blog :google\/products\/\ntranslate\/reducing-gender-bias-\ngoogle-translate\/ . Accessed: 2021-02-\n25.\nWilliam Labov. 1972. Sociolinguistic Patterns . 4.\nUniversity of Pennsylvania Press.\nJennifer Langston. 2020. New AI tools\nhelp writers be more clear, concise and\ninclusive in Of\ufb01ce and across the Web.\nhttps:\/\/blogs :microsoft :com\/ai\/\nmicrosoft-365-ai-tools\/ . Accessed:\n2021-02-25.\nBrian Larson. 2017. Gender as a Variable in\nNatural-Language Processing: Ethical Consid-\nerations. In Proceedings of the First ACL Work-\nshop on Ethics in Natural Language Processing ,\npages 1\u201311, Valencia, ES. Association for Com-\nputational Linguistics.\nRonan Le Nagard and Philipp Koehn. 2010. Aiding\nPronoun Translation with Co-reference Resolu-\ntion. In Proceedings of the Joint Fifth Workshop\non Statistical Machine Translation and Metric-\nsMATR , pages 252\u2013261, Uppsala, SE. Associa-\ntion for Computational Linguistics.\nEnora Lessinger. 2020. Le pr \u00b4esident est une\nfemme: The Challenges of Translating Gender in\nUN texts. In Luise von Flotow and Hala Kamal,\neditors, The Routledge Handbook of Translation,\nFeminism and Gender . Routledge, New York,\nUSA.\nHector J. Levesque. 2014. On Our Best Behaviour.\nArti\ufb01cial Intelligence , 212(1):27\u201335.Roger J. R. Levesque. 2011. Sex Roles and Gender\nRoles . Springer, New York, USA.\nMolly Lewis and Gary Lupyan. 2020. Gender\nstereotypes are re\ufb02ected in the distributional\nstructure of 25 languages. Nature human be-\nhaviour , 4(10):1021\u20131028.\nYitong Li, Timothy Baldwin, and Trevor Cohn.\n2018. Towards Robust and Privacy-preserving\nText Representations. In Proceedings of the 56th\nAnnual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers) ,\npages 25\u201330, Melbourne, AU. Association for\nComputational Linguistics.\nDaniel J. Liebling, Michal Lahav, Abigail Evans,\nAaron Donsbach, Jess Holbrook, Boris Smus,\nand Lindsey Boran. 2020. Unmet Needs and\nOpportunities for Mobile Translation AI. In Pro-\nceedings of the 2020 CHI Conference on Human\nFactors in Computing Systems , CHI \u201920, page\n1\u201313, New York, USA. Association for Comput-\ning Machinery.\nAnna Lindqvist, Emma A. Renstr \u00a8om, and Marie\nGustafsson Send \u00b4en. 2019. Reducing a Male\nBias in Language? Establishing the Ef\ufb01ciency of\nthree Different Gender-fair Language Strategies.\nSex Roles , 81(1-2):109\u2013117.\nPierre Lison and J \u00a8org Tiedemann. 2016. OpenSub-\ntitles2016: Extracting Large Parallel Corpora\nfrom Movie and TV Subtitles. In Proceedings of\nthe Tenth International Conference on Language\nResources and Evaluation (LREC\u201916) , pages\n923\u2013929, Portoro \u02c7z, SI. European Language Re-\nsources Association (ELRA).\nKatherine A. Liu and Natalie A. Dipietro Mager.\n2016. Women\u2019s Involvement in Clinical Trials:\nHistorical Perspective and Future Implications.\nPharmacy Practice , 14(1):708.\n\u00b4Artemis L \u00b4opez. 2019. T \u00b4u, yo, elle\ny el lenguaje no binario. http:\/\/\nwww:lalinternadeltraductor :org\/n19\/\ntraducir-lenguaje-no-binario :html .\nAccessed: 2021-02-25.\n\u00b4Artemis L \u00b4opez, Susana Rodr \u00b4\u0131guez Barcia, and\nMar\u00b4\u0131a del Carmen Cabeza Pereiro. 2020.\nVisibilizar o interpretar: respuesta al Informe\nde la Real Academia Espa \u02dcnola sobre el\nlenguaje inclusivo y cuestiones conexas.\nhttp:\/\/www :ngenespanol :com\/el-\nmundo\/la-rae-rechaza-nuevamente-\nel-lenguaje-inclusivo\/ . Accessed:\n2021-02-25.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam\nAmancharla, and Anupam Datta. 2020. Gender\nBias in Neural Natural Language Processing. In\nLogic, Language, and Security , volume 12300\nofLecture Notes in Computer Science , pages\n189\u2013202. Springer.\nJohn Lyons. 1977. Semantics , volume 2. Cam-\nbridge University Press, Cambrdige, UK.\nThomas Manzini, Lim Yao Chong, Alan W. Black,\nand Yulia Tsvetkov. 2019. Black is to Crim-\ninal as Caucasian is to Police: Detecting and\nRemoving Multiclass Bias in Word Embeddings.\nInProceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 615\u2013621, Minneapolis, USA. Association\nfor Computational Linguistics.\nMarianna Martindale and Marine Carpuat. 2018.\nFluency Over Adequacy: A Pilot Study in Mea-\nsuring User Trust in Imperfect MT. In Proceed-\nings of the 13th Conference of the Association\nfor Machine Translation in the Americas (Vol-\nume 1: Research Track) , pages 13\u201325, Boston,\nUSA. Association for Machine Translation in\nthe Americas.\nChandler May. 2019. Deconstructing Gender Pre-\ndiction in NLP. In Conference on Neural Infor-\nmation Processing Systems (NIPS) \u2013 Keynote ,\nVancouver, CA.\nSally McConnell-Ginet. 2013. Gender and its Re-\nlation to Sex: The Myth of \u2018Natural\u2019 Gender. In\nGreville G. Corbett, editor, The Expression of\nGender , pages 3\u201338. De Gruyter Mouton, Berlin,\nDE.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the Wrong Reasons: Diagnosing Syn-\ntactic Heuristics in Natural Language Inference.\nInProceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics ,\npages 3428\u20133448, Florence, IT. Association for\nComputational Linguistics.Ninareh Mehrabi, Fred Morstatter, Nripsuta Sax-\nena, Kristina Lerman, and Aram Galstyan. 2019.\nA Survey on Bias and Fairness in Machine Learn-\ning.\nEmiel van Miltenburg. 2019. Pragmatic factors\nin (automatic) image description . Ph.D. thesis,\nVrije Universiteit, Amsterdam, NL.\nShachar Mirkin, Scott Nowson, Caroline Brun,\nand Julien Perez. 2015. Motivating Personality-\nAware Machine Translation. In Proceedings of\nthe 2015 Conference on Empirical Methods in\nNatural Language Processing , pages 1102\u20131108,\nLisbon, PT. Association for Computational Lin-\nguistics.\nMargaret Mitchell, Dylan Baker, Nyalleng\nMoorosi, Emily Denton, Ben Hutchinson, Alex\nHanna, Timnit Gebru, and Jamie Morgenstern.\n2020. Diversity and Inclusion Metrics in Sub-\nset Selection. In Proceedings of the AAAI\/ACM\nConference on AI, Ethics, and Society , AIES \u201920,\npages 117\u2013123, New York, USA. Association\nfor Computing Machinery.\nBritta Mondorf. 2002. Gender Differences in En-\nglish Syntax. Journal of English Linguistics ,\n30:158\u2013180.\nJohanna Monti. 2017. Questioni di Genere in\nTraduzione Automatica. Al femminile. Scritti\nlinguistici in onore di Cristina Vallini , 139:411\u2013\n431.\nJohanna Monti. 2020. Gender Issues in Machine\nTranslation: An Unsolved Problem? In Luise\nvon Flotow and Hala Kamal, editors, The Rout-\nledge Handbook of Translation, Feminism and\nGender , pages 457\u2013468. Routledge.\nAmit Moryossef, Roee Aharoni, and Yoav Gold-\nberg. 2019. Filling Gender & Number Gaps\nin Neural Machine Translation with Black-Box\nContext Injection. In Proceedings of the First\nWorkshop on Gender Bias in Natural Language\nProcessing , pages 49\u201354, Florence, IT. Associa-\ntion for Computational Linguistics.\nHeiko Motschenbacher. 2014. Grammatical gen-\nder as a challenge for language policy: The\n(im)possibility of non-heteronormative language\nuse in German versus English. Language policy ,\n13(3):243\u2013261.\nAnthony Mulac, James J. Bradac, and Pamela Gib-\nbons. 2001. Empirical Support for the Gender-\nas-Culture Hypothesis. Human Communication\nResearch , 27:121\u2013 152.\nEl Mundo. 2018. La RAE rechaza\nnuevamente el lenguaje inclusivo.\nhttps:\/\/www :ngenespanol :com\/el-\nmundo\/la-rae-rechaza-nuevamente-\nel-lenguaje-inclusivo\/ . Accessed:\n2021-02-25.\nDavid A. B. Murray. 2003. Who is Takat \u00afapui?\nM\u00afaori Language, Sexuality and Identity in\nAotearoa\/New Zealand. Anthropologica , pages\n233\u2013244.\nTerttu Nevalainen and Helena Raumolin-Brunberg.\n1993. Its Strength and the Beauty of it: The\nStandardization of the Third Person Neuter Pos-\nsessive in Early Modern English. In Dieter Stein\nand Ingrid Tieken-Boon van Ostade, editors, To-\nwards a Standard English , pages 171\u2013216. De\nGruyter, Berlin, DE.\nMatthew L Newman, Carla J Groom, Lori D Han-\ndelman, and James W Pennebaker. 2008. Gen-\nder differences in language use: An analysis\nof 14,000 text samples. Discourse Processes ,\n45(3):211\u2013236.\nDong Nguyen, A. Seza Do \u02d8gru\u00a8oz, Carolyn P. Ros \u00b4e,\nand Franciska de Jong. 2016. Computational\nSociolinguistics: A Survey. Computational lin-\nguistics , 42(3):537\u2013593.\nJan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\nWaibel. 2016. Pre-Translation for Neural Ma-\nchine Translation. In Proceedings of COLING\n2016, the 26th International Conference on Com-\nputational Linguistics: Technical Papers , pages\n1828\u20131836, Osaka, JP. The COLING 2016 Or-\nganizing Committee.\nUwe Kj\u00e6r Nissen. 2002. Aspects of Translating\nGender. Linguistik Online , 11(2).\nMalvina Nissim and Rob van der Goot. 2020. Fair\nis Better than Sensational: Man is to Doctor as\nWoman is to Doctor. Computational Linguistics ,\n46(2):487\u2013497.\nParmy Olson. 2018. The Algorithm That Helped\nGoogle Translate Become Sexist. https:\n\/\/www :forbes :com\/sites\/parmyolson\/2018\/02\/15\/the-algorithm-that-\nhelped-google-translate-become-\nsexist\/?sh=d675b9c7daa2 . Accessed:\n2021-02-25.\nDimitrios Papadimoulis. 2018. GENDER-\nNEUTRAL LANGUAGE in the European Par-\nliament . European Parliament 2018.\nBenjamin Papadopoulos. 2019. Morphological\nGender Innovations in Spanish of Genderqueer\nSpeakers . UC Berkeley: Library.\nKishore Papineni, Salim Roukos, Todd Ward, and\nWei-Jing Zhu. 2002. Bleu: a Method for Auto-\nmatic Evaluation of Machine Translation. In\nProceedings of the 40th Annual Meeting of\nthe Association for Computational Linguistics ,\npages 311\u2013318, Philadelphia, USA. Association\nfor Computational Linguistics.\nAmandalynne Paullada, Inioluwa D. Raji, Emily M.\nBender, Emily Denton, and Alex Hanna. 2020.\nData and its (dis)contents: A survey of dataset\ndevelopment and use in machine learning re-\nsearch. In NeurIPS 2020 Workshop: ML Retro-\nspectives, Surveys & Meta-analyses (ML-RSA) ,\nVitual.\nJames Pennebaker and Lori Stone. 2003. Words\nof Wisdom: Language Use Over the Life Span.\nJournal of personality and social psychology ,\n85:291\u2013301.\nMarcelo O. R. Prates, Pedro H. C. Avelar, and\nLu\u00b4\u0131s C. Lamb. 2018. Assessing gender bias in\nmachine translation: a case study with Google\nTranslate. Neural Computing and Applications ,\npages 1\u201319.\nElla Rabinovich, Raj N. Patel, Shachar Mirkin, Lu-\ncia Specia, and Shuly Wintner. 2017. Personal-\nized Machine Translation: Preserving Original\nAuthor Traits. In Proceedings of the 15th Con-\nference of the European Chapter of the Associ-\nation for Computational Linguistics: Volume 1,\nLong Papers , pages 1074\u20131084, Valencia, ES.\nAssociation for Computational Linguistics.\nIyad Rahwan, Manuel Cebrian, Nick Obradovich,\nJosh Bongard, Jean-Fran c \u00b8ois Bonnefon, Cyn-\nthia Breazeal, Jacob W. Crandall, Nicholas A.\nChristakis, Iain D. Couzin, Matthew O. Jack-\nson, et al. 2019. Machine Behaviour. Nature ,\n568(7753):477\u2013486.\nIsabelle R \u00b4egner, Catherine Thinus-Blanc, Agn `es\nNetter, Toni Schmader, and Pascal Huguet. 2019.\nCommittees with implicit biases promote fewer\nwomen when they do not believe gender bias\nexists. Nature human behaviour , 3(11):1171\u2013\n1179.\nArgentina A. Rescigno, Johanna Monti, Andy Way,\nand Eva Vanmassenhove. 2020. A Case Study\nof Natural Gender Phenomena in Translation:\nA Comparison of Google Translate, Bing Mi-\ncrosoft Translator and DeepL for English to Ital-\nian, French and Spanish. In Proceedings of the\nWorkshop on the Impact of Machine Translation\n(iMpacT 2020) , pages 62\u201390, Online. Associa-\ntion for Machine Translation in the Americas.\nAlexander S. Rich and Todd M. Gureckis. 2019.\nLessons for arti\ufb01cial intelligence from the study\nof natural stupidity. Nature Machine Intelli-\ngence , 1(4):174\u2013180.\nChristina Richards, Walter P. Bouman, Leighton\nSeal, Meg J. Barker, Timo O. Nieder, and Guy\nT\u2019Sjoen. 2016. Non-binary or Genderqueer\nGenders. International Review of Psychiatry ,\n28(1):95\u2013102.\nBarbara J. Risman. 2018. Gender as a Social Struc-\nture. In Barbara Risman, Carissa Froyum, and\nWilliam J Scarborough, editors, Handbook of the\nSociology of Gender , pages 19\u201343. Springer.\nNicholas Roberts, Davis Liang, Graham Neubig,\nand Zachary C. Lipton. 2020. Decoding and Di-\nversity in Machine Translation. In Proceedings\nof the Resistance AI Workshop at 34th Confer-\nence on Neural Information Processing Systems\n(NeurIPS 2020) , Vancouver, CA.\nSuzanne Romaine. 1999. Communicating Gender .\nLawrence Erlbaum, Mahwah, USA.\nSuzanne Romaine. 2001. A Corpus-Based View of\nGender in British and American English. Gen-\nder across languages , 1:153\u2013175.\nRachel Rudinger, Jason Naradowsky, Brian\nLeonard, and Benjamin Van Durme. 2018. Gen-\nder Bias in Coreference Resolution. In Proceed-\nings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technolo-\ngies, Volume 2 (Short Papers) , pages 8\u201314, NewOrleans, Louisiana. Association for Computa-\ntional Linguistics.\nRam Samar. 2020. Machines Are Indif-\nferent, We Are Not: Yann LeCun\u2019s\nTweet Sparks ML Bias Debate. https:\n\/\/analyticsindiamag :com\/yann-lecun-\nmachine-learning-bias-debate\/ . Ac-\ncessed: 2021-02-25.\nKalinowsky Santiago. 2018. Todos\/Todas\/Todes.\nInterview with Megan Figueroa, host; Carrie\nGillon, host. In The Vocal Fries [Podcast] , Van-\ncouver, CA.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan\nJurafsky, Noah A. Smith, and Yejin Choi. 2020.\nSocial Bias Frames: Reasoning about Social and\nPower Implications of Language. In Proceed-\nings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics , pages 5477\u2013\n5490, Online. Association for Computational\nLinguistics.\nDanielle Saunders and Bill Byrne. 2020. Reducing\nGender Bias in Neural Machine Translation as a\nDomain Adaptation Problem. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics , pages 7724\u20137736,\nOnline. Association for Computational Linguis-\ntics.\nDanielle Saunders, Rosie Sallis, and Bill Byrne.\n2020. Neural Machine Translation Doesn\u2019t\nTranslate Gender Coreference Right Unless You\nMake It. In Proceedings of the Second Workshop\non Gender Bias in Natural Language Processing ,\npages 35\u201343, Online. Association for Computa-\ntional Linguistics.\nLonda Schiebinger. 2014. Scienti\ufb01c Research\nMust Take Gender into Account. Nature ,\n507(9).\nAri Schlesinger, W. Keith Edwards, and Rebecca E.\nGrinter. 2017. Intersectional HCI: Engaging\nIdentity through Gender, Race, and Class. In\nProceedings of the 2017 CHI Conference on Hu-\nman Factors in Computing Systems , CHI \u201917,\npages 5412\u20135427, New York, USA. Association\nfor Computing Machinery.\nNatalie Schluter. 2018. The Glass Ceiling in NLP.\nInProceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing ,\npages 2793\u20132798, Brussels, BE. Association for\nComputational Linguistics.\nMuriel R. Schulz. 1975. The Semantic Derogation\nof Woman. In Barrie Thorne and Nancy Henley,\neditors, Sex and language. Difference and domi-\nnance , pages 64\u201375. Newbury House, Rowley,\nUSA.\nTal Schuster, Darsh Shah, Yun Jie Serene Yeo,\nDaniel Roberto Filizzola Ortiz, Enrico Santus,\nand Regina Barzilay. 2019. Towards Debias-\ning Fact Veri\ufb01cation Models. In Proceedings\nof the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the\n9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages\n3419\u20133425, Hong Kong, CN. Association for\nComputational Linguistics.\nSabine Sczesny, Christa Nater, and Alice H. Eagly.\n2018. Agency and communion: Their implica-\ntions for gender stereotypes and gender identi-\nties. In Agency and Communion in Social Psy-\nchology , pages 103\u2013116. Taylor and Francis.\nAndrew D. Selbst, Danah Boyd, Sorelle A. Friedler,\nSuresh Venkatasubramanian, and Janet Vertesi.\n2019. Fairness and Abstraction in Sociotechni-\ncal Systems. In Proceedings of the Conference\non Fairness, Accountability, and Transparency ,\nFAT* \u201919, pages 59\u201368, New York, USA. Asso-\nciation for Computing Machinery.\nRico Sennrich. 2017. How Grammatical is\nCharacter-Level Neural Machine Translation?\nAssessing MT Quality with Contrastive Trans-\nlation Pairs. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short\nPapers , pages 376\u2013382, Valencia, ES. Associa-\ntion for Computational Linguistics.\nDeven S. Shah, Hansen A. Schwartz, and Dirk\nHovy. 2020. Predictive Biases in Natural Lan-\nguage Processing Models: A Conceptual Frame-\nwork and Overview. In Proceedings of the 58th\nAnnual Meeting of the Association for Compu-\ntational Linguistics , pages 5248\u20135264, Online.\nAssociation for Computational Linguistics.\nAlyx J. Shroy. 2016. Innovations in gender-neutral\nFrench: Language practices of nonbinary French\nspeakers on Twitter. Ms., University of Califor-\nnia, Davis .Jeanette Silveira. 1980. Generic Masculine Words\nand Thinking. Women\u2019s Studies International\nQuarterly , 3(2-3):165\u2013178.\nFabian H. Sinz, Xaq Pitkow, Jacob Reimer,\nMatthias Bethge, and Andreas S. Tolias. 2019.\nEngineering a Less Arti\ufb01cial Intelligence. Neu-\nron, 103(6):967\u2013979.\nJanet Smith. 2003. Gendered Structures in\nJapanese. Gender across languages: The lin-\nguistic representation of women and men , 3:201\u2013\n227.\nMatthew Snover, Bonnie Dorr, Richard Schwartz,\nLinnea Micciulla, and John Makhoul. 2006. A\nStudy of Translation Edit Rate with Targeted Hu-\nman Annotation. In Proceedings of the 7th Con-\nference of the Association for Machine Transla-\ntion in the Americas , pages 223\u2013231, Cambridge,\nUSA. The Association for Machine Translation\nin the Americas.\nArt\u00afurs Stafanovi \u02c7cs, M \u00afarcis Pinnis, and Toms\nBergmanis. 2020. Mitigating Gender Bias in\nMachine Translation with Target Gender Anno-\ntations. In Proceedings of the Fifth Conference\non Machine Translation , pages 629\u2013638, Online.\nAssociation for Computational Linguistics.\nDagmar Stahlberg, Friederike Braun, Lisa Irmen,\nand Sabine Sczesny. 2007. Representation of\nthe Sexes in Language. Social Communication ,\npages 163\u2013187.\nGabriel Stanovsky, Noah A. Smith, and Luke\nZettlemoyer. 2019. Evaluating Gender Bias in\nMachine Translation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 1679\u20131684, Florence,\nIT. Association for Computational Linguistics.\nSimone Stumpf, Anicia Peters, Shaowen Bardzell,\nMargaret Burnett, Daniela Busse, Jessica\nCauchard, and Elizabeth Churchill. 2020.\nGender-inclusive HCI research and design: A\nconceptual review. Foundations and Trends in\nHuman\u2013Computer Interaction , 13(1):1\u201369.\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin\nHuang, Mai ElSherief, Jieyu Zhao, Diba\nMirza, Elizabeth Belding, Kai-Wei Chang, and\nWilliam Yang Wang. 2019. Mitigating Gender\nBias in Natural Language Processing: Litera-\nture Review. In Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics , pages 1630\u20131640, Florence, IT. As-\nsociation for Computational Linguistics.\nTony Sun, Kellie Webster, Apu Shah, William Y .\nWang, and Melvin Johnson. 2021. They, Them,\nTheirs: Rewriting with Gender-Neutral English.\narXiv preprint arXiv:2102.06788 .\nHarini Suresh and John V . Guttag. 2019. A\nframework for understanding unintended con-\nsequences of machine learning. arXiv preprint\narXiv:1901.10002 .\nMasashi Takeshita, Yuki Katsumata, Rafal Rzepka,\nand Kenji Araki. 2020. Can Existing Methods\nDebias Languages Other than English? First At-\ntempt to Analyze and Mitigate Japanese Word\nEmbeddings. In Proceedings of the Second\nWorkshop on Gender Bias in Natural Language\nProcessing , pages 44\u201355, Online. Association\nfor Computational Linguistics.\nTina Tallon. 2019. A Century of \u201cShrill\u201d: How\nBias in Technology Has Hurt Women\u2019s V oices.\nThe New Yorker .\nRachael Tatman. 2017. Gender and Dialect Bias in\nYouTube\u2019s Automatic Captions. In Proceedings\nof the First ACL Workshop on Ethics in Natural\nLanguage Processing , pages 53\u201359, Valencia,\nES. Association for Computational Linguistics.\nPeter Trudgill. 2000. Sociolinguistics: An Introduc-\ntion to Language and Society . Penguin Books,\nLondon, UK.\nAnne M. Turner, Megumu K. Brownstein, Kate\nCole, Hilary Karasz, and Katrin Kirchhoff. 2015.\nModeling Work\ufb02ow to Design Machine Trans-\nlation Applications for Public Health practice.\nJournal of Biomedical Informatics , 53:136\u2013146.\nAmos Tversky and Daniel Kahneman. 1973. Avail-\nability: A heuristic for judging frequency and\nprobability. Cognitive psychology , 5(2):207\u2013\n232.\nAmos Tversky and Daniel Kahneman. 1974. Judg-\nment under Uncertainty: Heuristics and Biases.\nScience , 185(4157):1124\u20131131.\nEva Vanmassenhove, Christian Hardmeier, and\nAndy Way. 2018. Getting Gender Right in Neu-\nral Machine Translation. In Proceedings of the2018 Conference on Empirical Methods in Nat-\nural Language Processing , pages 3003\u20133008,\nBrussels, BE. Association for Computational\nLinguistics.\nEva Vanmassenhove, Dimitar Shterionov, and\nMatthew Gwilliam. 2021. Machine Transla-\ntionese: Effects of Algorithmic Bias on Linguis-\ntic Complexity in Machine Translation. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational\nLinguistics: Main Volume , pages 2203\u20132213.\nEva Vanmassenhove, Dimitar Shterionov, and\nAndy Way. 2019. Lost in Translation: Loss and\nDecay of Linguistic Richness in Machine Trans-\nlation. In Proceedings of Machine Translation\nSummit XVII Volume 1: Research Track , pages\n222\u2013232, Dublin, IE. European Association for\nMachine Translation.\nMihaela V orvoreanu, Lingyi Zhang, Yun-Han\nHuang, Claudia Hilderbrand, Zoe Steine-\nHanson, and Margaret Burnett. 2019. From Gen-\nder Biases to Gender-Inclusive Design: An Em-\npirical Investigation. In Proceedings of the 2019\nCHI Conference on Human Factors in Comput-\ning Systems , CHI \u201919, page 1\u201314, New York,\nUSA. Association for Computing Machinery.\nClaudia Wagner, David Garcia, Mohsen Jadidi,\nand Markus Strohmaier. 2015. It\u2019s a man\u2019s\nWikipedia? Assessing gender inequality in an\nonline encyclopedia. In Proceedings of the In-\nternational AAAI Conference on Web and Social\nMedia , volume 9.\nMario Wandruszka. 1969. Sprachen: Vergleichbar\nund Vnvergleichlich . R. Piper & Co. Verlag,\nMunich, DE.\nZeerak Waseem. 2016. Are You a Racist or Am\nI Seeing Things? Annotator In\ufb02uence on Hate\nSpeech Detection on Twitter. In Proceedings of\nthe First Workshop on NLP and Computational\nSocial Science , pages 138\u2013142, Austin, USA.\nAssociation for Computational Linguistics.\nZeerak Waseem, Smarika Lulz, Joachim Bingel,\nand Isabelle Augenstein. 2020. Disembodied\nMachine Learning: On the Illusion of Objectiv-\nity in NLP. OpenReview Preprint.\nKellie Webster, Marta R. Costa-juss `a, Christian\nHardmeier, and Will Radford. 2019. Gendered\nambiguous pronoun (GAP) shared task at the\ngender bias in NLP workshop 2019. In Proceed-\nings of the First Workshop on Gender Bias in\nNatural Language Processing , pages 1\u20137, Flo-\nrence, IT. Association for Computational Lin-\nguistics.\nIlka B. Wolter and Bettina Hannover. 2016. Gender\nrole self-concept at school start and its impact\non academic self-concept and performance in\nmathematics and reading. European Journal of\nDevelopmental Psychology , 13(6):681\u2013703.\nJieyu Zhao, Subhabrata Mukherjee, Saghar\nHosseini, Kai-Wei Chang, and Ahmed Has-\nsan Awadallah. 2020. Gender Bias in Multi-\nlingual Embeddings and Cross-Lingual Transfer.\nInProceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics ,\npages 2896\u20132907, Online. Association for Com-\nputational Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente\nOrdonez, and Kai-Wei Chang. 2017. Men also\nlike Shopping: Reducing Gender Bias Ampli\ufb01-\ncation using Corpus-Level Constraints. In Pro-\nceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n2979\u20132989, Copenhagen, DK. Association for\nComputational Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente\nOrdonez, and Kai-Wei Chang. 2018a. Gender\nBias in Coreference Resolution: Evaluation and\nDebiasing Methods. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short\nPapers) , pages 15\u201320, New Orleans, USA. As-\nsociation for Computational Linguistics.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang,\nand Kai-Wei Chang. 2018b. Learning Gender-\nNeutral Word Embeddings. In Proceedings of\nthe 2018 Conference on Empirical Methods in\nNatural Language Processing , pages 4847\u20134853,\nBrussels, BE. Association for Computational\nLinguistics.\nPei Zhou, Weijia Shi, Jieyu Zhao, Kuan-Hao\nHuang, Muhao Chen, Ryan Cotterell, and Kai-\nWei Chang. 2019. Examining Gender Bias in\nLanguages with Grammatical Gender. In Pro-\nceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and\nthe 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP) ,\npages 5276\u20135284, Hong Kong, CN. Association\nfor Computational Linguistics.\nLal Zimman. 2020. Transgender language, trans-\ngender moment: Toward a trans linguistics. In\nKira Hall and Rusty Barrett, editors, The Oxford\nHandbook of Language and Sexuality . Oxford\nUniversity Press.\nLal Zimman, Evan Hazenberg, and Miriam Mey-\nerhoff. 2017. Trans people\u2019s linguistic self-\ndetermination and the dialogic nature of iden-\ntity. Representing trans: Linguistic, legal and\neveryday perspectives , pages 226\u2013248.\nRan Zmigrod, Sabrina J. Mielke, Hanna Wal-\nlach, and Ryan Cotterell. 2019. Counterfac-\ntual Data Augmentation for Mitigating Gender\nStereotypes in Languages with Rich Morphol-\nogy. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 1651\u20131661, Florence, IT. Association for\nComputational Linguistics.","metadata":{"primary_category":"cs.CL","published":"20210413","title":"Gender Bias in Machine Translation","updated":"20210507"}}