{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84acd29e",
   "metadata": {},
   "source": [
    "# Brief\n",
    "\n",
    "This guide aims to help you get started with generating embeddings for your own data. \n",
    "\n",
    "## Quick refresher\n",
    "\n",
    "Embeddings the what and why\n",
    "\n",
    "**The What ?**\n",
    "  - Embeddings are a way to represent unstructured data of different modalities text, audio or image data as vectors in a high-dimensional space. \n",
    "  - The distance between the embedding of a query and the embedding of a data point is small if the data point is relevant to the query.\n",
    "  - A search index makes use of approximate nearest neighbor search to retrieve the most relevant data for a given query.\n",
    "\n",
    "**The Why ?**\n",
    "  - Semantic search\n",
    "    -  [gong.io](https://www.gong.io/) for [searching sales conversations](https://www.pinecone.io/customers/gong/).\n",
    "  - QA using Retrieval augmented generation\n",
    "    - Notion AI's [Question Answering functionality](https://www.notion.so/help/notion-ai-security-practices)\n",
    "\n",
    "\n",
    "## Goal of this guide\n",
    "The goal of this guide is to show how to **generate embeddings at scale using Ray and Pinecone**.\n",
    "\n",
    "More specifically, we will cover how to:\n",
    "- Build a production-ready embeddings pipeline.\n",
    "- Use Ray Data to easily scale the generation of embeddings.\n",
    "- Assess Ray Data's pipeline's performance.\n",
    "- Upsert embeddings at scale with pinecone.\n",
    "- Query a search index with pinecone.\n",
    "\n",
    "## The road ahead\n",
    "\n",
    "Here is our roadmap for this guide:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "1. Setup\n",
    "2. Embeddings pipeline overview\n",
    "3. Simplest possible embedding pipeline\n",
    "4. Simple pipeline for a real use-case\n",
    "5. Migrating the simple pipeline to Ray Data\n",
    "6. Scaling the pipeline with Ray Data\n",
    "7. Upserting embeddings to Pinecone\n",
    "8. Querying Pinecone\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038051e6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569c771f-2c84-4016-8dea-0eb9d844d6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import psutil\n",
    "import ray\n",
    "import torch\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec, PodSpec\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1680d0",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6e7c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/cluster_storage')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path(\"/mnt/cluster_storage/\")\n",
    "shutil.copytree(Path(\"../data/\"), DATA_DIR, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da494e7d",
   "metadata": {},
   "source": [
    "## Embeddings pipeline overview\n",
    "\n",
    "What are the steps involved in generating embeddings? In the most common case for text data, the steps are as follows:\n",
    "\n",
    "1. Load documents\n",
    "2. Process documents into chunks\n",
    "   1. Process documents into chunks\n",
    "   2. Optionally persist chunks\n",
    "3. Generate embeddings from chunks\n",
    "   1. Generate embeddings from chunks\n",
    "   2. Optionally persist embeddings\n",
    "4. Upsert embeddings into a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eea7ad-1858-4dde-ad90-c07beeca48f4",
   "metadata": {},
   "source": [
    "## Simplest possible pipeline\n",
    "\n",
    "Let's start with the simplest implementation of these steps. As we go along, we will replace the simple implementation with real components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f866ca0",
   "metadata": {},
   "source": [
    "### 1. Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b69e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = [\"this is a document\", \"this is another document\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29ca55",
   "metadata": {},
   "source": [
    "### 2. Process documents into chunks\n",
    "\n",
    "In our case, we will chunk our documents into words - the simplest chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ba2bda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'document', 'this', 'is', 'another', 'document']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_fn(doc):\n",
    "    return doc.split(\" \")\n",
    "\n",
    "chunks = []\n",
    "for doc in dataset:\n",
    "    chunks.extend(chunk_fn(doc))\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b4a7d",
   "metadata": {},
   "source": [
    "### 3. Generate embeddings from chunks\n",
    "\n",
    "To keep it very simple, our embedding model is a lookup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79180510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_to_vec = {\n",
    "    \"this\": [0.1, 0.2],\n",
    "    \"is\": [0.3, 0.4],\n",
    "    \"a\": [0.5, 0.6],\n",
    "    \"document\": [0.7, 0.8],\n",
    "    \"another\": [0.9, 1.0],\n",
    "}\n",
    "word_to_vec[\"<UNK>\"] = [0.0, 0.0]\n",
    "\n",
    "\n",
    "def embed_model(word):\n",
    "    return word_to_vec.get(word, word_to_vec[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7003dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.1, 0.2],\n",
       " [0.3, 0.4],\n",
       " [0.5, 0.6],\n",
       " [0.7, 0.8],\n",
       " [0.1, 0.2],\n",
       " [0.3, 0.4],\n",
       " [0.9, 1.0],\n",
       " [0.7, 0.8]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = [embed_model(chunk) for chunk in chunks]\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f651c3d",
   "metadata": {},
   "source": [
    "#### 3b. Persist embeddings to disk\n",
    "\n",
    "We will use a simple json file to persist the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482bf17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dest_dir = DATA_DIR / \"simplest_pipeline\"\n",
    "\n",
    "dest_dir.mkdir(exist_ok=True, parents=True)\n",
    "with open(dest_dir / \"embeddings.json\", \"w\") as f:\n",
    "    json.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c98134",
   "metadata": {},
   "source": [
    "We confirm the data has been saved to the correct location by listing the contents of the directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1029ae67-851f-481f-ac2b-9eb6033e659d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "-rw-r--r-- 1 ray users  2 Mar  9 11:57 air.jsonl\n",
      "-rw-r--r-- 1 ray users 96 Mar  9 12:08 embeddings.json\n"
     ]
    }
   ],
   "source": [
    "!ls -llh {dest_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93016d07-0c94-4335-9d07-372ab269015e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Upsert embeddings to vector store\n",
    "\n",
    "The final step is to upsert the embeddings into a database. We will skip this step for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c493e7",
   "metadata": {},
   "source": [
    "## Simple pipeline for a real use-case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf7061",
   "metadata": {},
   "source": [
    "Let's now assume we want to \"embed the ray documentation website\". \n",
    "\n",
    "We will circle back and start with a small sample dataset taken from the ray documentation. \n",
    "\n",
    "To visualize our pipeline, see the diagram below:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-bootcamp-mar-2024/simple_embeddings_pipeline.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c035b5b",
   "metadata": {},
   "source": [
    "### 1. Load documents\n",
    "\n",
    "First step, we load the data using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7680f3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m [2024-03-09 13:09:48,471 E 2354 2354] (raylet) node_manager.cc:2948: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a, IP: 10.0.44.131) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.44.131`\n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_json(DATA_DIR / \"small_sample\" / \"sample-input.json\", lines=True)\n",
    "\n",
    "df = pd.read_json(DATA_DIR / \"small_sample\" / \"air.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6158e-febf-48ae-aa1d-2ddb1d3a99bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "We have a dataset of 4 documents fetched from online content and stored as objects in a json file.\n",
    "\n",
    "Here are some of the notable columns:\n",
    "- `text` column which contains the text of the document that we want to embed.\n",
    "- `section_url` column which contains the section under which the document is found.\n",
    "- `page_url` column which contains the page under which the document is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f189a745-6c07-432c-8a45-0e8f551e6a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2210.03945</td>\n",
       "      <td>http://arxiv.org/pdf/2210.03945</td>\n",
       "      <td>UNDERSTANDING HTML WITH LARGE LANGUAGE\\nMODELS...</td>\n",
       "      <td>{'primary_category': 'cs.LG', 'published': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1711.05101</td>\n",
       "      <td>http://arxiv.org/pdf/1711.05101</td>\n",
       "      <td>Published as a conference paper at ICLR 2019\\n...</td>\n",
       "      <td>{'primary_category': 'cs.LG', 'published': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2305.17493</td>\n",
       "      <td>http://arxiv.org/pdf/2305.17493</td>\n",
       "      <td>THECURSE OF RECURSION :\\nTRAINING ON GENERATED...</td>\n",
       "      <td>{'primary_category': 'cs.LG', 'published': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2205.09712</td>\n",
       "      <td>http://arxiv.org/pdf/2205.09712</td>\n",
       "      <td>2022-5-20\\nSelection-Inference: Exploiting Lar...</td>\n",
       "      <td>{'primary_category': 'cs.AI', 'published': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2104.06001</td>\n",
       "      <td>http://arxiv.org/pdf/2104.06001</td>\n",
       "      <td>Gender Bias in Machine Translation\\nBeatrice S...</td>\n",
       "      <td>{'primary_category': 'cs.CL', 'published': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>2204.06745</td>\n",
       "      <td>http://arxiv.org/pdf/2204.06745</td>\n",
       "      <td>GPT-NeoX-20B: An Open-Source Autoregressive La...</td>\n",
       "      <td>{'primary_category': 'cs.CL', 'published': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>1707.06347</td>\n",
       "      <td>http://arxiv.org/pdf/1707.06347</td>\n",
       "      <td>Proximal Policy Optimization Algorithms\\nJohn ...</td>\n",
       "      <td>{'primary_category': 'cs.LG', 'published': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>2307.09288</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "      <td>L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle : Open ...</td>\n",
       "      <td>{'primary_category': 'cs.CL', 'published': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2211.05100</td>\n",
       "      <td>http://arxiv.org/pdf/2211.05100</td>\n",
       "      <td>BLOOM: A 176B-Parameter Open-Access Multilingu...</td>\n",
       "      <td>{'primary_category': 'cs.CL', 'published': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>2009.10031</td>\n",
       "      <td>http://arxiv.org/pdf/2009.10031</td>\n",
       "      <td>1\\nTraining Production Language Models\\nwithou...</td>\n",
       "      <td>{'primary_category': 'cs.LG', 'published': '20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                           source   \n",
       "0    2210.03945  http://arxiv.org/pdf/2210.03945  \\\n",
       "1    1711.05101  http://arxiv.org/pdf/1711.05101   \n",
       "2    2305.17493  http://arxiv.org/pdf/2305.17493   \n",
       "3    2205.09712  http://arxiv.org/pdf/2205.09712   \n",
       "4    2104.06001  http://arxiv.org/pdf/2104.06001   \n",
       "..          ...                              ...   \n",
       "418  2204.06745  http://arxiv.org/pdf/2204.06745   \n",
       "419  1707.06347  http://arxiv.org/pdf/1707.06347   \n",
       "420  2307.09288  http://arxiv.org/pdf/2307.09288   \n",
       "421  2211.05100  http://arxiv.org/pdf/2211.05100   \n",
       "422  2009.10031  http://arxiv.org/pdf/2009.10031   \n",
       "\n",
       "                                                  text   \n",
       "0    UNDERSTANDING HTML WITH LARGE LANGUAGE\\nMODELS...  \\\n",
       "1    Published as a conference paper at ICLR 2019\\n...   \n",
       "2    THECURSE OF RECURSION :\\nTRAINING ON GENERATED...   \n",
       "3    2022-5-20\\nSelection-Inference: Exploiting Lar...   \n",
       "4    Gender Bias in Machine Translation\\nBeatrice S...   \n",
       "..                                                 ...   \n",
       "418  GPT-NeoX-20B: An Open-Source Autoregressive La...   \n",
       "419  Proximal Policy Optimization Algorithms\\nJohn ...   \n",
       "420  L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle : Open ...   \n",
       "421  BLOOM: A 176B-Parameter Open-Access Multilingu...   \n",
       "422  1\\nTraining Production Language Models\\nwithou...   \n",
       "\n",
       "                                              metadata  \n",
       "0    {'primary_category': 'cs.LG', 'published': '20...  \n",
       "1    {'primary_category': 'cs.LG', 'published': '20...  \n",
       "2    {'primary_category': 'cs.LG', 'published': '20...  \n",
       "3    {'primary_category': 'cs.AI', 'published': '20...  \n",
       "4    {'primary_category': 'cs.CL', 'published': '20...  \n",
       "..                                                 ...  \n",
       "418  {'primary_category': 'cs.CL', 'published': '20...  \n",
       "419  {'primary_category': 'cs.LG', 'published': '20...  \n",
       "420  {'primary_category': 'cs.CL', 'published': '20...  \n",
       "421  {'primary_category': 'cs.CL', 'published': '20...  \n",
       "422  {'primary_category': 'cs.LG', 'published': '20...  \n",
       "\n",
       "[423 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291194b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "**Considerations for scaling the pipeline:**\n",
    "- Memory: We currently load the entire file into memory. This is not a problem for small files, but can be a problem for large files.\n",
    "- Latency: Reading the file from disk is slow. We can speed this up by using a faster disk, but we can also speed this up by splitting the file into smaller files and reading them in parallel (more on this later).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b84e3-aca6-4b9d-bbb3-0b16ba5a6c30",
   "metadata": {},
   "source": [
    "### 2. Process documents into chunks\n",
    "\n",
    "We will use langchain's `RecursiveCharacterTextSplitter` to split the text into chunks. \n",
    "\n",
    "It works by first splitting on paragraphs, then sentences, then words, then characters. It is a recursive algorithm that will stop once the chunk size is satisfied.\n",
    "\n",
    "Let's try it out on a sampe document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5c0df2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first part. Estimate me like 12 words long.\\n\\nThis is the second part. Estimate me like 12 words long.',\n",
       " 'This is the third part. Estimate me like 12 words long.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "This is the first part. Estimate me like 12 words long.\n",
    "\n",
    "This is the second part. Estimate me like 12 words long.\n",
    "\n",
    "This is the third part. Estimate me like 12 words long.\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # The default separators used by the splitter\n",
    "    chunk_size=24,\n",
    "    chunk_overlap=0,\n",
    "    length_function=lambda x: len(x.split(\" \")),\n",
    ")\n",
    "splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0810e6d8",
   "metadata": {},
   "source": [
    "If we change the paragraphs, the chunk contents will change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77a22363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first part. Estimate me like 12 words long.',\n",
       " 'This is the second part. Estimate me like 12 words long.\\nThis is the third part. Estimate me like 12 words long.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "This is the first part. Estimate me like 12 words long.\n",
    "\n",
    "This is the second part. Estimate me like 12 words long.\n",
    "This is the third part. Estimate me like 12 words long.\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # The default separators used by the splitter\n",
    "    chunk_size=24,\n",
    "    chunk_overlap=0,\n",
    "    length_function=lambda x: len(x.split(\" \")),\n",
    ")\n",
    "splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de14fc",
   "metadata": {},
   "source": [
    "We now proceed to:\n",
    "\n",
    "1. Configure the `RecursiveCharacterTextSplitter`\n",
    "2. Run it over all the documents in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d80d46e9-7fd3-4c51-bf67-61dffbe6c8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 128  #  Chunk size is usually specified in tokens\n",
    "words_to_tokens = 1.2  # Heuristic for converting tokens to words\n",
    "chunk_size_in_words = int(chunk_size // words_to_tokens)\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_in_words,\n",
    "    length_function=lambda x: len(x.split()),\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for idx, row in df.iterrows():\n",
    "    for chunk in splitter.split_text(row[\"text\"]):\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"text\": chunk,\n",
    "                \"section_url\": row[\"id\"],\n",
    "                \"page_url\": row[\"source\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ff5d5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "**Considerations for choosing the chunk size**\n",
    "\n",
    "  - We want the chunks small enough to:\n",
    "    - Fit into the context window of our chosen embedding model\n",
    "    - Be semantically coherent - i.e. concentrate on ideally a single topic\n",
    "  - We want the chunks large enough to:\n",
    "    - Contain enough information to be semantically meaningful.\n",
    "    - Avoid creating too many embeddings which can be expensive to store and query.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a2912",
   "metadata": {},
   "source": [
    "Let's inspect the chunks produced for the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a6c9f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first document is 10438 words\n"
     ]
    }
   ],
   "source": [
    "first_document = df[\"text\"].iloc[0]\n",
    "print(\"first document is\", len(first_document.split()), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce8416b2-5ca1-40ad-9dc0-7505cbc97c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first chunk of first document is 98 words\n",
      "section_url 2210.03945\n",
      "page_url http://arxiv.org/pdf/2210.03945\n"
     ]
    }
   ],
   "source": [
    "for k, v in chunks[0].items():\n",
    "    if k == \"text\":\n",
    "        print(\"first chunk of first document is\", len(v.split()), \"words\")\n",
    "    else:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81caba36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second chunk of first document is 102 words\n",
      "section_url 2210.03945\n",
      "page_url http://arxiv.org/pdf/2210.03945\n"
     ]
    }
   ],
   "source": [
    "for k, v in chunks[1].items():\n",
    "    if k == \"text\":\n",
    "        print(\"second chunk of first document is\", len(v.split()), \"words\")\n",
    "    else:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506c675-6335-44aa-857d-8383bd1d58eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "**Considerations for switching to a recursive chunker:**\n",
    "\n",
    "- CPU: Recursive chunking is a CPU-intensive task which is being done in serial, iterating over every row. \n",
    "- Latency: Recursive chunking is slower than naive text splitting and is a blocking operation. We need to wait for the chunking to finish before we can start embedding.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617ebad-f9b8-4cc7-b29d-f69b4ca5036f",
   "metadata": {},
   "source": [
    "### 3. Generate embeddings from chunks\n",
    "\n",
    "For our third step, we want to load a good embedding model. \n",
    "\n",
    "**Suggested steps to choosing an embedding model:**\n",
    "1. Visit the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) on HuggingFace.\n",
    "2. Find a model that satisfies the following considerations:\n",
    "  - Does the model perform well overall and in the task you are interested in?\n",
    "  - Is the model closed-source or open-source?\n",
    "    - If it is closed-source:\n",
    "      - What are the costs, security, and privacy implications?\n",
    "    - If it is open-source:\n",
    "      - What are its resource requirements if you want to self-host it?\n",
    "      - Is it readily available as a service by third-party providers like Anyscale, Fireworks, or Togther AI?\n",
    "\n",
    "We will use `thenlper/gte-large` model from the [HuggingFace Model Hub](https://huggingface.co/thenlper/gte-large) given it is an open-source model and is available as a service by Anyscale and performs relatively well in the MTEB leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c301f9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.053916931152344"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmem = psutil.virtual_memory()\n",
    "\n",
    "# memory used in GB\n",
    "memory_used = svmem.total - svmem.available\n",
    "memory_used_gb_before_model_load = memory_used / (1024**3)\n",
    "memory_used_gb_before_model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3112f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcaa206a00514ec49be359253150ee2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739a48ce1a3d47b0bc56cc84f3c93442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba03cf4bf7646a09665502c0c1a723d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/67.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e1a6c93b0c4629adc4ff1c4c2b1595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bdee397564452fb7c0e524f8f80dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e588c1dab485428fafb2ca37d5d594da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bcf78d96e641e8a549998f2b1bdf6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b05c5e6ebc4042a3ef02372de3267e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dae2f541e04aa1b616e0489f7bb194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c284d93933a4b73a9fd90440eb97ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed30c89c0a804bcbb635f5ff31231b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b508ab1425f74468837262865a8e4daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d8d090d0ce40b6add8afe69bcfa8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5198d675ba7647bb90b7ee1dbf1b17c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfba276a11db4e9aaacf1e8b2fb3634f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75f6b885f5442ba9aa48bcbee495fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4e9ad0ae7a4a8f81af8a4c460029bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34e69e81210444c8d5fca57e1363561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.92 s, sys: 6.25 s, total: 9.17 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = SentenceTransformer('thenlper/gte-large', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b25590d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.332324981689453"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmem = psutil.virtual_memory()\n",
    "memory_used = svmem.total - svmem.available\n",
    "memory_used_gb_after_model_load = memory_used / (1024**3)\n",
    "memory_used_gb_after_model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd92639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2784080505371094"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_memory_usage = memory_used_gb_after_model_load - memory_used_gb_before_model_load\n",
    "model_memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad64a18",
   "metadata": {},
   "source": [
    "Loading the embedding model took around 1 GB of memory.\n",
    "\n",
    "Let's see how slow it is to generate an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9340e099",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1016\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1017\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1018\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1020\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1021\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1022\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    613\u001b[0m         past_key_value,\n\u001b[1;32m    614\u001b[0m         output_attentions,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = model.encode([chunk[\"text\"] for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f9601-0def-44fc-9852-74ea78abe329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49537d91",
   "metadata": {},
   "source": [
    "It takes on the order of a few seconds to embed 8 chunks on our CPU. We will most definitely need a GPU to speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35f3d5-eaeb-4338-a00b-d07cd477d034",
   "metadata": {},
   "source": [
    "#### Save embeddings to disk\n",
    "\n",
    "As a fourth step, we want to store our generated embeddings as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153e93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75eb31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output[\"embeddings\"] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2bca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aad3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output.to_parquet(DATA_DIR / \"small_sample\" / \"sample-output.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5a657-90c6-4af7-91d2-4e0ef75005cb",
   "metadata": {},
   "source": [
    "### 4. Upsert embeddings to vector store\n",
    "\n",
    "The final step is to upsert the embeddings into a database. We will skip this step for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ae888-4217-4240-9749-59997de57a36",
   "metadata": {},
   "source": [
    "## Migrating the simple pipeline to Ray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2203991",
   "metadata": {},
   "source": [
    "We now want to migrate our implementation to use Ray Data to drastically scale our pipeline for larger datasets.\n",
    "\n",
    "### 1. Load documents\n",
    "\n",
    "Let's start with a first pass conversion of our data pipeline to use Ray Data. \n",
    "\n",
    "Instead of `pandas.read_json`, use `ray.data.read_json` to instantiate a `ray.data.Dataset` that will eventually read our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02879966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 13:58:55,737\tINFO worker.py:1569 -- Connecting to existing Ray cluster at address: 10.0.18.223:6379...\n",
      "2024-03-09 13:58:55,746\tINFO worker.py:1744 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-ibknczdgu4wks3dek2647tkxx6.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2024-03-09 13:58:55,748\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_ce5950edc27d0d99aea3e3379ec688aa89757f6d.zip' (0.24MiB) to Ray cluster...\n",
      "2024-03-09 13:58:55,749\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_ce5950edc27d0d99aea3e3379ec688aa89757f6d.zip'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ray.data.dataset.Dataset"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ray.data.read_json(DATA_DIR / \"small_sample\" / \"air.jsonl\")\n",
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee87a5",
   "metadata": {},
   "source": [
    "`ray.data.read_json` returns a `ray.data.Dataset` which is a distributed collection of data. Execution in Ray Data by default is:\n",
    "- **Lazy**: `Dataset` transformations aren’t executed until you call a consumption operation.\n",
    "- **Streaming**: `Dataset` transformations are executed in a streaming way, incrementally on the base data, one block at a time.\n",
    "\n",
    "Accordingly `ray.data.Dataset` will only fetch back some high-level metadata and schema information about the file, but not the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b079ec1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column    Type\n",
       "------    ----\n",
       "id        string\n",
       "source    string\n",
       "text      string\n",
       "metadata  struct<primary_category: string, published: string, title: string, updated: string>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +2m50s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    }
   ],
   "source": [
    "ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c30d74",
   "metadata": {},
   "source": [
    "### Under the hood\n",
    "\n",
    "Ray Data uses Ray tasks to read files in parallel. Each read task reads one or more files and produces one or more output blocks.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-bootcamp-mar-2024/dataset-read-cropped-2.svg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa786d3c",
   "metadata": {},
   "source": [
    "### 2. Process documents into chunks\n",
    "\n",
    "Given a `ray.data.Dataset`, we can apply transformations to it. There are two types of transformations:\n",
    "1. **row-wise transformations**\n",
    "  - `map`: a 1-to-1 function that is applied to each row in the dataset.\n",
    "  - `filter`: a 1-to-1 function that is applied to each row in the dataset and filters out rows that don’t satisfy the condition.\n",
    "  - `flat_map`: a 1-to-many function that is applied to each row in the dataset and then flattens the results into a single dataset.\n",
    "2. **batch-wise transformations**\n",
    "  - `map_batches`: a 1-to-n function that is applied to each batch in the dataset.\n",
    "\n",
    "\n",
    "We chose to make use of `flat_map` to generate a list of chunk rows. `flat_map` will create `FlatMap` tasks which will be scheduled in parallel to process as many rows as possible at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f52bb9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_row(row):\n",
    "    chunk_size = 128\n",
    "    words_to_tokens = 1.2\n",
    "    num_tokens = int(chunk_size // words_to_tokens)\n",
    "\n",
    "    def get_num_words(text):\n",
    "        return len(text.split())\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=num_tokens,\n",
    "        keep_separator=True, \n",
    "        length_function=get_num_words, \n",
    "        chunk_overlap=0,\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for chunk in splitter.split_text(row[\"text\"]):\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"text\": chunk,\n",
    "                \"section_url\": row[\"source\"],\n",
    "                \"page_url\": row[\"id\"],\n",
    "            }\n",
    "        )\n",
    "    return chunks\n",
    "\n",
    "#ds = ds.flat_map(chunk_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5ee9b",
   "metadata": {},
   "source": [
    "To verify our `flat_map` is working, we can consume a limited number of rows from the dataset.\n",
    "\n",
    "To do so, we an either call\n",
    "- `take` to specify a limited number of rows from the dataset.\n",
    "- `take_batch` to specify a limited number of batches from the dataset.\n",
    "\n",
    "Here we call `take(2)` to return 2 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89bf2c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 13:10:43,635\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=64 for operator ReadJSON to satisfy parallelism at least twice the available number of CPUs (32).\n",
      "2024-03-09 13:10:43,635\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 64, each read task output is split into 64 smaller blocks.\n",
      "2024-03-09 13:10:43,636\tINFO streaming_executor.py:110 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON] -> TaskPoolMapOperator[FlatMap(chunk_row)] -> LimitOperator[limit=2]\n",
      "2024-03-09 13:10:43,636\tINFO streaming_executor.py:111 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-03-09 13:10:43,637\tINFO streaming_executor.py:113 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5372e792264fdea64993aaedf17e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'UNDERSTANDING HTML WITH LARGE LANGUAGE\\nMODELS\\nIzzeddin Gur, Oﬁr Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang\\nAakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust\\nGoogle Research\\nfizzeddin,ofirnachum,yingjiemiao,msafdari,austinvhuang\\nchowdhery,sharannarang,nfiedel,sandrafaust g@google.com\\nABSTRACT\\nLarge language models (LLMs) have shown exceptional performance on a va-\\nriety of natural language tasks. Yet, their capabilities for HTML understanding\\n– i.e., parsing the raw HTML of a webpage, with applications to automation of\\nweb-based tasks, crawling, and browser-assisted retrieval – have not been fully\\nexplored. We contribute HTML understanding models (ﬁne-tuned LLMs) and an\\nin-depth analysis of their capabilities under three tasks: (i) Semantic Classiﬁca-',\n",
       "  'section_url': '2210.03945',\n",
       "  'page_url': 'http://arxiv.org/pdf/2210.03945'},\n",
       " {'text': 'tionof HTML elements, (ii) Description Generation for HTML inputs, and (iii)\\nAutonomous Web Navigation of HTML pages. While previous work has devel-\\noped dedicated architectures and training procedures for HTML understanding,\\nwe show that LLMs pretrained on standard natural language corpora transfer re-\\nmarkably well to HTML understanding tasks. For instance, ﬁne-tuned LLMs are\\n12% more accurate at semantic classiﬁcation compared to models trained exclu-\\nsively on the task dataset. Moreover, when ﬁne-tuned on data from the MiniWoB\\nbenchmark, LLMs successfully complete 50% more tasks using 192x less data\\ncompared to the previous best supervised model. Out of the LLMs we evalu-',\n",
       "  'section_url': '2210.03945',\n",
       "  'page_url': 'http://arxiv.org/pdf/2210.03945'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c2abb",
   "metadata": {},
   "source": [
    "### 3. Generate embeddings from chunks\n",
    "\n",
    "For our third step, we apply the embeddings using `map_batches`, which will be implemented using `MapBatches` tasks scheduled in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e85a86f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_batch(batch):\n",
    "    assert isinstance(batch, dict)\n",
    "    for key in batch.keys():\n",
    "        assert key in [\"text\", \"section_url\", \"page_url\"]\n",
    "    for val in batch.values():\n",
    "        assert isinstance(val, np.ndarray), type(val)\n",
    "\n",
    "    model = SentenceTransformer('thenlper/gte-large')\n",
    "    text = batch[\"text\"].tolist()\n",
    "    embeddings = model.encode(text, batch_size=len(text))\n",
    "    batch[\"embeddings\"] = embeddings.tolist()\n",
    "    return batch\n",
    "\n",
    "ds = ds.map_batches(embed_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a97e92",
   "metadata": {},
   "source": [
    "#### Save embeddings to disk\n",
    "\n",
    "For our fourth step, we write our dataset to JSON using `write_json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e8c08db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 13:26:33,260\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=64 for operator ReadJSON to satisfy parallelism at least twice the available number of CPUs (32).\n",
      "2024-03-09 13:26:33,261\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 64, each read task output is split into 64 smaller blocks.\n",
      "2024-03-09 13:26:33,262\tINFO streaming_executor.py:110 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON] -> TaskPoolMapOperator[FlatMap(chunk_row)->MapBatches(embed_batch)->Write]\n",
      "2024-03-09 13:26:33,262\tINFO streaming_executor.py:111 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-03-09 13:26:33,263\tINFO streaming_executor.py:113 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e8f87eaa1c4025a0d887f07fbcfdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b3fd36b91ac59816d50355f690d039b4a9c8ba1703000000 Worker ID: 945214dbeb3b735ad11581b51e8e2de03aaa58b1a3e3c697c29c150c Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10052 Worker PID: 13013 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m [2024-03-09 13:26:48,497 E 2354 2354] (raylet) node_manager.cc:2948: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a, IP: 10.0.44.131) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.44.131`\n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: a36eeeba162a1182989c067640f77e892be88aea03000000 Worker ID: 8dc79c2d16857231b5aabf9a4a5d6986eeca80f23a7886de7c079694 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10064 Worker PID: 26821 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 3f784f24f7e5a5db02e4deb997b28263fa30147d03000000 Worker ID: 6ef1398f322e2f222267b9739c3601e6101215152298dcf9998eea55 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10056 Worker PID: 26464 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 73e24d1e16ed0a8d4e169f380e6dd42c5db861ef03000000 Worker ID: 48378791b463061ee44ddaa515694f5a79282e78d6ed96930ec5cb1d Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10070 Worker PID: 27583 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-03-09 13:26:59,779 E 2996 2996] (raylet) node_manager.cc:2948: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2, IP: 10.0.18.223) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.18.223`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 9742a0065dc7affb066125c61a519a48bbd29a6303000000 Worker ID: 58a64ad3ec60177dad46fadfd35e711a9a6dd81c205d70a581d2a1fc Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10047 Worker PID: 12535 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 20871f66953b44813b1cc73dade0fb4fc389deeb03000000 Worker ID: 887a293a0db2c66369620f3e9f86ac73023b8dbc142a7e67310af30e Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10051 Worker PID: 12944 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 08fafe10b3da128795c832891959f8b5ba4abf8803000000 Worker ID: 08cca031288c2d6e384776bdb29292e7edc45a7de78890e93681f41a Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10069 Worker PID: 14029 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 9a096c1ab9ef8e2203a6fdc4eba4f13115d72d5703000000 Worker ID: 61db73de0617a468a09493084902adb1cc85927be4cccd61d7b410f1 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10063 Worker PID: 13697 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 2a449a8a812c1543be32329abddffbfa702bfe0303000000 Worker ID: 2020354c53c5ad6ea49bd1357ae45dd906fc70225d734c74251e55df Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10076 Worker PID: 15111 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: a568357ba189cf6a08f7354635b308eadbaa129803000000 Worker ID: cd173cb6d9a36955a4e0fe31e7aeb676abc2317be1469e9e00263f9f Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10064 Worker PID: 13698 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 469e2edf999833c8bf0773af7e67f6255748a00c03000000 Worker ID: 6d71a5dd94be98b6a671c5dcf78681fd94c257b374d024e85a273497 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10066 Worker PID: 13866 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 82a6e87369243ac366e0ce462ffc601f682613cd03000000 Worker ID: de7ebd4c4c0f47ad6755b97bd2785e34e329d1e6cfea23fb05226c0f Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10051 Worker PID: 25883 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 9a0e1d54392036a10b652208044f2231ae33b61003000000 Worker ID: 58dfb857d8a70b84558d4488fefee6d4808bfc6b3e690884275c08f2 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10086 Worker PID: 28619 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 47c77b9ec2bf241858189678dc20d11fc6994dcf03000000 Worker ID: 2461e7459a9905efa7e97b79575da01e8cb4c7080cc4738b1e7bb26b Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10067 Worker PID: 13857 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m [2024-03-09 13:27:48,499 E 2354 2354] (raylet) node_manager.cc:2948: 11 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a, IP: 10.0.44.131) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.44.131`\n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: e97deee0abf46a5c843f91bb74b7462c9f53e5c103000000 Worker ID: 4218ae836d3786056cef00719265d84e3fc495ece1f2d92a72f34080 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10072 Worker PID: 14255 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b66c19c8de102eb31fad060955d3c2d767cb8a3203000000 Worker ID: de3435b06270a5054928e9c1537146971490deb3f1ed35aca02366f7 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10049 Worker PID: 25886 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-03-09 13:27:59,783 E 2996 2996] (raylet) node_manager.cc:2948: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2, IP: 10.0.18.223) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.18.223`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 9839f68687c53f1310aa223bc4036c9b5247477103000000 Worker ID: 7a49cf63ca083107e538e66ecb393bcd03e8c87ae92dfdf02798e529 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10080 Worker PID: 28230 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 86a4bca0c70e094a4004af8f1e70f90be7c3baeb03000000 Worker ID: 8e3828d712debde10ed3e2006b28265ca3e4a706ad8e3d8f5cc51ade Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10078 Worker PID: 15180 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: e66482eceb15addb892b2092af912710364fba2f03000000 Worker ID: 09000e1e2fcf40d1c8b20b3b6f568823fb212f1e88288352df50a699 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10082 Worker PID: 28372 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 41b31fc8e3ec1b821d71d773d6cab90c287859f503000000 Worker ID: 1e32e4aba63867faf3006590b98ee2b7e51615c802f3a52b0cb0f19e Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10077 Worker PID: 15179 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: d8269369f7302ca0c62db05659c4e87b5c84322e03000000 Worker ID: 34f34ed69882543e56182742736b7b487aae161d6719a6585723215c Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10088 Worker PID: 15726 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 5b3830abc2e8af83f7b026a3b0178628e4adccb103000000 Worker ID: 418803c46be122aff4bf2ce90fee2c3a970167180e04ccbf7a2fe331 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10052 Worker PID: 25942 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: be8b7d4e8a53c697ac96ab6b4d4337a59773496a03000000 Worker ID: b757e32984e7ed188c032f3af25604fd88040626f7c72a84afea125d Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10091 Worker PID: 15956 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b584a49b576029e5388cf302ad4916cf4c52879a03000000 Worker ID: 1a2266637c500f65e333b8aa0da83311f0f4d065780a3100e374cc00 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10081 Worker PID: 28315 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 4a3a0e524e1641dae781f9c07adeca0600cd18a403000000 Worker ID: 559c9a85a8a8965289da269969cac6e933830e7a42fbc64cb99c8dd1 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10098 Worker PID: 30860 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 2e1814d9ba6f2c723f435e7b9b12eff55b0a7e2503000000 Worker ID: f5e21692a221c3067f6f2159442e05e6c34a3bc8dc5ca8af26606abd Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10094 Worker PID: 16134 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: dc7e00cc51808dd077c930e30db06f81f229257a03000000 Worker ID: 218722c06b156468b633cb28908224c870e8e5324dcdf2f4772e2b88 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10086 Worker PID: 15728 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 0d9633891a3d01d77dee60ae7e11bf402bb3340c03000000 Worker ID: fcd98dca2590222271e769c13902b0f6c885487f623f6c98d2adfef2 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10093 Worker PID: 16133 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 344bdcd3e4d2e8ec0b82967f4beb1d7cdc8b3b6a03000000 Worker ID: 9ad5172ee39d13f320868e0078118475732cec99056d5290411f6b13 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10096 Worker PID: 16307 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: c19b431ccca95fe0e50da65ccec7a2aa1f60b8c803000000 Worker ID: 9ef0ca9e0fc15f4f1b20a280b598e10d8097f2d81dc00b9b9b46802e Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10108 Worker PID: 16954 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 86002eaab1bab920675e38b092f046ee28b0490b03000000 Worker ID: 2e4ee220048bca7ee29a3e3671c1598f1e37d306fe76161c1777c999 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10088 Worker PID: 30361 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m [2024-03-09 13:28:48,502 E 2354 2354] (raylet) node_manager.cc:2948: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a, IP: 10.0.44.131) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.44.131`\n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 12d9b07cbda429053ef48d7939f18407deaebfa203000000 Worker ID: e3cffc6bb3bbdbca1c231c30a37286eea8a22a00b5bb9b4abba3d544 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10101 Worker PID: 16572 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: e3778d63156d48f6d2262b0abc76ca16625434e203000000 Worker ID: f88d9181bd125e8eda1fb9d1246b48dd446d63d72f7c71d4700c2537 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10103 Worker PID: 16574 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 984ab86a361b7f2254d3e1e4bfdb0e2ff1174a6403000000 Worker ID: f8290db4c8198502b6ad0401464bda32ea8383e4d2ad0157129932ea Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10100 Worker PID: 16573 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: eb26427b5b75aa6598d7e64a931afad0ea8e691303000000 Worker ID: e8c29b695bba35a6264afd920837b3452ededa7d03fc83a5ba008c5d Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10090 Worker PID: 30371 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-03-09 13:29:05,300 E 2996 2996] (raylet) node_manager.cc:2948: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2, IP: 10.0.18.223) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.18.223`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 48450f2c28fbf06dcae9f2273e343d1d55c7cf1703000000 Worker ID: 198c269df8ef87f1d496411ed69b9f40dad92b323f2905b20a197705 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10089 Worker PID: 30360 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 27b33a2e4f0543239fe1f32b87c3a127b432a9a603000000 Worker ID: cd3e65b62066f9f8de46b6a1c0f81a226d93bf3b1567ee381b92edf9 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10096 Worker PID: 30837 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: beb94907f852f3a841cf49a6cd7d93b9c5d9657903000000 Worker ID: 46817d3a0522f42d238617363ac30a2b2838cbb246dde937d23eb259 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10102 Worker PID: 16571 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ad5e92f51c548d9a2c64e2b81859fea47f20d47403000000 Worker ID: d972cc97b54c5ee11338d186b7e6a2b68c664d854ed96f85047ce445 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10107 Worker PID: 16951 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 4993c00cf069eb263da255673c41a734c409bbc703000000 Worker ID: e562b7eda0c74f179ac658add041968a537a94f0e4290646fa9cfa6b Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10106 Worker PID: 16952 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: c5a6c78f0ff9576567e8d6e1b581393f7e4546e603000000 Worker ID: 95d7c2d74cedc3cfcd2721316b316a428c11d1468543ad7df0e0edcc Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10095 Worker PID: 30836 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 999ae362e52a44e054b2dc0fde492752f491653b03000000 Worker ID: f13e64631e9990c7951857ea110a53dceab2f8116afca831bdf3d67a Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10125 Worker PID: 18076 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m [2024-03-09 13:29:48,504 E 2354 2354] (raylet) node_manager.cc:2948: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a, IP: 10.0.44.131) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.44.131`\n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 499b86546f4ea8d104454dd8f9dd2b4aabf6d7a603000000 Worker ID: a097a34fa92b36932c63afc8af97fc041a3f61eb033fd9f02deb46cc Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10116 Worker PID: 17487 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 72efc4cf1708b9e847f23807ea062cf057f28f9b03000000 Worker ID: 833652120ae57a38563d7029efb12222d8fd17652ca8f799c2360fc8 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10110 Worker PID: 31678 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-03-09 13:30:05,305 E 2996 2996] (raylet) node_manager.cc:2948: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2, IP: 10.0.18.223) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.18.223`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 09fda81fc2a942d2b167f43f1ec9624f5d7a7f3203000000 Worker ID: 5be80e9316798d02a0f2a6660a77ef27f074042ef0f72d8c4599220b Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10117 Worker PID: 17624 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: debbc61657509c2ddb196b469a700e5d0843526c03000000 Worker ID: f97fe03b7f5586e9bec1c2b3ec983cfcfeda4dcf15893c23f82754bd Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10122 Worker PID: 32542 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: d547e4d2d9ee739d63c78de00124bfecf496030503000000 Worker ID: 14b5ba26d54dcf4b736ffc3531f83426d33d5858f91a5023bc6b2322 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10121 Worker PID: 17877 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 21a431d555a7cbf9daa2fddfec6a425234599a8703000000 Worker ID: 03ecba78b81ff34cbaee740127bec4caec6ba49f1b56097471bb525c Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10111 Worker PID: 31717 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b7c8bcdb113ddd6b1355018ae0e88f387f9d86b403000000 Worker ID: 783caaa35e20c6cde237ef4aaa9079b7bfd962363b9e1ae18a2b2d42 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10123 Worker PID: 18002 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: eeab6b7ea247d1660478003c644f6ba75ea680d503000000 Worker ID: 021d742e17a5162e08f1707f4c79108e8977a201b580f5f5b46dc77e Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10113 Worker PID: 31716 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 53aa49f1fcdc7354cee4d7c1c20b3725906bb25b03000000 Worker ID: ee98a70f2b828437a2996f173db746b13907d9d06b5fb09b720c1bd2 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10112 Worker PID: 31718 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: e943dcb8b3b9c93ae38bda02735dc09ee1b328f303000000 Worker ID: bd2566e9499658a25c360a83eb55a028589d9601c15325e542c3c9b0 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10124 Worker PID: 18001 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 4fb7d6b1035f242dbb173408d0e146062021ca9203000000 Worker ID: ecc8287537ccd7cea6d7d4536bf723604f8bd719e5e67d95e8ce0cae Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10132 Worker PID: 18549 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 78fd1b0124defb011afd1b4f2f29b4d8e04a30ef03000000 Worker ID: 69e030c9add310b3c1445cb6a3a9ae5483d5bb1cb49a1140eedf94ab Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10142 Worker PID: 19177 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 0b43718c19b47e921f084c7b378fa83fec2d91bb03000000 Worker ID: 4cc9e16c712051c7917d4922cdb19bf05642fe64081f04188c0154da Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10134 Worker PID: 18548 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m [2024-03-09 13:30:48,506 E 2354 2354] (raylet) node_manager.cc:2948: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a, IP: 10.0.44.131) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.44.131`\n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 4b566aa22cea1a7a84ab04d840f622a21f5012af03000000 Worker ID: f0f366332c801b78900dd629c4beea5645e5d4957611297c8175365a Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10133 Worker PID: 18547 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 7d0bd741dabe6eb6373d0b9df3aab26fda60e36a03000000 Worker ID: 1167de9a0576d2a50289324b7d98b2d40a8c771c0ff283e11a20875d Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10118 Worker PID: 32340 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 94266cd91b15d0caec5afc54d032eaa1094357b103000000 Worker ID: d2cd6af56fce5b78b82c9cfd0ddd0fb31479383a56b5a05d5c4d2173 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10135 Worker PID: 18729 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ef33f9e6a57d03ad9e694ffd8660004549b26ff203000000 Worker ID: 86511ada5bb9d66d71c3a5a4c837cfe6e1df83972edeceaa3daa0bb4 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10120 Worker PID: 32352 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-03-09 13:31:06,608 E 2996 2996] (raylet) node_manager.cc:2948: 10 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2, IP: 10.0.18.223) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.18.223`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: cda54ce49e4da2dc6009a4b2ddc0cf08a6bb1ad003000000 Worker ID: fb4696d630b5b4a88ea654bf615ce026682c54d3989943750bcf70f7 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10119 Worker PID: 32339 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: cff0304dfe6dc9072d2b09598fcb72774d5266d003000000 Worker ID: 5b851e53824cf6fa266dcb3573b62f59adb7fe66d1f2f0a259a43d50 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10137 Worker PID: 18869 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 15596df5feac861fde2a4f006b592f0de0449a9603000000 Worker ID: 76dfc822bb3f513b6333f2355dba4caadd1665146b2bded36333f182 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10138 Worker PID: 18868 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 8d97da457e3ccb24b4d69199b252e03d2fcd6f1b03000000 Worker ID: f118c5f65d68aaf8bbcada48951446b0826f5dde642826ce1b889906 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10127 Worker PID: 32931 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 7e3323ddedf68a66fead6de8974e3d2eec6a750703000000 Worker ID: d9a151819c4c08f6519bfadd2288e788f05a1b6b57cccbb7f78068b5 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10144 Worker PID: 19305 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 7ea132556fd5fa8f3a7336a02ec6a9cc0c3d88f703000000 Worker ID: deae46ebfec84bbc5105429b868ff7b638767b819fd800a9cd9c79e0 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10151 Worker PID: 19728 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 70e109336d9099618f32f1608501fbfb4a24bfc803000000 Worker ID: e1eabb361e848f7a298e266f3577c68144936696a03c6b44cbfb1725 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10130 Worker PID: 32970 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: db050d8f7ed02294ad5cdfc33a5e8d7d6002bf5c03000000 Worker ID: 27454fdb051953b8aba362112675b7e8546201e7a3ab43e4530b1e13 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10148 Worker PID: 19548 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: e55d77f96bc562227d6fd7ae56794b354117c44803000000 Worker ID: 4cb27048d0bf7533426f9d5245ae4c1352d8df779841c49129083e8e Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10149 Worker PID: 19613 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m [2024-03-09 13:31:48,509 E 2354 2354] (raylet) node_manager.cc:2948: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a, IP: 10.0.44.131) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.44.131`\n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 2d3d777bfac1447454573277aa9f06a03b91140003000000 Worker ID: 935d159e029132344e8f240a04735a61b013bd60476d2f9e68f63fde Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10128 Worker PID: 32933 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: efddc842b0b32b5e139421074875527851ce101003000000 Worker ID: f007b6145b1f756e16c2e792bd03bda62b6e851cc48ff47a761e9052 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10132 Worker PID: 33088 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 48fde69c6b1fab6a1e8e3c44c4d21b81c682cf3d03000000 Worker ID: bad7fbd0fe691a815355363962b0ead04bcbea025f7a3385be98bb18 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10147 Worker PID: 19492 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 2052fe8cc027f64cdce23cca5f3cceeb8d50c9d003000000 Worker ID: c11c8079f3961bc3558ca283db155b5bd4ed6d3eeb0219bcd37882e1 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10154 Worker PID: 19922 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: bd3a405e73d83b7818cdac014adaea44b6f9fac103000000 Worker ID: 0a6642255539f9d0acc8b87881ef8d198e74f143ad685bf6f37caf9d Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10129 Worker PID: 32971 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-03-09 13:32:06,611 E 2996 2996] (raylet) node_manager.cc:2948: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2, IP: 10.0.18.223) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.18.223`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 2cbbde8655007b8b0f83591221cc62247b19645703000000 Worker ID: 1684064fef635f5185b0176a30541828b9937bab9d39f25de55cf8b1 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10156 Worker PID: 19981 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 7aa2f8161fd3cdbe2967deaaf600407dabdc5f1503000000 Worker ID: fd331a24c712bc026a5a6a6ca64a2a1f2e66eb733f74fb541db859ec Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10135 Worker PID: 33479 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: d51e5a4718bab55bd7935f3ec4276e6ad2684ce303000000 Worker ID: 93b45511aeced4d20aaac0a6ec949183d3b8e78ddc77d5f57f9c9c3c Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10155 Worker PID: 19980 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fc5a851c8de8b0c38e3bafd23a1952aae7c0c4ee03000000 Worker ID: 11ef4bd83064cdfad4b6b69d885dcf6353e5f09dc906747f86466c22 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10139 Worker PID: 33738 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: cf45c2bd3f2bab0da4be60e044ac4df01726694b03000000 Worker ID: 270f72d3693ed264b6e0fbb901b28328699d250396c40197534020fa Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10158 Worker PID: 20169 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 4c24f1c677f0b45a6b140e8e6fd64d56ee18a32503000000 Worker ID: f7b334403dc4350c29b149902f59640df492428385f24bae53cbcda1 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10160 Worker PID: 20299 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 8d24f2d42d8ae30092ed1772ce4e4ad909e8d5a303000000 Worker ID: 63f40ef2959f5ace7a847cf6f66356380340ef0b6619cbc685f8333b Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10147 Worker PID: 34131 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 3f412cf751021010c1c3b1b69561d90181fb01ac03000000 Worker ID: 6b9beada8e6be7124e448bd66f12f6522ec3025fe242a3fa719c547b Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10161 Worker PID: 20309 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 8ebd7f1312356e75a1c73248dfa6aa66e425c30003000000 Worker ID: 06e698afa55d1650bd216968ed66f5e71614e4cf4e0dd2de4f924878 Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10145 Worker PID: 34157 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m [2024-03-09 13:32:48,511 E 2354 2354] (raylet) node_manager.cc:2948: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a, IP: 10.0.44.131) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.44.131`\n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b20e19e60ae0140bd05106ab8517c5b2cb714f3903000000 Worker ID: 8dfb2983bb139f0090d07e40c745055d09d242b4d53f0bdb1e3beeb4 Node ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a Worker IP address: 10.0.44.131 Worker port: 10166 Worker PID: 20674 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fd272c63e4ba895681f2528b3b53eab4db9af1fe03000000 Worker ID: 1646fdfe3f829d0cfe3af74461d5b358fa07512cf24822b2d7de3abe Node ID: 17a8cb9889c4716653cf7628f2f6966736f442fa8464c3074f6774d2 Worker IP address: 10.0.18.223 Worker port: 10159 Worker PID: 35055 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: Socket closed; RPC Error details: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py:2856\u001b[0m, in \u001b[0;36mDataset.write_json\u001b[0;34m(self, path, filesystem, try_create_dir, arrow_open_stream_args, filename_provider, block_path_provider, pandas_json_args_fn, num_rows_per_file, ray_remote_args, concurrency, **pandas_json_args)\u001b[0m\n\u001b[1;32m   2769\u001b[0m \u001b[39m\"\"\"Writes the :class:`~ray.data.Dataset` to JSON and JSONL files.\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \n\u001b[1;32m   2771\u001b[0m \u001b[39mThe number of files is determined by the number of blocks in the dataset.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2842\u001b[0m \u001b[39m        are dict(orient=\"records\", lines=True) by default.\u001b[39;00m\n\u001b[1;32m   2843\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2844\u001b[0m datasink \u001b[39m=\u001b[39m _JSONDatasink(\n\u001b[1;32m   2845\u001b[0m     path,\n\u001b[1;32m   2846\u001b[0m     pandas_json_args_fn\u001b[39m=\u001b[39mpandas_json_args_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2854\u001b[0m     dataset_uuid\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_uuid,\n\u001b[1;32m   2855\u001b[0m )\n\u001b[0;32m-> 2856\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_datasink(\n\u001b[1;32m   2857\u001b[0m     datasink,\n\u001b[1;32m   2858\u001b[0m     ray_remote_args\u001b[39m=\u001b[39;49mray_remote_args,\n\u001b[1;32m   2859\u001b[0m     concurrency\u001b[39m=\u001b[39;49mconcurrency,\n\u001b[1;32m   2860\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py:3596\u001b[0m, in \u001b[0;36mDataset.write_datasink\u001b[0;34m(self, datasink, ray_remote_args, concurrency)\u001b[0m\n\u001b[1;32m   3592\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m   3594\u001b[0m datasink\u001b[39m.\u001b[39mon_write_start()\n\u001b[0;32m-> 3596\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_ds \u001b[39m=\u001b[39m Dataset(plan, logical_plan)\u001b[39m.\u001b[39;49mmaterialize()\n\u001b[1;32m   3597\u001b[0m blocks \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_ds\u001b[39m.\u001b[39m_plan\u001b[39m.\u001b[39mexecute()\u001b[39m.\u001b[39mget_blocks())\n\u001b[1;32m   3598\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m   3599\u001b[0m     \u001b[39misinstance\u001b[39m(block, pd\u001b[39m.\u001b[39mDataFrame) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(block) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m blocks\n\u001b[1;32m   3600\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py:4556\u001b[0m, in \u001b[0;36mDataset.materialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4537\u001b[0m \u001b[39m\"\"\"Execute and materialize this dataset into object store memory.\u001b[39;00m\n\u001b[1;32m   4538\u001b[0m \n\u001b[1;32m   4539\u001b[0m \u001b[39mThis can be used to read all blocks into memory. By default, Dataset\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4553\u001b[0m \u001b[39m    A MaterializedDataset holding the materialized data blocks.\u001b[39;00m\n\u001b[1;32m   4554\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4555\u001b[0m copy \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m, _deep_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, _as\u001b[39m=\u001b[39mMaterializedDataset)\n\u001b[0;32m-> 4556\u001b[0m copy\u001b[39m.\u001b[39;49m_plan\u001b[39m.\u001b[39;49mexecute(force_read\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   4558\u001b[0m blocks \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39m_plan\u001b[39m.\u001b[39m_snapshot_blocks\n\u001b[1;32m   4559\u001b[0m blocks_with_metadata \u001b[39m=\u001b[39m blocks\u001b[39m.\u001b[39mget_blocks_with_metadata() \u001b[39mif\u001b[39;00m blocks \u001b[39melse\u001b[39;00m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/plan.py:572\u001b[0m, in \u001b[0;36mExecutionPlan.execute\u001b[0;34m(self, allow_clear_input_blocks, force_read, preserve_order)\u001b[0m\n\u001b[1;32m    567\u001b[0m metrics_tag \u001b[39m=\u001b[39m create_dataset_tag(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_uuid)\n\u001b[1;32m    568\u001b[0m executor \u001b[39m=\u001b[39m StreamingExecutor(\n\u001b[1;32m    569\u001b[0m     copy\u001b[39m.\u001b[39mdeepcopy(context\u001b[39m.\u001b[39mexecution_options),\n\u001b[1;32m    570\u001b[0m     metrics_tag,\n\u001b[1;32m    571\u001b[0m )\n\u001b[0;32m--> 572\u001b[0m blocks \u001b[39m=\u001b[39m execute_to_legacy_block_list(\n\u001b[1;32m    573\u001b[0m     executor,\n\u001b[1;32m    574\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    575\u001b[0m     allow_clear_input_blocks\u001b[39m=\u001b[39;49mallow_clear_input_blocks,\n\u001b[1;32m    576\u001b[0m     dataset_uuid\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_uuid,\n\u001b[1;32m    577\u001b[0m     preserve_order\u001b[39m=\u001b[39;49mpreserve_order,\n\u001b[1;32m    578\u001b[0m )\n\u001b[1;32m    579\u001b[0m stats \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mget_stats()\n\u001b[1;32m    580\u001b[0m stats_summary_string \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mto_summary()\u001b[39m.\u001b[39mto_string(\n\u001b[1;32m    581\u001b[0m     include_parent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    582\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:108\u001b[0m, in \u001b[0;36mexecute_to_legacy_block_list\u001b[0;34m(executor, plan, allow_clear_input_blocks, dataset_uuid, preserve_order)\u001b[0m\n\u001b[1;32m    102\u001b[0m dag, stats \u001b[39m=\u001b[39m _get_execution_dag(\n\u001b[1;32m    103\u001b[0m     executor,\n\u001b[1;32m    104\u001b[0m     plan,\n\u001b[1;32m    105\u001b[0m     preserve_order,\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m bundles \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mexecute(dag, initial_stats\u001b[39m=\u001b[39mstats)\n\u001b[0;32m--> 108\u001b[0m block_list \u001b[39m=\u001b[39m _bundles_to_block_list(bundles)\n\u001b[1;32m    109\u001b[0m \u001b[39m# Set the stats UUID after execution finishes.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m _set_stats_uuid_recursive(executor\u001b[39m.\u001b[39mget_stats(), dataset_uuid)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:204\u001b[0m, in \u001b[0;36m_bundles_to_block_list\u001b[0;34m(bundles)\u001b[0m\n\u001b[1;32m    202\u001b[0m blocks, metadata \u001b[39m=\u001b[39m [], []\n\u001b[1;32m    203\u001b[0m owns_blocks \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[39mfor\u001b[39;00m ref_bundle \u001b[39min\u001b[39;00m bundles:\n\u001b[1;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ref_bundle\u001b[39m.\u001b[39mowns_blocks:\n\u001b[1;32m    206\u001b[0m         owns_blocks \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/executor.py:37\u001b[0m, in \u001b[0;36mOutputIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RefBundle:\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_next()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py:144\u001b[0m, in \u001b[0;36mStreamingExecutor.execute.<locals>.StreamIterator.get_next\u001b[0;34m(self, output_split_idx)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_next\u001b[39m(\u001b[39mself\u001b[39m, output_split_idx: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RefBundle:\n\u001b[1;32m    143\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outer\u001b[39m.\u001b[39;49m_output_node\u001b[39m.\u001b[39;49mget_output_blocking(\n\u001b[1;32m    145\u001b[0m             output_split_idx\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outer\u001b[39m.\u001b[39m_global_info:\n\u001b[1;32m    148\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outer\u001b[39m.\u001b[39m_global_info\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m, dag\u001b[39m.\u001b[39m_estimated_output_blocks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor_state.py:283\u001b[0m, in \u001b[0;36mOpState.get_output_blocking\u001b[0;34m(self, output_split_idx)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mif\u001b[39;00m ref \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m ref\n\u001b[0;32m--> 283\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output_path = DATA_DIR / \"small_sample\" / \"sample-output\"\n",
    "if output_path.exists():\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "ds.write_json(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6ccf4",
   "metadata": {},
   "source": [
    "We inspect the created JSON output directory. Every write task will create a separate file in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a389e8f8-e2c1-484c-9df4-c2e383b34fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "drwxr-xr-x 2 ray users 6.0K Mar  9 13:10 .\n",
      "drwxr-xr-x 3 ray users 6.0K Mar  9 13:10 ..\n"
     ]
    }
   ],
   "source": [
    "!ls -llah {output_path} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16a305d0-81e0-4c93-9770-9ed0a8b65e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmall_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample-output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/read_api.py:1176\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(paths, filesystem, parallelism, ray_remote_args, arrow_open_stream_args, meta_provider, partition_filter, partitioning, include_paths, ignore_missing_paths, shuffle, file_extensions, concurrency, override_num_blocks, **arrow_json_args)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m meta_provider \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1174\u001b[0m     meta_provider \u001b[39m=\u001b[39m get_generic_metadata_provider(JSONDatasource\u001b[39m.\u001b[39m_FILE_EXTENSIONS)\n\u001b[0;32m-> 1176\u001b[0m datasource \u001b[39m=\u001b[39m JSONDatasource(\n\u001b[1;32m   1177\u001b[0m     paths,\n\u001b[1;32m   1178\u001b[0m     arrow_json_args\u001b[39m=\u001b[39;49marrow_json_args,\n\u001b[1;32m   1179\u001b[0m     filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   1180\u001b[0m     open_stream_args\u001b[39m=\u001b[39;49marrow_open_stream_args,\n\u001b[1;32m   1181\u001b[0m     meta_provider\u001b[39m=\u001b[39;49mmeta_provider,\n\u001b[1;32m   1182\u001b[0m     partition_filter\u001b[39m=\u001b[39;49mpartition_filter,\n\u001b[1;32m   1183\u001b[0m     partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[1;32m   1184\u001b[0m     ignore_missing_paths\u001b[39m=\u001b[39;49mignore_missing_paths,\n\u001b[1;32m   1185\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1186\u001b[0m     include_paths\u001b[39m=\u001b[39;49minclude_paths,\n\u001b[1;32m   1187\u001b[0m     file_extensions\u001b[39m=\u001b[39;49mfile_extensions,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[1;32m   1189\u001b[0m \u001b[39mreturn\u001b[39;00m read_datasource(\n\u001b[1;32m   1190\u001b[0m     datasource,\n\u001b[1;32m   1191\u001b[0m     parallelism\u001b[39m=\u001b[39mparallelism,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     override_num_blocks\u001b[39m=\u001b[39moverride_num_blocks,\n\u001b[1;32m   1195\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/datasource/json_datasource.py:30\u001b[0m, in \u001b[0;36mJSONDatasource.__init__\u001b[0;34m(self, paths, arrow_json_args, **file_based_datasource_kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     22\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     23\u001b[0m     paths: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfile_based_datasource_kwargs,\n\u001b[1;32m     27\u001b[0m ):\n\u001b[1;32m     28\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m \u001b[39mimport\u001b[39;00m json\n\u001b[0;32m---> 30\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(paths, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfile_based_datasource_kwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m arrow_json_args \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m         arrow_json_args \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/datasource/file_based_datasource.py:130\u001b[0m, in \u001b[0;36mFileBasedDatasource.__init__\u001b[0;34m(self, paths, filesystem, schema, open_stream_args, meta_provider, partition_filter, partitioning, ignore_missing_paths, shuffle, include_paths, file_extensions)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_include_paths \u001b[39m=\u001b[39m include_paths\n\u001b[1;32m    129\u001b[0m paths, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filesystem \u001b[39m=\u001b[39m _resolve_paths_and_filesystem(paths, filesystem)\n\u001b[0;32m--> 130\u001b[0m paths, file_sizes \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mlist\u001b[39m,\n\u001b[1;32m    132\u001b[0m     \u001b[39mzip\u001b[39m(\n\u001b[1;32m    133\u001b[0m         \u001b[39m*\u001b[39mmeta_provider\u001b[39m.\u001b[39mexpand_paths(\n\u001b[1;32m    134\u001b[0m             paths,\n\u001b[1;32m    135\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filesystem,\n\u001b[1;32m    136\u001b[0m             partitioning,\n\u001b[1;32m    137\u001b[0m             ignore_missing_paths\u001b[39m=\u001b[39mignore_missing_paths,\n\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    139\u001b[0m     ),\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m ignore_missing_paths \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(paths) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNone of the provided paths exist. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore_missing_paths\u001b[39m\u001b[39m'\u001b[39m\u001b[39m field is set to True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m [2024-03-09 13:12:48,477 E 2354 2354] (raylet) node_manager.cc:2948: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d9e5357aad0282ee3a993afcc0ec4c5c9985a2ce3658c5d3bf9d961a, IP: 10.0.44.131) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.44.131`\n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m \n",
      "\u001b[33m(raylet, ip=10.0.44.131)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "ray.data.read_json(DATA_DIR / \"small_sample\" / \"sample-output\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f7b49-4a0c-4160-bc35-341894ad4149",
   "metadata": {},
   "source": [
    "### 4. Upsert embeddings to vector store\n",
    "\n",
    "The final step is to upsert the embeddings into a database. We will skip this step for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efabf980",
   "metadata": {},
   "source": [
    "**Recap**\n",
    "\n",
    "Here is our entire pipeline:\n",
    "\n",
    "```python\n",
    "(\n",
    "    ray.data.read_json(DATA_DIR / \"small_sample\" / \"sample-input.json\")\n",
    "    .flat_map(chunk_row)\n",
    "    .map_batches(embed_batch)\n",
    "    .write_json(DATA_DIR / \"small_sample\" / \"sample-output\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e0822",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Use a different embedding model\n",
    "\n",
    "Re-run the entire data pipeline but this time use a different embedding model `BAAI/bge-large-en-v1.5` which outperforms `thenlper/gte-large` on certain parts of the MTEB leaderboard.\n",
    "\n",
    "NOTE: make sure to output the results to a different directory.\n",
    "\n",
    "<details> \n",
    "\n",
    "<summary>Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "def embed_batch(batch):\n",
    "    # Load the embedding model\n",
    "    model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "    text = batch[\"text\"].tolist()\n",
    "    embeddings = model.encode(text, batch_size=len(text))\n",
    "    batch[\"embeddings\"] = embeddings.tolist()\n",
    "    return batch\n",
    "\n",
    "(\n",
    "    ray.data.read_json(DATA_DIR / \"small_sample\" / \"sample-input.json\")\n",
    "    .flat_map(chunk_row)\n",
    "    .map_batches(embed_batch)\n",
    "    .write_json(DATA_DIR / \"small_sample\" / \"sample-output-bge\")\n",
    ")\n",
    "\n",
    "# inspect output\n",
    "ray.data.read_json(DATA_DIR / \"small_sample\" / \"sample-output-bge\").to_pandas()\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e64859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7880df",
   "metadata": {},
   "source": [
    "## Scaling the pipeline with Ray Data\n",
    "\n",
    "Let's explore how to scale our pipeline to a larger dataset using Ray Data.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-bootcamp-mar-2024/full_scale_embeddings_pipeline.svg\" width=\"1000px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b675c09",
   "metadata": {},
   "source": [
    "### Phase 1: Preparing input files\n",
    "\n",
    "First, we need to prepare our documents by performing the following steps\n",
    "1. Fetch all the Ray documentation from the web.\n",
    "2. Parse the web pages to extract the text.\n",
    "3. Store the text into input files that are ideal for Ray Data.\n",
    "\n",
    "Consider converting to parquet files which allow for pruning to improve reads:\n",
    "- When working with column-oriented file formats like parquet, specify which columns you want to read. This might help significantly reduce the memory footprint of the read task.\n",
    "- Similarly, you can pass in a filter to `ray.data.read_parquet()` (filter pushdown) which is applied at the file scan so only rows that match the filter predicate are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c2c2f",
   "metadata": {},
   "source": [
    "#### 1. Fetch all the Ray documentation from the web.\n",
    "\n",
    "We can make use of `wget` to crawl the web and download all the webpages under a given domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c88dbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_web_pages_dir = DATA_DIR / \"full_scale\" / \"00_raw_web_pages\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635c38a",
   "metadata": {},
   "source": [
    "If you uncomment the following cell, it will crawl the web pages and save them to the `raw_web_pages_dir` directory.This will take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7ef19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://docs.ray.io/en/master/ -e robots=off --recursive --page-requisites \\\n",
    "#   --html-extension --convert-links --restrict-file-names=windows \\\n",
    "#   --domains docs.ray.io --no-parent --accept=html --retry-on-http-error=429 \\\n",
    "#   -P {raw_web_pages_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a475e72",
   "metadata": {},
   "source": [
    "Instead we will fetch a zip file containing the webpages and extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d556b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf {raw_web_pages_dir}\n",
    "!wget https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-ray-documentation-html-files/ray_docs_web_pages.zip \\\n",
    "    -P {str(raw_web_pages_dir)}\n",
    "!ls -ll {raw_web_pages_dir}\n",
    "!unzip -o {raw_web_pages_dir / \"ray_docs_web_pages.zip\"} -d {raw_web_pages_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ecabf",
   "metadata": {},
   "source": [
    "We count the total number of files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3126f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -R {raw_web_pages_dir} | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47892328",
   "metadata": {},
   "source": [
    "We also take the total size of the raw web pages directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908fbda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!du -sh {raw_web_pages_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc4601",
   "metadata": {},
   "source": [
    "Note that this only includes the latest version of the ray documentation. This size would easily be in the gegabytes if we included all versions of the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2b87f",
   "metadata": {},
   "source": [
    "#### Parse the web pages to extract the text.\n",
    "\n",
    "We first read all HTML files in the raw web pages directory into a `ray.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe9021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ray.data.from_items(\n",
    "    [{\"path\": path} for path in raw_web_pages_dir.rglob(\"*.html\") if not path.is_dir()]\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130e7e5",
   "metadata": {},
   "source": [
    "We then implement a function to extract the text from the HTML files. Given for each HTML file, we extract a single document, we will use `map` to apply the function to each row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e5f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_to_uri(\n",
    "    path: str, scheme: str = \"https://\", domain: str = \"docs.ray.io\"\n",
    ") -> str:\n",
    "    return scheme + domain + str(path).split(domain)[-1]\n",
    "\n",
    "\n",
    "def extract_document_from_html(row: dict) -> list[dict]:\n",
    "    \"\"\"Extract a document from an HTML file.\"\"\"\n",
    "    # 1. Request the page and extract the text using BeautifulSoup\n",
    "    with open(row[\"path\"], \"r\", encoding=\"utf-8\") as html_file:\n",
    "        soup = BeautifulSoup(html_file, \"html.parser\")\n",
    "\n",
    "    # 2. Create a document object with the text and page_url\n",
    "    return {\n",
    "        \"text\": soup.text,\n",
    "        \"page_url\": path_to_uri(row[\"path\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "ds = ds.map(extract_document_from_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34eb8ff",
   "metadata": {},
   "source": [
    "For now we write out the data to a \"01_documents\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07bd6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if (DATA_DIR / \"full_scale\" / \"01_documents\").exists():\n",
    "    shutil.rmtree(DATA_DIR / \"full_scale\" / \"01_documents\")\n",
    "ds.write_json(DATA_DIR / \"full_scale\" / \"01_documents\", num_rows_per_file=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f98adc",
   "metadata": {},
   "source": [
    "We inspect the produced documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d8b4be-4f1d-4ae2-a9c7-d861d3c412e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mray\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mread_json(DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ray' is not defined"
     ]
    }
   ],
   "source": [
    "ray.data.read_json(DATA_DIR / \"small_sample\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f788923",
   "metadata": {},
   "source": [
    "##### Utilize inherent structure to improve the documents \n",
    "\n",
    "Documentation [webpages](https://docs.ray.io/en/latest/rllib/rllib-env.html) are naturally split into sections. We can use this to our advantage by returning our documents as sections. This will facilitate producing semantically coherent chunks. \n",
    "\n",
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/1eFnKmG5xqPIFtPupZ327X/f6152723e18322b90aaa8be5d2d5a6e4/image5.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8348cd",
   "metadata": {},
   "source": [
    "We re-instantiate the dataset from the HTML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b892b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ray.data.from_items(\n",
    "    [{\"path\": path} for path in raw_web_pages_dir.rglob(\"*.html\") if not path.is_dir()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5a4dc",
   "metadata": {},
   "source": [
    "This time we are producing multiple documents from each HTML file. We will use the `flat_map` method to produce multiple documents from each HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44037192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_sections_from_html(record: dict) -> list[dict]:\n",
    "    documents = []\n",
    "    # 1. Request the page and parse it using BeautifulSoup\n",
    "    with open(record[\"path\"], \"r\", encoding=\"utf-8\") as html_file:\n",
    "        soup = BeautifulSoup(html_file, \"html.parser\")\n",
    "\n",
    "    url = path_to_uri(record[\"path\"])\n",
    "\n",
    "    # 2. Find all sections\n",
    "    sections = soup.find_all(\"section\")\n",
    "    for section in sections:\n",
    "        # 3. Extract text from the section but not from the subsections\n",
    "        section_text = \"\\n\".join(\n",
    "            [child.text for child in section.children if child.name != \"section\"]\n",
    "        )\n",
    "        # 4. Construct the section url\n",
    "        section_url = url + \"#\" + section[\"id\"]\n",
    "        # 5. Create a document object with the text, source page, source section uri\n",
    "        documents.append(\n",
    "            {\n",
    "                \"text\": section_text,\n",
    "                \"section_url\": section_url,\n",
    "                \"page_url\": url,\n",
    "            }\n",
    "        )\n",
    "    return documents\n",
    "\n",
    "\n",
    "ds = ds.flat_map(extract_sections_from_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bae970",
   "metadata": {},
   "source": [
    "#### Store the text into input files that are ideal for Ray Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d6649",
   "metadata": {},
   "source": [
    "The following are good heuristics to keep in mind:\n",
    "- Avoid reading large (1 GiB or more) binary files.\n",
    "  - `ray.data.read_*` cannot parallelize reading a single file - i.e., it maps 1 file to 1 read task.\n",
    "- Avoid too many tiny files (less than 1 MiB).\n",
    "  - There is a default minimum block size that `ray.data` uses. This means that `ray.data` will need to group together the tiny blocks into larger blocks, which can be expensive.\n",
    "- Avoid transforming a Dataset where individual rows are large (100 MiB or more).\n",
    "  - There is a default maximum block size that `ray.data` uses. This means that `ray.data` will need spill the output into multiple blocks, which could lead to OOM errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f684fdb",
   "metadata": {},
   "source": [
    "We choose a `num_rows_per_file` of 400 so our produced files are not of a reasonable size given the above heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fff8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if (DATA_DIR / \"full_scale\" / \"02_sections\").exists():\n",
    "    shutil.rmtree(DATA_DIR / \"full_scale\" / \"02_sections\")\n",
    "ds.write_json(DATA_DIR / \"full_scale\" / \"02_sections\", num_rows_per_file=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27d396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -llh {DATA_DIR / \"full_scale\" / \"02_sections\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda8999-9b56-4347-b3bd-6f740ece7b54",
   "metadata": {},
   "source": [
    "Let's count how many documents we will have after processing the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d20170-970e-4c41-a830-19b28a36ed08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.data.read_json(DATA_DIR / \"full_scale\" / \"02_sections\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c55a29",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "**Further considerations for creating input files for Ray Data:**\n",
    "\n",
    "Consider converting to parquet files which allow for pruning to improve reads:\n",
    "- When working with column-oriented file formats like parquet, you can specify which columns you want to read. This might help significantly reduce the memory footprint of the read task.\n",
    "- Similarly, you can pass in a filter to `ray.data.read_parquet()` (filter pushdown) which is applied at the file scan so only rows that match the filter predicate are returned.\n",
    "\n",
    "In our case the bulk of the memory is taken up by the text column. Using parquet files will not significantly help us reduce the memory footprint of the read task.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6349758",
   "metadata": {},
   "source": [
    "### Phase 2: Generating Embeddings\n",
    "\n",
    "Now that we have our documents, we can proceed to generate embeddings.\n",
    "\n",
    "#### 1. Load documents\n",
    "We begin by reading the documents from the \"02_sections\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cabac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#ds = ray.data.read_json(DATA_DIR / \"full_scale\" / \"02_sections\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mread_json(DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#ds.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ray' is not defined"
     ]
    }
   ],
   "source": [
    "#ds = ray.data.read_json(DATA_DIR / \"full_scale\" / \"02_sections\")\n",
    "\n",
    "ds = ray.data.read_json(DATA_DIR / \"small_sample\")\n",
    "\n",
    "#ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75131c7b",
   "metadata": {},
   "source": [
    "#### Applying chunking as a transformation\n",
    "\n",
    "We apply our chunking transformation using `flat_map`, which applies a 1-to-many function to each row in the dataset and then flattens the results into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ed69ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f2f96",
   "metadata": {},
   "source": [
    "We could have used `map_batches` instead to apply a many-to-many function to each batch of rows in the dataset. However, given our chunking transformation is not vectorized, `map_batches` will not be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229450f6",
   "metadata": {},
   "source": [
    "Let's run the chunking and count our total number of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a8aaa46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d659f0b57488419fac0a6c1b2d0cc755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Read progress 0:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b485be",
   "metadata": {},
   "source": [
    "#### Applying embedding as a transformation\n",
    "\n",
    "For the embedding part, we will want to run the embedding model on the fastest possible device.\n",
    "Let's check the available devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10dcd03a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.has_mps:\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aed538-bf9e-4938-a11d-98431ecdea21",
   "metadata": {
    "tags": []
   },
   "source": [
    "We want to load the embedding model once and reuse it across multiple transformation tasks.\n",
    "\n",
    "To do so, we want to use call `map_batches` with **stateful transform** instead of a *stateless transform*. \n",
    "\n",
    "This means we create a pool of processes called actors where the model is already loaded in memory.\n",
    "\n",
    "Each actor will run a `MapBatch` task where:\n",
    "  - initial state is handled in `__init__`\n",
    "  - task is invoked using `__call__` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab910d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_gpus = 2\n",
    "num_cpus = psutil.cpu_count()\n",
    "\n",
    "class EmbedBatch:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(\"thenlper/gte-large\", device=device)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        text = batch[\"text\"].tolist()\n",
    "        embeddings = self.model.encode(text, batch_size=len(text))\n",
    "        batch[\"embeddings\"] = embeddings.tolist()\n",
    "        return batch\n",
    "\n",
    "ds = ds.map_batches(\n",
    "    EmbedBatch,\n",
    "    # Maximum number of actors to launch.\n",
    "    concurrency=num_gpus if device == \"cuda\" else num_cpus,\n",
    "    # Size of batches passed to embeddings actor.\n",
    "    batch_size=100,\n",
    "    # 1 GPU for each actor.\n",
    "    num_gpus=1 if device == \"cuda\" else 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaadfbcb",
   "metadata": {},
   "source": [
    "#### Writing the embeddings to disk\n",
    "\n",
    "When writing, we can use the `num_rows_per_file` parameter to control the number of rows per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dab59319-0e69-4fd5-8f0e-d556c5c4881e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 15:35:49,061\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=64 for operator ReadJSON to satisfy parallelism at least twice the available number of CPUs (32).\n",
      "2024-03-09 15:35:49,062\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 64, each read task output is split into 32 smaller blocks.\n",
      "2024-03-09 15:35:49,063\tINFO streaming_executor.py:110 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON] -> ActorPoolMapOperator[MapBatches(EmbedBatch)] -> TaskPoolMapOperator[Write]\n",
      "2024-03-09 15:35:49,063\tINFO streaming_executor.py:111 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-03-09 15:35:49,064\tINFO streaming_executor.py:113 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2024-03-09 15:35:49,089\tINFO actor_pool_map_operator.py:126 -- MapBatches(EmbedBatch): Waiting for 2 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a17833e0a7417f966bc25635a23146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=51617)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/arrow_block.py:148: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=51617)\u001b[0m   return transform_pyarrow.concat(tables)\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=51617)\u001b[0m Could not construct Arrow block from numpy array; encountered values of unsupported numpy type `17` in column named 'metadata', which cannot be casted to an Arrow data type. Falling back to using pandas block type, which is slower and consumes more memory. For maximum performance, consider applying the following suggestions before ingesting into Ray Data in order to use native Arrow block types:\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=51617)\u001b[0m - Expand out each key-value pair in the dict column into its own column\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=51617)\u001b[0m - Replace `None` values with an Arrow supported data type\n",
      "\u001b[36m(MapWorker(MapBatches(EmbedBatch)) pid=51617)\u001b[0m \n",
      "2024-03-09 15:35:57,906\tWARNING actor_pool_map_operator.py:294 -- To ensure full parallelization across an actor pool of size 2, the Dataset should consist of at least 2 distinct blocks. Consider increasing the parallelism when creating the Dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 337 ms, sys: 52.9 ms, total: 390 ms\n",
      "Wall time: 8.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if (DATA_DIR / \"full_scale\" / \"03_embeddings\").exists():\n",
    "    shutil.rmtree(DATA_DIR / \"full_scale\" / \"03_embeddings\")\n",
    "(\n",
    "    ds\n",
    "    .write_json(\n",
    "        num_rows_per_file=50,\n",
    "        path=DATA_DIR / \"full_scale\" / \"03_embeddings\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4cf7c",
   "metadata": {},
   "source": [
    "##### Inspecting the ray data dashboard\n",
    "\n",
    "If we take a look at the metrics tab of the ray data dashboard, we can check to see:\n",
    "\n",
    "- The GPU utilization\n",
    "    - Ideally, we would like to see the GPU utilization at 100% for the duration of the embedding process\n",
    "- The time spent on io and network by different tasks\n",
    "\n",
    "We can then use this information to optimize our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68b208",
   "metadata": {},
   "source": [
    "##### Inspecting the output\n",
    "\n",
    "We check to see if the embeddings were written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60f381af-8c34-4e08-9014-c916f8ac3687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 512K\n",
      "-rw-r--r-- 1 ray users 512K Mar  9 15:35 64_000000_000000.json\n"
     ]
    }
   ],
   "source": [
    "!ls -llh {DATA_DIR / \"full_scale\" / \"03_embeddings\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e368b",
   "metadata": {},
   "source": [
    "### Recap of the pipeline\n",
    "\n",
    "Here is our entire pipeline so far:\n",
    "\n",
    "```python\n",
    "ds = (\n",
    "    ray.data.read_json(\n",
    "        DATA_DIR / \"full_scale\" / \"02_sections\",\n",
    "    )\n",
    "    .flat_map(chunk_row)\n",
    "    .map_batches(\n",
    "        EmbedBatch,\n",
    "        concurrency=num_gpus,\n",
    "        batch_size=100,\n",
    "        num_gpus=1,\n",
    "    )\n",
    "    .write_json(\n",
    "        path=DATA_DIR / \"full_scale\" / \"03_embeddings_tuning\",\n",
    "        num_rows_per_file=50,\n",
    "    )\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2771a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Tuning the pipeline\n",
    "\n",
    "Proceed to tune your pipeline by:\n",
    "- Changing the batch size on `map_batches` and see what effect it has on the GPU utilization.\n",
    "- Changing the number of GPUs and see whether it helps to scale the pipeline.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8b0c3",
   "metadata": {},
   "source": [
    "### Upserting embeddings to Pinecone\n",
    "\n",
    "We will use [Pinecone](https://www.pinecone.io/) to index our document embeddings in a vector store. Pinecone is a fully managed vector database optimized for similarity search and is user-friendly. We chose Pinecone for its ease of use and its free tier, which meets our needs.\n",
    "\n",
    "Index your document embeddings in Pinecone as follows:\n",
    "\n",
    "\n",
    "1. Create a Pinecone client.\n",
    "2. Create a Pinecone index.\n",
    "3. Load the embeddings from disk.\n",
    "4. Transform the embeddings into Pinecone’s index format.\n",
    "5. Upsert the embeddings into the Pinecone index.\n",
    "6. Query the Pinecone index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b8083",
   "metadata": {},
   "source": [
    "#### 1. Create a Pinecone client \n",
    "1. Sign up for a free account at https://www.pinecone.io/ and obtain an API key.\n",
    "\n",
    "Follow the instructions on the Pinecone website to sign up and obtain an API key.\n",
    "\n",
    "2. Initialize a Pinecone client with your API key.\n",
    "\n",
    "Replace YOUR_API_KEY with your actual API key to initialize a Pinecone client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7878b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_PINECONE_API_KEY = \"9386359a-0227-4d5b-80d9-b1bb7600dd08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3594d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\", YOUR_PINECONE_API_KEY)\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f4443",
   "metadata": {},
   "source": [
    "Let's list out the available indices in your Pinecone account by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b815e31d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1024,\n",
       "              'host': 'canopy--shanker-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--shanker-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--cong-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--cong-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--atlas-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--atlas-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'pink-salmon-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pink-salmon',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--dylans-rag-bootcamp-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--dylans-rag-bootcamp-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'atlas-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'atlas-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'test-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'test-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'brhoades-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'brhoades',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'test-index-jr-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'test-index-jr',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'i-like-turtles-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'i-like-turtles',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--salmon-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--salmon',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--pieter-12345-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--pieter-12345',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'joshua-c-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'joshua-c',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'y-okitsu-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'y-okitsu-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--brendanwhiting-weeeeeee-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--brendanwhiting-weeeeeee',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--rushil-rox-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--rushil-rox',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'jay-destories-rag-02-embeddings-generation-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'jay-destories-rag-02-embeddings-generation',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'pwaivers-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pwaivers',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'amitpphatak-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'amitpphatak',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'anya-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'anya',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--y-okitsu-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--y-okitsu-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--yangyong-anyscale-demo-kb-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--yangyong-anyscale-demo-kb',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'hien-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'hien-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--robert-index-3423423-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--robert-index-3423423',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'dylans-rag-bootcamp-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'dylans-rag-bootcamp-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--samsara-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--samsara',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'ert-test-x-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'ert-test-x',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--amitpphatak-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--amitpphatak',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'mango-jelly-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'mango-jelly',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--joshua-c-demo-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--joshua-c-demo',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--mo-really-good-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--mo-really-good-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'gang-rag-bootcamp-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'gang-rag-bootcamp',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'sachin-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'sachin-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'satancisco-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'satancisco',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'banda-ki-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'banda-ki',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'marwan-ray-docs-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'marwan-ray-docs',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'om-rag-dev-2024-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'om-rag-dev-2024',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'another-good-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'another-good-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--mango-jelly-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--mango-jelly',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'pieter-12345-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pieter-12345',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'althaf-pinecone-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'althaf-pinecone-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--dufwagwitfip-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--dufwagwitfip',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--pwaivers-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--pwaivers',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--test-index-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--test-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'chris-rag-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'chris-rag',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'roie-test-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'roie-test',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--canopy-demo-anyscale-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--canopy-demo-anyscale',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'rushil-rox-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'rushil-rox',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--luo-r-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--luo-r',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'canopy--marwan-ray-docs-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--marwan-ray-docs',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'deep-pulusani-zhxkhfk.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'deep-pulusani',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed9863",
   "metadata": {},
   "source": [
    "#### 2. Create a Pinecone index \n",
    "\n",
    "##### Choosing an index type\n",
    "Pinecone offers two types of indexes - [see the docs for more details](https://docs.pinecone.io/docs/indexes): \n",
    "- pod-based indices\n",
    "  -  you choose:\n",
    "     -  pod type\n",
    "     -  pod size\n",
    "     -  number of pods\n",
    "  - Depending on your choice, you get:\n",
    "     -  different amounts of storage\n",
    "     -  higher or lower latency\n",
    "     -  higher or lower throughput\n",
    "  - A starter index is free and has a limit of 100,00 vectors.\n",
    "- serverless indices\n",
    "  - You don't configure or manage any compute or storage resources.\n",
    "  - Scales automatically based on usage.\n",
    "  - You pay only for the amount of data stored and operations performed, with no minimums.\n",
    "\n",
    "\n",
    "##### Choosing a distance metric\n",
    "When creating an index, you will need to specify the dimension of the embeddings and the metric you want to use for similarity search.\n",
    "\n",
    "Pinecone offers these two main metrics:\n",
    "- Euclidean:\n",
    "  - The Euclidean distance between two vectors is the square root of the sum of the squared differences between the elements of the vectors.\n",
    "  - The lower the distance, the more similar the vectors.\n",
    "- Cosine:\n",
    "  - The cosine similarity between two vectors is the cosine of the angle between them.\n",
    "  - Between -1 and 1, where 1 means the vectors are identical, -1 means they are diametrically opposed, and 0 means they are orthogonal.\n",
    "\n",
    "It is common to choose cosine similarity for high-dimensional embeddings, as it is suffers less from the curse of dimensionality than Euclidean distance. [See this article for more details.](https://www.imaurer.com/which-vector-similarity-metric-should-i-use/) \n",
    "\n",
    "Note pinecone also offers dot-product which you can think of as an un-normlized cosine similarity (i.e. not bound between -1 and 1).\n",
    "\n",
    "##### Configuring the metadata\n",
    "\n",
    "By default Pinecone will attempt to index all the metadata provided. This can be expensive and slow. We can configure the metadata to only index the fields we are interested in by passing in a `metadata_config` parameter.\n",
    "\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "To create a new index in Pinecone use the `create_index` method on the Pinecone client. In case you want to overwrite an existing index, you can use the `delete_index` method to delete the existing index before creating a new one. We implement a `create_index` method that parameterizes the main configuration options we discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "134c2d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_index(\n",
    "    index_name: str,\n",
    "    cloud: str,\n",
    "    region: str,\n",
    "    metric: str,\n",
    "    embedding_dimension: int,\n",
    "    index_type: str,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\", YOUR_PINECONE_API_KEY))\n",
    "    existing_index_names = {index.name for index in pc.list_indexes().indexes}\n",
    "\n",
    "    if index_name in existing_index_names:\n",
    "        pc.delete_index(index_name)\n",
    "\n",
    "    if index_type == \"serverless\":\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=embedding_dimension,\n",
    "            metric=metric,\n",
    "            spec=ServerlessSpec(cloud=cloud, region=region),\n",
    "        )\n",
    "    elif index_type == \"pod\":\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=embedding_dimension,\n",
    "            metric=metric,\n",
    "            spec=PodSpec(\n",
    "                environment=\"gcp-starter\",\n",
    "                metadata_config={\"indexed\": [\"source\", \"source\", \"id\"]},\n",
    "                **kwargs,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8eec11c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cloud = \"aws\"\n",
    "region = \"us-west-2\"\n",
    "metric = \"cosine\"\n",
    "index_type = \"serverless\"  # \"serverless\" or \"pod\"\n",
    "index_name = \"shanker-index\" # A unique name for the index under your organization\n",
    "embedding_dimension = 1024  # From the model page of thenlper/gte-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e3b38db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_index(\n",
    "    index_name=index_name,\n",
    "    cloud=cloud,\n",
    "    region=region,\n",
    "    metric=metric,\n",
    "    index_type=index_type,\n",
    "    embedding_dimension=embedding_dimension,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80c116",
   "metadata": {},
   "source": [
    "#### 3. Load the embeddings from disk \n",
    "\n",
    "We will load the embeddings from disk using `ray.data.read_json` to initiate a distributed upsert of the embeddings to Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14a3f696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024e186193e84933a70850246f655612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset(\n",
       "   num_blocks=64,\n",
       "   num_rows=9,\n",
       "   schema={\n",
       "      id: string,\n",
       "      source: string,\n",
       "      text: string,\n",
       "      meta...: struct<primary_category: string, published: string, title: string, updated: string>,\n",
       "      section_url: string,\n",
       "      page_url: string,\n",
       "      embeddings: list<item: double>\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ray.data.read_json(DATA_DIR / \"full_scale\" / \"03_embeddings/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883bcb2",
   "metadata": {},
   "source": [
    "#### 4. Transform the embeddings into Pinecone index format \n",
    "\n",
    "Pinecone requires vectors in a specific format. Construct a list of dictionaries where each dictionary contains the following.\n",
    "\n",
    "- `id`: a unique identifier for the vector.\n",
    "- `values`: the embedding vector for the document chunk.\n",
    "- `metadata`: a dictionary with the document chunk’s metadata, including the original text.\n",
    "\n",
    "On building an `id` value:\n",
    "- The `id` should be unique across all the vectors in the index.\n",
    "- For RAG, Pinecone offers [ID prefixing](https://docs.pinecone.io/docs/manage-rag-documents#use-id-prefixes-to-reference-parent-documents)\n",
    "  - [ID prefixing]((https://docs.pinecone.io/docs/manage-rag-documents#use-id-prefixes-to-reference-parent-documents)) allows to quickly filter on a common prefix `ID` without having to rely on `metadata` fields.\n",
    "\n",
    "On building a `metadata` value:\n",
    "- High cardinality metadata can slow down the indexing process.\n",
    "- When creating the index, you can configure which metadata fields are indexed and which are not.\n",
    "  -  This can help to reduce the size of the index and improve the indexing speed.\n",
    "  -  At the moment this is a feature specific to pod-based indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "168a85f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_pinecone_vectors(row):\n",
    "    row_hash = joblib.hash(row)\n",
    "    page_name = row[\"source\"].split(\"/\")[-1]\n",
    "    section_name = row[\"source\"].split(\"#\")[-1]\n",
    "    return {\n",
    "        \"id\": f\"{page_name}#{section_name}#{row_hash}\", # sample ID prefix\n",
    "        \"values\": row[\"embeddings\"],\n",
    "        \"metadata\": {\n",
    "            \"section_url\": row[\"source\"], # not needed if ID prefix is used\n",
    "            \"page_url\": row[\"id\"], # not needed if ID prefix is used\n",
    "            \"text\": row[\"text\"], # Perhaps this is stored on a separate storage if metadata will index it\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "ds = ds.map(convert_to_pinecone_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154edf44",
   "metadata": {},
   "source": [
    "We inspect a single row to see the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42c6730f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 15:40:58,575\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=64 for operator ReadJSON to satisfy parallelism at least twice the available number of CPUs (32).\n",
      "2024-03-09 15:40:58,576\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 64, each read task output is split into 64 smaller blocks.\n",
      "2024-03-09 15:40:58,576\tINFO streaming_executor.py:110 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON] -> TaskPoolMapOperator[Map(convert_to_pinecone_vectors)->Map(convert_to_pinecone_vectors)] -> LimitOperator[limit=1]\n",
      "2024-03-09 15:40:58,577\tINFO streaming_executor.py:111 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-03-09 15:40:58,577\tINFO streaming_executor.py:113 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f772d9a9963a44e8b90a4e41a8cc42ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 15:40:58,619\tERROR streaming_executor_state.py:453 -- An exception was raised from a task of operator \"Map(convert_to_pinecone_vectors)->Map(convert_to_pinecone_vectors)\". Dataset execution will now abort. To ignore this exception and continue, set DataContext.max_errored_blocks.\n"
     ]
    },
    {
     "ename": "RayTaskError(AttributeError)",
     "evalue": "\u001b[36mray::Map(convert_to_pinecone_vectors)->Map(convert_to_pinecone_vectors)()\u001b[39m (pid=29333, ip=10.0.44.131)\n    for b_out in map_transformer.apply_transform(iter(blocks), ctx):\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 371, in __call__\n    for data in iter:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 196, in __call__\n    yield from self._row_fn(input, ctx)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 232, in transform_fn\n    for row in rows:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 248, in __call__\n    for block in blocks:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 371, in __call__\n    for data in iter:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 196, in __call__\n    yield from self._row_fn(input, ctx)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 233, in transform_fn\n    out_row = fn(row)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 119, in fn\n    return op_fn(item, *fn_args, **fn_kwargs)\n  File \"/tmp/ipykernel_49712/2085184142.py\", line 3, in convert_to_pinecone_vectors\nAttributeError: 'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py:2330\u001b[0m, in \u001b[0;36mDataset.take_batch\u001b[0;34m(self, batch_size, batch_format)\u001b[0m\n\u001b[1;32m   2327\u001b[0m limited_ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit(batch_size)\n\u001b[1;32m   2329\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2330\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\n\u001b[1;32m   2331\u001b[0m         \u001b[39miter\u001b[39;49m(\n\u001b[1;32m   2332\u001b[0m             limited_ds\u001b[39m.\u001b[39;49miter_batches(\n\u001b[1;32m   2333\u001b[0m                 batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2334\u001b[0m                 prefetch_batches\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2335\u001b[0m                 batch_format\u001b[39m=\u001b[39;49mbatch_format,\n\u001b[1;32m   2336\u001b[0m             )\n\u001b[1;32m   2337\u001b[0m         )\n\u001b[1;32m   2338\u001b[0m     )\n\u001b[1;32m   2339\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   2340\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe dataset is empty.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/iterator.py:162\u001b[0m, in \u001b[0;36mDataIterator.iter_batches.<locals>._create_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    158\u001b[0m \u001b[39m# Iterate through the dataset from the start each time\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m# _iterator_gen is called.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m# This allows multiple iterations of the dataset without\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m# needing to explicitly call `iter_batches()` multiple times.\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m block_iterator, stats, blocks_owned_by_consumer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_block_iterator()\n\u001b[1;32m    164\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\n\u001b[1;32m    165\u001b[0m     iter_batches(\n\u001b[1;32m    166\u001b[0m         block_iterator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m )\n\u001b[1;32m    180\u001b[0m dataset_tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dataset_tag()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/iterator/iterator_impl.py:33\u001b[0m, in \u001b[0;36mDataIteratorImpl._to_block_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_block_iterator\u001b[39m(\n\u001b[1;32m     26\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     27\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[39mbool\u001b[39m,\n\u001b[1;32m     31\u001b[0m ]:\n\u001b[1;32m     32\u001b[0m     ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_dataset\n\u001b[0;32m---> 33\u001b[0m     block_iterator, stats, executor \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49m_plan\u001b[39m.\u001b[39;49mexecute_to_iterator()\n\u001b[1;32m     34\u001b[0m     ds\u001b[39m.\u001b[39m_current_executor \u001b[39m=\u001b[39m executor\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m block_iterator, stats, \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/plan.py:506\u001b[0m, in \u001b[0;36mExecutionPlan.execute_to_iterator\u001b[0;34m(self, allow_clear_input_blocks, force_read)\u001b[0m\n\u001b[1;32m    504\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(block_iter)\n\u001b[1;32m    505\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m     block_iter \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39mchain([\u001b[39mnext\u001b[39;49m(gen)], gen)\n\u001b[1;32m    507\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:45\u001b[0m, in \u001b[0;36mexecute_to_legacy_block_iterator\u001b[0;34m(executor, plan, allow_clear_input_blocks, dataset_uuid)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m\"\"\"Same as execute_to_legacy_bundle_iterator but returning blocks and metadata.\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m bundle_iter \u001b[39m=\u001b[39m execute_to_legacy_bundle_iterator(\n\u001b[1;32m     43\u001b[0m     executor, plan, allow_clear_input_blocks, dataset_uuid\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m \u001b[39mfor\u001b[39;00m bundle \u001b[39min\u001b[39;00m bundle_iter:\n\u001b[1;32m     46\u001b[0m     \u001b[39mfor\u001b[39;00m block, metadata \u001b[39min\u001b[39;00m bundle\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m     47\u001b[0m         \u001b[39myield\u001b[39;00m block, metadata\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/executor.py:37\u001b[0m, in \u001b[0;36mOutputIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RefBundle:\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_next()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py:144\u001b[0m, in \u001b[0;36mStreamingExecutor.execute.<locals>.StreamIterator.get_next\u001b[0;34m(self, output_split_idx)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_next\u001b[39m(\u001b[39mself\u001b[39m, output_split_idx: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RefBundle:\n\u001b[1;32m    143\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outer\u001b[39m.\u001b[39;49m_output_node\u001b[39m.\u001b[39;49mget_output_blocking(\n\u001b[1;32m    145\u001b[0m             output_split_idx\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outer\u001b[39m.\u001b[39m_global_info:\n\u001b[1;32m    148\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outer\u001b[39m.\u001b[39m_global_info\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m, dag\u001b[39m.\u001b[39m_estimated_output_blocks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor_state.py:277\u001b[0m, in \u001b[0;36mOpState.get_output_blocking\u001b[0;34m(self, output_split_idx)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     \u001b[39m# Check if StreamingExecutor has caught an exception or is done execution.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    278\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finished \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutqueue\u001b[39m.\u001b[39mhas_next(output_split_idx):\n\u001b[1;32m    279\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py:211\u001b[0m, in \u001b[0;36mStreamingExecutor.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39m\"\"\"Run the control loop in a helper thread.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[39mResults are returned via the output node's outqueue.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[39m# Run scheduling loop until complete.\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scheduling_loop_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_topology) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown:\n\u001b[1;32m    212\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    214\u001b[0m     \u001b[39m# Propagate it to the result iterator.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py:259\u001b[0m, in \u001b[0;36mStreamingExecutor._scheduling_loop_step\u001b[0;34m(self, topology)\u001b[0m\n\u001b[1;32m    254\u001b[0m     logger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mScheduling loop step...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    256\u001b[0m \u001b[39m# Note: calling process_completed_tasks() is expensive since it incurs\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# ray.wait() overhead, so make sure to allow multiple dispatch per call for\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# greater parallelism.\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m num_errored_blocks \u001b[39m=\u001b[39m process_completed_tasks(\n\u001b[1;32m    260\u001b[0m     topology, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backpressure_policies, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_errored_blocks\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_errored_blocks \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    263\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_errored_blocks \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m num_errored_blocks\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor_state.py:454\u001b[0m, in \u001b[0;36mprocess_completed_tasks\u001b[0;34m(topology, backpressure_policies, max_errored_blocks)\u001b[0m\n\u001b[1;32m    448\u001b[0m             error_message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    449\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m Dataset execution will now abort.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m To ignore this exception and continue, set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m DataContext.max_errored_blocks.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m             )\n\u001b[1;32m    453\u001b[0m             logger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39merror(error_message)\n\u001b[0;32m--> 454\u001b[0m             \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    455\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(task, MetadataOpTask)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor_state.py:421\u001b[0m, in \u001b[0;36mprocess_completed_tasks\u001b[0;34m(topology, backpressure_policies, max_errored_blocks)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(task, DataOpTask):\n\u001b[1;32m    420\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m         num_blocks_read \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39;49mon_data_ready(\n\u001b[1;32m    422\u001b[0m             max_blocks_to_read_per_op\u001b[39m.\u001b[39;49mget(state, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    423\u001b[0m         )\n\u001b[1;32m    424\u001b[0m         \u001b[39mif\u001b[39;00m state \u001b[39min\u001b[39;00m max_blocks_to_read_per_op:\n\u001b[1;32m    425\u001b[0m             max_blocks_to_read_per_op[state] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m num_blocks_read\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/physical_operator.py:102\u001b[0m, in \u001b[0;36mDataOpTask.on_data_ready\u001b[0;34m(self, max_blocks_to_read)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m    101\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_done_callback(ex)\n\u001b[0;32m--> 102\u001b[0m         \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_ready_callback(\n\u001b[1;32m    104\u001b[0m     RefBundle([(block_ref, meta)], owns_blocks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m num_blocks_read \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/physical_operator.py:98\u001b[0m, in \u001b[0;36mDataOpTask.on_data_ready\u001b[0;34m(self, max_blocks_to_read)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[39m# The generator should always yield 2 values (block and metadata)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[39m# each time. If we get a StopIteration here, it means an error\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# TODO(hchen): Ray Core should have a better interface for\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39m# detecting and obtaining the exception.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         ray\u001b[39m.\u001b[39;49mget(block_ref)\n\u001b[1;32m     99\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mAbove ray.get should raise an exception.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mauto_init_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py:2647\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2642\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid type of object refs, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(object_refs)\u001b[39m}\u001b[39;00m\u001b[39m, is given. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject_refs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2644\u001b[0m     )\n\u001b[1;32m   2646\u001b[0m \u001b[39m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2647\u001b[0m values, debugger_breakpoint \u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39;49mget_objects(object_refs, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   2648\u001b[0m \u001b[39mfor\u001b[39;00m i, value \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(values):\n\u001b[1;32m   2649\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py:864\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    862\u001b[0m     global_worker\u001b[39m.\u001b[39mcore_worker\u001b[39m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m    863\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m--> 864\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m    865\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[39mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m: \u001b[36mray::Map(convert_to_pinecone_vectors)->Map(convert_to_pinecone_vectors)()\u001b[39m (pid=29333, ip=10.0.44.131)\n    for b_out in map_transformer.apply_transform(iter(blocks), ctx):\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 371, in __call__\n    for data in iter:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 196, in __call__\n    yield from self._row_fn(input, ctx)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 232, in transform_fn\n    for row in rows:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 248, in __call__\n    for block in blocks:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 371, in __call__\n    for data in iter:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 196, in __call__\n    yield from self._row_fn(input, ctx)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 233, in transform_fn\n    out_row = fn(row)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 119, in fn\n    return op_fn(item, *fn_args, **fn_kwargs)\n  File \"/tmp/ipykernel_49712/2085184142.py\", line 3, in convert_to_pinecone_vectors\nAttributeError: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "sample = ds.take_batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28823d0a",
   "metadata": {},
   "source": [
    "#### 5. Upsert the embeddings into the Pinecone index. \n",
    "\n",
    "Insert the chunks into the Pinecone index by performing these two steps:\n",
    "\n",
    "1. Instantiate a Pinecone connection object using the client’s Index.\n",
    "2. Use the `Index.upsert` method to insert the data into the index.\n",
    "\n",
    "Here are some considerations:\n",
    "- Avoid creating a new connection object for every transform task. \n",
    "  - Make use of stateful transforms to launch a pool of actors with their connection loaded and ready.\n",
    "- Avoid launching too large of a pool of actors.\n",
    "  - While pinecone is able to handle a large number of connections, it is still a good practice to limit the number of connections to avoid overwhelming the pinecone server.\n",
    "- Avoid the batch size being too large.\n",
    "  - Find the batch size that fits into the allowed limit of the upsert operation.\n",
    "    - [Max size for an upsert request is 2MB](https://docs.pinecone.io/v1/docs/limits#upserts)\n",
    "    - [Recommended number of vectors per upsert request is 100](https://docs.pinecone.io/v1/docs/limits#upserts)\n",
    "\n",
    "<div class=\"alert alert-box alert-secondary\">\n",
    "\n",
    "Additionally for further control and isolation, you can create namespaces within an index. This is useful when you want to:\n",
    "- Store different types of data in the same index.\n",
    "- Store data from different sources in the same index.\n",
    "\n",
    "For our purposes, we will only use a single namespace. For more details see the [Pinecone documentation](https://docs.pinecone.io/docs/namespaces).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6b519",
   "metadata": {},
   "source": [
    "Let's first determine the ideal batch size. We do so by looking at the statistics of size in MB of the upsert requests. We start by checking if a batch size of 100 (the recommended maximum by pinecone) is too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c07f952f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 15:38:12,624\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=64 for operator ReadJSON to satisfy parallelism at least twice the available number of CPUs (32).\n",
      "2024-03-09 15:38:12,625\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 64, each read task output is split into 64 smaller blocks.\n",
      "2024-03-09 15:38:12,626\tINFO streaming_executor.py:110 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON] -> TaskPoolMapOperator[Map(convert_to_pinecone_vectors)->MapBatches(get_size_of_batch)]\n",
      "2024-03-09 15:38:12,626\tINFO streaming_executor.py:111 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-03-09 15:38:12,627\tINFO streaming_executor.py:113 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64183122bdc1490ab038acc4c70b6d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 15:38:12,784\tERROR streaming_executor_state.py:453 -- An exception was raised from a task of operator \"Map(convert_to_pinecone_vectors)->MapBatches(get_size_of_batch)\". Dataset execution will now abort. To ignore this exception and continue, set DataContext.max_errored_blocks.\n"
     ]
    },
    {
     "ename": "RayTaskError(AttributeError)",
     "evalue": "\u001b[36mray::Map(convert_to_pinecone_vectors)->MapBatches(get_size_of_batch)()\u001b[39m (pid=29333, ip=10.0.44.131)\n    for b_out in map_transformer.apply_transform(iter(blocks), ctx):\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 139, in apply_transform\n    iter = transform_fn(iter, ctx)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 288, in __call__\n    first = next(block_iter, None)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 371, in __call__\n    for data in iter:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 196, in __call__\n    yield from self._row_fn(input, ctx)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 233, in transform_fn\n    out_row = fn(row)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 119, in fn\n    return op_fn(item, *fn_args, **fn_kwargs)\n  File \"/tmp/ipykernel_49712/2085184142.py\", line 3, in convert_to_pinecone_vectors\nAttributeError: 'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     size_of_batch_in_mb \u001b[38;5;241m=\u001b[39m size_of_batch_in_bytes \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize_in_mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: [size_of_batch_in_mb]}\n\u001b[0;32m----> 9\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_size_of_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize_in_mb\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py:4396\u001b[0m, in \u001b[0;36mDataset.to_pandas\u001b[0;34m(self, limit)\u001b[0m\n\u001b[1;32m   4389\u001b[0m \u001b[39mif\u001b[39;00m limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m count \u001b[39m>\u001b[39m limit:\n\u001b[1;32m   4390\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4391\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe dataset has more than the given limit of \u001b[39m\u001b[39m{\u001b[39;00mlimit\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4392\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrows: \u001b[39m\u001b[39m{\u001b[39;00mcount\u001b[39m}\u001b[39;00m\u001b[39m. If you are sure that a DataFrame with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4393\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcount\u001b[39m}\u001b[39;00m\u001b[39m rows will fit in local memory, set ds.to_pandas(limit=None) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto disable limits.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4395\u001b[0m     )\n\u001b[0;32m-> 4396\u001b[0m blocks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_internal_block_refs()\n\u001b[1;32m   4397\u001b[0m output \u001b[39m=\u001b[39m DelegatingBlockBuilder()\n\u001b[1;32m   4398\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m blocks:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py:4644\u001b[0m, in \u001b[0;36mDataset.get_internal_block_refs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[39m@ConsumptionAPI\u001b[39m(pattern\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime complexity:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4626\u001b[0m \u001b[39m@DeveloperAPI\u001b[39m\n\u001b[1;32m   4627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_internal_block_refs\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[ObjectRef[Block]]:\n\u001b[1;32m   4628\u001b[0m     \u001b[39m\"\"\"Get a list of references to the underlying blocks of this dataset.\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \n\u001b[1;32m   4630\u001b[0m \u001b[39m    This function can be used for zero-copy access to the data. It blocks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4642\u001b[0m \u001b[39m        A list of references to this dataset's blocks.\u001b[39;00m\n\u001b[1;32m   4643\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4644\u001b[0m     blocks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plan\u001b[39m.\u001b[39;49mexecute()\u001b[39m.\u001b[39mget_blocks()\n\u001b[1;32m   4645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_synchronize_progress_bar()\n\u001b[1;32m   4646\u001b[0m     \u001b[39mreturn\u001b[39;00m blocks\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/plan.py:572\u001b[0m, in \u001b[0;36mExecutionPlan.execute\u001b[0;34m(self, allow_clear_input_blocks, force_read, preserve_order)\u001b[0m\n\u001b[1;32m    567\u001b[0m metrics_tag \u001b[39m=\u001b[39m create_dataset_tag(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_uuid)\n\u001b[1;32m    568\u001b[0m executor \u001b[39m=\u001b[39m StreamingExecutor(\n\u001b[1;32m    569\u001b[0m     copy\u001b[39m.\u001b[39mdeepcopy(context\u001b[39m.\u001b[39mexecution_options),\n\u001b[1;32m    570\u001b[0m     metrics_tag,\n\u001b[1;32m    571\u001b[0m )\n\u001b[0;32m--> 572\u001b[0m blocks \u001b[39m=\u001b[39m execute_to_legacy_block_list(\n\u001b[1;32m    573\u001b[0m     executor,\n\u001b[1;32m    574\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    575\u001b[0m     allow_clear_input_blocks\u001b[39m=\u001b[39;49mallow_clear_input_blocks,\n\u001b[1;32m    576\u001b[0m     dataset_uuid\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_uuid,\n\u001b[1;32m    577\u001b[0m     preserve_order\u001b[39m=\u001b[39;49mpreserve_order,\n\u001b[1;32m    578\u001b[0m )\n\u001b[1;32m    579\u001b[0m stats \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mget_stats()\n\u001b[1;32m    580\u001b[0m stats_summary_string \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mto_summary()\u001b[39m.\u001b[39mto_string(\n\u001b[1;32m    581\u001b[0m     include_parent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    582\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:108\u001b[0m, in \u001b[0;36mexecute_to_legacy_block_list\u001b[0;34m(executor, plan, allow_clear_input_blocks, dataset_uuid, preserve_order)\u001b[0m\n\u001b[1;32m    102\u001b[0m dag, stats \u001b[39m=\u001b[39m _get_execution_dag(\n\u001b[1;32m    103\u001b[0m     executor,\n\u001b[1;32m    104\u001b[0m     plan,\n\u001b[1;32m    105\u001b[0m     preserve_order,\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m bundles \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mexecute(dag, initial_stats\u001b[39m=\u001b[39mstats)\n\u001b[0;32m--> 108\u001b[0m block_list \u001b[39m=\u001b[39m _bundles_to_block_list(bundles)\n\u001b[1;32m    109\u001b[0m \u001b[39m# Set the stats UUID after execution finishes.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m _set_stats_uuid_recursive(executor\u001b[39m.\u001b[39mget_stats(), dataset_uuid)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:204\u001b[0m, in \u001b[0;36m_bundles_to_block_list\u001b[0;34m(bundles)\u001b[0m\n\u001b[1;32m    202\u001b[0m blocks, metadata \u001b[39m=\u001b[39m [], []\n\u001b[1;32m    203\u001b[0m owns_blocks \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[39mfor\u001b[39;00m ref_bundle \u001b[39min\u001b[39;00m bundles:\n\u001b[1;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ref_bundle\u001b[39m.\u001b[39mowns_blocks:\n\u001b[1;32m    206\u001b[0m         owns_blocks \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/executor.py:37\u001b[0m, in \u001b[0;36mOutputIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RefBundle:\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_next()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py:144\u001b[0m, in \u001b[0;36mStreamingExecutor.execute.<locals>.StreamIterator.get_next\u001b[0;34m(self, output_split_idx)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_next\u001b[39m(\u001b[39mself\u001b[39m, output_split_idx: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RefBundle:\n\u001b[1;32m    143\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outer\u001b[39m.\u001b[39;49m_output_node\u001b[39m.\u001b[39;49mget_output_blocking(\n\u001b[1;32m    145\u001b[0m             output_split_idx\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outer\u001b[39m.\u001b[39m_global_info:\n\u001b[1;32m    148\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outer\u001b[39m.\u001b[39m_global_info\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m, dag\u001b[39m.\u001b[39m_estimated_output_blocks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor_state.py:277\u001b[0m, in \u001b[0;36mOpState.get_output_blocking\u001b[0;34m(self, output_split_idx)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     \u001b[39m# Check if StreamingExecutor has caught an exception or is done execution.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    278\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finished \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutqueue\u001b[39m.\u001b[39mhas_next(output_split_idx):\n\u001b[1;32m    279\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py:211\u001b[0m, in \u001b[0;36mStreamingExecutor.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39m\"\"\"Run the control loop in a helper thread.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[39mResults are returned via the output node's outqueue.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[39m# Run scheduling loop until complete.\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scheduling_loop_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_topology) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown:\n\u001b[1;32m    212\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    214\u001b[0m     \u001b[39m# Propagate it to the result iterator.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py:259\u001b[0m, in \u001b[0;36mStreamingExecutor._scheduling_loop_step\u001b[0;34m(self, topology)\u001b[0m\n\u001b[1;32m    254\u001b[0m     logger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mScheduling loop step...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    256\u001b[0m \u001b[39m# Note: calling process_completed_tasks() is expensive since it incurs\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# ray.wait() overhead, so make sure to allow multiple dispatch per call for\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# greater parallelism.\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m num_errored_blocks \u001b[39m=\u001b[39m process_completed_tasks(\n\u001b[1;32m    260\u001b[0m     topology, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backpressure_policies, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_errored_blocks\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_errored_blocks \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    263\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_errored_blocks \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m num_errored_blocks\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor_state.py:454\u001b[0m, in \u001b[0;36mprocess_completed_tasks\u001b[0;34m(topology, backpressure_policies, max_errored_blocks)\u001b[0m\n\u001b[1;32m    448\u001b[0m             error_message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    449\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m Dataset execution will now abort.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m To ignore this exception and continue, set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m DataContext.max_errored_blocks.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m             )\n\u001b[1;32m    453\u001b[0m             logger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39merror(error_message)\n\u001b[0;32m--> 454\u001b[0m             \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    455\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(task, MetadataOpTask)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor_state.py:421\u001b[0m, in \u001b[0;36mprocess_completed_tasks\u001b[0;34m(topology, backpressure_policies, max_errored_blocks)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(task, DataOpTask):\n\u001b[1;32m    420\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m         num_blocks_read \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39;49mon_data_ready(\n\u001b[1;32m    422\u001b[0m             max_blocks_to_read_per_op\u001b[39m.\u001b[39;49mget(state, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    423\u001b[0m         )\n\u001b[1;32m    424\u001b[0m         \u001b[39mif\u001b[39;00m state \u001b[39min\u001b[39;00m max_blocks_to_read_per_op:\n\u001b[1;32m    425\u001b[0m             max_blocks_to_read_per_op[state] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m num_blocks_read\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/physical_operator.py:102\u001b[0m, in \u001b[0;36mDataOpTask.on_data_ready\u001b[0;34m(self, max_blocks_to_read)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m    101\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_done_callback(ex)\n\u001b[0;32m--> 102\u001b[0m         \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_ready_callback(\n\u001b[1;32m    104\u001b[0m     RefBundle([(block_ref, meta)], owns_blocks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m num_blocks_read \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/physical_operator.py:98\u001b[0m, in \u001b[0;36mDataOpTask.on_data_ready\u001b[0;34m(self, max_blocks_to_read)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[39m# The generator should always yield 2 values (block and metadata)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[39m# each time. If we get a StopIteration here, it means an error\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# TODO(hchen): Ray Core should have a better interface for\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39m# detecting and obtaining the exception.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         ray\u001b[39m.\u001b[39;49mget(block_ref)\n\u001b[1;32m     99\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mAbove ray.get should raise an exception.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mauto_init_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py:2647\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2642\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid type of object refs, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(object_refs)\u001b[39m}\u001b[39;00m\u001b[39m, is given. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject_refs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2644\u001b[0m     )\n\u001b[1;32m   2646\u001b[0m \u001b[39m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2647\u001b[0m values, debugger_breakpoint \u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39;49mget_objects(object_refs, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   2648\u001b[0m \u001b[39mfor\u001b[39;00m i, value \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(values):\n\u001b[1;32m   2649\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py:864\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    862\u001b[0m     global_worker\u001b[39m.\u001b[39mcore_worker\u001b[39m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m    863\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m--> 864\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m    865\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[39mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m: \u001b[36mray::Map(convert_to_pinecone_vectors)->MapBatches(get_size_of_batch)()\u001b[39m (pid=29333, ip=10.0.44.131)\n    for b_out in map_transformer.apply_transform(iter(blocks), ctx):\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 139, in apply_transform\n    iter = transform_fn(iter, ctx)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 288, in __call__\n    first = next(block_iter, None)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 371, in __call__\n    for data in iter:\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 196, in __call__\n    yield from self._row_fn(input, ctx)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 233, in transform_fn\n    out_row = fn(row)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 119, in fn\n    return op_fn(item, *fn_args, **fn_kwargs)\n  File \"/tmp/ipykernel_49712/2085184142.py\", line 3, in convert_to_pinecone_vectors\nAttributeError: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "def get_size_of_batch(batch):\n",
    "    size_of_batch_in_bytes = pd.DataFrame(batch).memory_usage(deep=True).sum().sum()\n",
    "    size_of_batch_in_mb = size_of_batch_in_bytes / 1024**2\n",
    "    return {\"size_in_mb\": [size_of_batch_in_mb]}\n",
    "\n",
    "\n",
    "out = ds.map_batches(get_size_of_batch, batch_size=batch_size).to_pandas()\n",
    "out[\"size_in_mb\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e50ad0",
   "metadata": {},
   "source": [
    "Turns out a batch_size of 100 is still well below the 2MB limit that pinecone has set so we proceed with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26daf08",
   "metadata": {},
   "source": [
    "Next, we will determine the number of concurrent connections to use for the upsert. We do so by:\n",
    "- counting how many batches we need to upload\n",
    "- reasoning about the latency per batch, how fast we want the upsert and avoiding overloading the network\n",
    "\n",
    "So we compute `approx_total_batches` as the total number of chunks divided by the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f7a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "approx_total_batches = ds.count() // batch_size\n",
    "approx_total_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936519bb",
   "metadata": {},
   "source": [
    "We then choose a concurrency of 10 actor pools given this means running 13 upserts per connection which is a reasonable wait time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38375609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concurrency = 9\n",
    "approx_num_upserts_per_connection = approx_total_batches // concurrency\n",
    "approx_num_upserts_per_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe207e",
   "metadata": {},
   "source": [
    "Finally we apply the upsert as a stateful transform using `map_batches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364fe76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinecone_namespace = \"example-namespace\"\n",
    "\n",
    "\n",
    "class UpsertVectors:\n",
    "    def __init__(self):\n",
    "        self.pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\", YOUR_PINECONE_API_KEY))\n",
    "        self.index = self.pc.Index(index_name)\n",
    "        self.namespace = pinecone_namespace\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        self.index.upsert(\n",
    "            vectors=[\n",
    "                {\n",
    "                    \"id\": id_,\n",
    "                    \"values\": values,\n",
    "                    \"metadata\": metadata,\n",
    "                }\n",
    "                for id_, values, metadata in zip(\n",
    "                    batch[\"id\"], batch[\"values\"], batch[\"metadata\"]\n",
    "                )\n",
    "            ],\n",
    "            namespace=self.namespace,\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "ds = ds.map_batches(\n",
    "    UpsertVectors,\n",
    "    concurrency=concurrency,\n",
    "    batch_size=batch_size,\n",
    "    num_cpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e882fd",
   "metadata": {},
   "source": [
    "One thing to note is that pinecone offers a simple solution to run [upserts in parallel from a single machine](https://docs.pinecone.io/docs/upsert-data#send-upserts-in-parallel) We are not making use of this feature in this guide, in favor of an even more distributed approach for large scale data.\n",
    "\n",
    "We finally proceed to \"materialize\" the upserts by calling `to_pandas` which is a consumption operation triggering the execution of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea00f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_written = ds.to_pandas().drop_duplicates(subset=[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103c7d6b",
   "metadata": {},
   "source": [
    "A small note from the cell above is given the id is produced by taking a hash of the chunk, for certain edge cases were the same text is repeated in a document the same id will be produced. Therefore we drop duplicates from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c62af3",
   "metadata": {},
   "source": [
    "### 2. Verify the index\n",
    "\n",
    "Ensure the upsert operation is complete by checking the index status and getting the number of vectors in the index.\n",
    "\n",
    "Pinecone is eventually consistent, so it may take a few seconds for the index to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea2c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def verify_index(index_name: str, num_expected_vectors: int):\n",
    "    index = pc.Index(index_name)\n",
    "    stats = index.describe_index_stats()\n",
    "\n",
    "    while stats.total_vector_count != num_expected_vectors:\n",
    "        time.sleep(5)\n",
    "        stats = index.describe_index_stats()\n",
    "\n",
    "\n",
    "verify_index(index_name=index_name, num_expected_vectors=df_written.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb5d69",
   "metadata": {},
   "source": [
    "### Querying the Pinecone index\n",
    "\n",
    "Given we have indexed our embeddings, we can now query the index to retrieve the most similar documents to a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a48944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the default number of replicas for a Ray Serve deployment?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742abec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('thenlper/gte-large', device=get_device())\n",
    "query_embedding = model.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828281f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)\n",
    "result = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=5,\n",
    "    namespace=pinecone_namespace,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9b6be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result[\"matches\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ce0c7",
   "metadata": {},
   "source": [
    "If want to include the metadata in the result, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271befe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = index.query(\n",
    "    vector=query_embedding, top_k=5, include_metadata=True, namespace=pinecone_namespace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6ba3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result[\"matches\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59bf49",
   "metadata": {},
   "source": [
    "We can additionally introduce a filter on score to only return results with a score above a certain threshold based on the fetched results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08e14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = [match[\"score\"] for match in result[\"matches\"]]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28653062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_threshold = 0.93  # determined based on data distribution\n",
    "matches_above_threshold = [\n",
    "    match for match in result[\"matches\"] if match[\"score\"] > score_threshold\n",
    "]\n",
    "len(matches_above_threshold), len(result[\"matches\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb620af",
   "metadata": {},
   "source": [
    "###  Understanding Pinecone query pricing\n",
    "\n",
    "Pinecone measures usage of a query in terms of read units. Read units are priced based on the chosen cloud (see [pinecone pricing page](https://www.pinecone.io/pricing/) for more details.)\n",
    "\n",
    "More specifically, the number of read units used by a query depends on:\n",
    "- Record count: \n",
    "  - the number of records in your namespace\n",
    "- Record size: \n",
    "  - the dimension of the vector you use\n",
    "  - whether you are retrieving the vector or its metadata or just fetching back the ID\n",
    "\n",
    "Read units have good scaling properties as we will see below.\n",
    "\n",
    "See the [documentation page](https://docs.pinecone.io/docs/understanding-cost#query ) for more details\n",
    "\n",
    "Let's run a sample query to inspect the recorded usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d3245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=5,\n",
    "    namespace=pinecone_namespace,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa5c45",
   "metadata": {},
   "source": [
    "We can inspect `usage` to view how much this query cost us in terms of units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f53d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response[\"usage\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370fcfc",
   "metadata": {},
   "source": [
    "We can see we consumed 5 Read Units (RUs) running the query with top_k = 5. What is cool, is this is the same cost for running with a top_k of 100. Think of this as an index scan is of complexity O(1) and the cost of the query is the cost of fetching the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491c8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=100,\n",
    "    namespace=pinecone_namespace,\n",
    ")\n",
    "response[\"usage\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11340945",
   "metadata": {},
   "source": [
    "Running with `include_metadata=True` however will increase the cost of the query. \n",
    "\n",
    "For top_k = 1-10, this incurs an additional 1 RU for fetching the metadata. For top_k = 11-20, this incurs an additional 2 RU for fetching the metadata and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f57fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=1,\n",
    "    namespace=pinecone_namespace,\n",
    "    include_metadata=True,\n",
    ")\n",
    "response[\"usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c02b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=10,\n",
    "    namespace=pinecone_namespace,\n",
    "    include_metadata=True,\n",
    ")\n",
    "response[\"usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f2423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=11,\n",
    "    namespace=pinecone_namespace,\n",
    "    include_metadata=True,\n",
    ")\n",
    "response[\"usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0419f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=20,\n",
    "    namespace=pinecone_namespace,\n",
    "    include_metadata=True,\n",
    ")\n",
    "response[\"usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1cfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
