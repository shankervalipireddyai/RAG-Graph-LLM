{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39cccf9",
   "metadata": {},
   "source": [
    "# Brief \n",
    "\n",
    "This guide will teach you how to build and evaluate a RAG application.\n",
    "\n",
    "## Goal of this guide\n",
    "\n",
    "- Build a basic RAG application \n",
    "- Evaluate the RAG application\n",
    "- Assess Trade-offs\n",
    "- Progress to more granular evaluation techniques\n",
    "\n",
    "## The road ahead\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Here is our roadmap for this guide:\n",
    "1. Setup\n",
    "2. Build a Basic RAG application\n",
    "3. Evaluation Driven Development\n",
    "4. Build an Evaluation Dataset\n",
    "5. Evaluating Retrieval Quality\n",
    "6. Evaluating Overall Quality\n",
    "7. Next Steps\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef871a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from typing import Any, Iterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import ray\n",
    "from openai.resources.chat.completions import ChatCompletion\n",
    "from pathlib import Path\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1e90f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AVAILABLE_CPUS = psutil.cpu_count(logical=False) - 1\n",
    "DATA_DIR = Path(\"/mnt/cluster_storage/\")\n",
    "shutil.copytree(Path(\"../data/\"), DATA_DIR, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model we used to build the search index on Pinecone\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-large\"\n",
    "# The Pinecone search index we built\n",
    "PINECONE_INDEX_NAME = None # your unique index name e.g. \"marwan-ray-docs\"\n",
    "PINECONE_NAMESPACE = \"example-namespace\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778b47a",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "We will set them directly in the notebook for our educational purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21008460",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_ANYSCALE_API_KEY = \"esecret_f6dz2g16nnrai635si83z8upk8\"\n",
    "YOUR_PINECONE_API_KEY = \"9386359a-0227-4d5b-80d9-b1bb7600dd08\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c257a14",
   "metadata": {},
   "source": [
    "## Build a Basic RAG Application\n",
    "\n",
    "We will build a simple RAG application that can answer questions about [Ray](https://docs.ray.io/). \n",
    "\n",
    "As a recap, see the diagram below for a visual representation of the components required for RAG.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-bootcamp-mar-2024/with_rag.svg\" alt=\"With RAG\" width=\"80%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832d715",
   "metadata": {},
   "source": [
    "### Retrieval\n",
    "\n",
    "Retrieval is implemented in the following steps:\n",
    "1. Encode the user query\n",
    "2. Search the vector store\n",
    "3. Compose a context from the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb291d1a",
   "metadata": {},
   "source": [
    "#### 1. Encode the user query\n",
    "To encode the query, we will use the same embedding model that we used to encode the documents. This time, however, we make use of the anyscale embeddings service to encode the query. \n",
    "\n",
    "\n",
    "See the code below for a sample `QueryEncoder` implementation that delegates to the anyscale embeddings service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEncoder:\n",
    "    def __init__(self):\n",
    "        self.embedding_model_name = EMBEDDING_MODEL_NAME\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=\"https://api.endpoints.anyscale.com/v1\",\n",
    "            api_key=YOUR_ANYSCALE_API_KEY,\n",
    "        )\n",
    "\n",
    "    def encode(self, query: str) -> list[float]:\n",
    "        response = self.client.embeddings.create(\n",
    "            input=query, model=self.embedding_model_name\n",
    "        )\n",
    "        return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c279b6",
   "metadata": {},
   "source": [
    "We try out our `QueryEncoder` by encoding a sample query relevant to our domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6833f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder = QueryEncoder()\n",
    "query = \"How can I deploy Ray Serve to Kubernetes?\"\n",
    "embeddings_vector = query_encoder.encode(query)\n",
    "\n",
    "type(embeddings_vector), len(embeddings_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd10811",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "**Considerations for using a third-party service for encoding the query:**\n",
    "\n",
    "- Pros:\n",
    "  - No need to maintain infrastructure for the embedding model\n",
    "  - Cheaper than constantly running an embedding model on a GPU instance\n",
    "- Cons:\n",
    "  - Potential increase in latency due to network calls.\n",
    "  - Risk of hitting concurrency limits in case of high traffic spikes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d3dc0",
   "metadata": {},
   "source": [
    "#### 2. Search the vector store\n",
    "\n",
    "Next, we will search the vector store to retrieve the closest documents to the query. \n",
    "\n",
    "We implement a `VectorStore` abstraction that reiles on the pinecone client to search the vector store. Note that given we stored the document text as part of the document metadata, we can retrieve the document text directly from the search index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601de136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.pc = Pinecone(api_key=YOUR_PINECONE_API_KEY)\n",
    "        self.index = self.pc.Index(PINECONE_INDEX_NAME)\n",
    "        self.pinecone_namespace = PINECONE_NAMESPACE\n",
    "\n",
    "    def query(self, query_embedding: list[float], top_k: int) -> dict:\n",
    "        \"\"\"Retrieve the most similar chunks to the given query embedding.\"\"\"\n",
    "        if top_k == 0:\n",
    "            return {\"documents\": [], \"usage\": {}}\n",
    "\n",
    "        response = self.index.query(\n",
    "            namespace=self.pinecone_namespace,\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_values=False,\n",
    "            include_metadata=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"text\": match[\"metadata\"][\"text\"],\n",
    "                    \"section_url\": match[\"metadata\"][\"section_url\"],\n",
    "                }\n",
    "                for match in response[\"matches\"]\n",
    "            ],\n",
    "            \"usage\": response[\"usage\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cce05e",
   "metadata": {},
   "source": [
    "Let's test the `VectorStore` with the `embeddings_vector` we got from the `query_encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "vector_store_response = vector_store.query(\n",
    "    query_embedding=embeddings_vector,\n",
    "    top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4542f4f",
   "metadata": {},
   "source": [
    "We can inspect the retrieved document URLs given our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69de6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in vector_store_response[\"documents\"]:\n",
    "    print(doc[\"section_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bd026",
   "metadata": {},
   "source": [
    "We can then also inspect usage information to better understand the cost of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9840d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_response[\"usage\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1ce4e",
   "metadata": {},
   "source": [
    "#### 3. Compose a context from the retrieved documents\n",
    "\n",
    "We put together a `Retriever` that encapsulates the entire retrieval process so far.\n",
    "\n",
    "It also composes the context from the retrieved documents by simply concatenating the retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        self.encoder = QueryEncoder()\n",
    "        self.vector_store = VectorStore()\n",
    "\n",
    "    def _compose_context(self, contexts: list[str]) -> str:\n",
    "        sep = 100 * \"-\"\n",
    "        return \"\\n\\n\".join([f\"{sep}\\n{context}\" for context in contexts])\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int) -> dict:\n",
    "        \"\"\"Retrieve the context and sources for the given query.\"\"\"\n",
    "        encoded_query = self.encoder.encode(query)\n",
    "        vector_store_response = self.vector_store.query(\n",
    "            query_embedding=encoded_query,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        contexts = [chunk[\"text\"] for chunk in vector_store_response[\"documents\"]]\n",
    "        sources = [chunk[\"section_url\"] for chunk in vector_store_response[\"documents\"]]\n",
    "        return {\n",
    "            \"contexts\": contexts,\n",
    "            \"composed_context\": self._compose_context(contexts),\n",
    "            \"sources\": sources,\n",
    "            \"usage\": vector_store_response[\"usage\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c94e4",
   "metadata": {},
   "source": [
    "We run the retriever to check it is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98155b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever()\n",
    "retrieval_response = retriever.retrieve(\n",
    "    query=query,\n",
    "    top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeadb60",
   "metadata": {},
   "source": [
    "We inspect the retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieval_response[\"composed_context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42775b6f",
   "metadata": {},
   "source": [
    "### Generation\n",
    "\n",
    "We will generate a response using Anyscale endpoints which is available via the `openai` client library.\n",
    "\n",
    "To do so we implement a simple `LLM` class that encapsulates the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, model: str):\n",
    "        # Initialize a client to perform API requests\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=\"https://api.endpoints.anyscale.com/v1\",\n",
    "            api_key=YOUR_ANYSCALE_API_KEY,\n",
    "        )\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, user_prompt: str, temperature: float = 0, **kwargs: Any) -> ChatCompletion:\n",
    "        \"\"\"Generate a completion from the given user prompt.\"\"\"\n",
    "        # Call the chat completions endpoint\n",
    "        chat_completion = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                # Prime the system with a system message - a common best practice\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                # Send the user message with the proper \"user\" role and \"content\"\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f401a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "response = llm.generate(\"Hello there!\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76edf18",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Given a user query we will want our RAG based QA engine to perform the following steps:\n",
    "1. Retrieve the closest documents to the query\n",
    "2. Augment the query with the context\n",
    "3. Generate a response to the augmented query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10197527",
   "metadata": {},
   "source": [
    "We decide on a simple prompt template to augment the user's query with the retrieved context. The template is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_rag = \"\"\"\n",
    "Given the following context:\n",
    "{composed_context}\n",
    "\n",
    "Answer the following question:\n",
    "{query}\n",
    "\n",
    "If you cannot provide an answer based on the context, please say \"I don't know.\"\n",
    "Do not use the term \"context\" in your response.\"\"\"\n",
    "\n",
    "\n",
    "def augment_prompt(query: str, composed_context: str) -> str:\n",
    "    \"\"\"Augment the prompt with the given query and contexts.\"\"\"\n",
    "    return prompt_template_rag.format(composed_context=composed_context, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt = augment_prompt(\n",
    "    \"How can I deploy Ray Serve to Kubernetes?\",\n",
    "    retrieval_response[\"composed_context\"],\n",
    ")\n",
    "print(augmented_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380ede7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "**Considerations for building a prompt-template for RAG:**\n",
    "\n",
    "Prompt engineering techniques can be used need to be purpose built for the usecase and chosen model. For example, if you want the model to still use its own knowledge in certain cases, you might want to use a different prompt template than if you want the model to only use the retrieved context.\n",
    "\n",
    "For comparison, here are the links to popular third-party library prompt templates which are fairly generic in nature:\n",
    "- [LangChain's default RAG prompt template](https://smith.langchain.com/hub/rlm/rag-prompt)\n",
    "- [LlamaIndex's RAG prompt template](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/default_prompts.py#L99)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ad902",
   "metadata": {},
   "source": [
    "We implement our question answering `QA` class below that composed all the steps together. Additionally, it does the following:\n",
    "- Stream the response to avoid waiting on all the tokens to be generated by the LLM service.\n",
    "- Include the source links for attribution of what documents were used to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA:\n",
    "    def __init__(self, model: str):\n",
    "        self.retriever = Retriever()\n",
    "        self.llm = LLM(model=model)\n",
    "\n",
    "    def answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int,\n",
    "        include_sources: bool = True,\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"Answer the given question and provide sources.\"\"\"\n",
    "        retrieval_response = self.retriever.retrieve(\n",
    "            query=query,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        prompt = augment_prompt(query, retrieval_response[\"composed_context\"])\n",
    "        response = self.llm.generate(\n",
    "            user_prompt=prompt,\n",
    "            stream=True,\n",
    "        )\n",
    "        for chunk in response:\n",
    "            choice = chunk.choices[0]\n",
    "            if choice.delta.content is None:\n",
    "                continue\n",
    "            yield choice.delta.content\n",
    "\n",
    "        if include_sources:\n",
    "            yield \"\\n\" * 2\n",
    "            sources_str = \"\\n\".join(set(retrieval_response[\"sources\"]))\n",
    "            yield sources_str\n",
    "            yield \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c965b",
   "metadata": {},
   "source": [
    "We now test out our `QA` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_agent = QA(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "response = qa_agent.answer(query=query, top_k=3)\n",
    "for r in response:\n",
    "    print(r, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b4a73",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Activity: Prompt the QA agent with different top_k values\n",
    "\n",
    "Prompt the same QA agent with the question \"How to deploy Ray Serve on Kubernetes?\" with `top_k=0` - is the answer still helpful and correct? \n",
    "\n",
    "<details>\n",
    "<summary>Click here to see the solution</summary>\n",
    "\n",
    "\n",
    "If you prompt the QA agent with `top_k=0`, the answer will not be meaningful. This is because the RAG application will not be able to retrieve any documents from the search index and therefore will not be able to generate an answer.\n",
    "\n",
    "```python\n",
    "qa_agent = QA(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "response = qa_agent.answer(query=query, top_k=0)\n",
    "for r in response:\n",
    "    print(r, end=\"\")\n",
    "```\n",
    "\n",
    "This will now produce a hallucinated answer about using a helm chart that does not exist.\n",
    "\n",
    "\n",
    "</details>\n",
    "</summary>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bbb3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c30be",
   "metadata": {},
   "source": [
    "## Evaluation Driven Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352fb13e",
   "metadata": {},
   "source": [
    "### What does evaluation-driven development look like?\n",
    "\n",
    "See the diagram below for a visual representation of the evaluation-driven development process.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-bootcamp-mar-2024/evaluation_driven_development.svg\" alt=\"Evaluation Driven Development\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505256b1",
   "metadata": {},
   "source": [
    "### What does RAG evaluation look like?\n",
    "\n",
    "Evaluating a RAG application involves two main steps\n",
    "\n",
    "- Producing metrics that assess your RAG application to estimate and generalize the overall performance, these include metrics that score:\n",
    "  - Quality of responses\n",
    "  - Cost of responses\n",
    "  - Latency of responses\n",
    "- Assessing performance by running a test suite that includes:\n",
    "  - Canonical queries\n",
    "  - Edge cases\n",
    "  - Production usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cf0fc",
   "metadata": {},
   "source": [
    "\n",
    "### RAG Quality Metrics\n",
    "\n",
    "There are many metrics that can be used to assess the quality of a RAG application. This guide will focus on the following basic metrics:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/rag-bootcamp-mar-2024/basic_rag_evaluation.svg\" alt=\"Evaluation Driven Development\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23921bc",
   "metadata": {},
   "source": [
    "### External References\n",
    "The following are third-party references that provide a similar view of the Evaluation Driven Development process:\n",
    "\n",
    "- [DeepEval: High-level Evaluation Workflow Diagram](https://docs.confident-ai.com/docs/evaluation-introduction)\n",
    "- [RAGAS: Introduction to Metrics Driven Development](https://docs.ragas.io/en/latest/concepts/metrics_driven.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e09731",
   "metadata": {},
   "source": [
    "## Build an Evaluation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8bf353",
   "metadata": {},
   "source": [
    "### What is an evaluation dataset?\n",
    "\n",
    "An evaluation dataset\n",
    "\n",
    "- contains questions (queries) and answers (ground truths) that are representative of what the intended user will ask. \n",
    "- covers a varying degree of complexity about the particular domain and scope that we intend to cover.\n",
    "- is more specific than generic datasets available for question answering on platforms like [Hugging Face](https://huggingface.co/datasets). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9413e",
   "metadata": {},
   "source": [
    "### How to build an evaluation dataset?\n",
    "\n",
    "Ideally, we want to build an evaluation dataset that is representative of the intended user's questions. If you have the resources and time to parse through historical logs of user queries, this is the best way to build an evaluation dataset. \n",
    "\n",
    "#### Overcoming the cold start problem \n",
    "However, if you do not have access to historical data, you can build an evaluation dataset in two main stages:\n",
    "\n",
    "1. Use LLMs to build a dataset\n",
    "2. Check the dataset for quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514c80b",
   "metadata": {},
   "source": [
    "#### 1. Use LLMs to build a dataset\n",
    "\n",
    "The evaluation dataset we want to build is a list of rows each comprising of:\n",
    "\n",
    "- A reference to the context that was used for generating the question and answer\n",
    "- A question generated by the LLM given a context\n",
    "- An answer generated by the LLM given the question and the context\n",
    "\n",
    "\n",
    "Here is a simple attempt at building an evaluation dataset using LLMs:\n",
    "\n",
    "We start by suggesting a prompt template for our LLM to use to generate questions and answers. The prompt template is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5678bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_evaluate = \"\"\"\n",
    "You are generating questions and answers provided a given document extracted from the ray documentation.\n",
    "The questions you ask should strictly ahdere to the following guidelines:\n",
    "- be strictly relevant to the document\n",
    "- mimic what a developer might ask when they encounter an issue with ray\n",
    "- make sense without needing to read the document\n",
    "\n",
    "Return your response in JSON format with the following fields:\n",
    "- question: the question you generated\n",
    "- answer: the answer to the question\n",
    "\n",
    "See the document below and generate {num_questions_per_document} questions:\n",
    "{document}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2777d",
   "metadata": {},
   "source": [
    "We then proceed to generate the evaluation dataset using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationDatasetBuilder:\n",
    "    def __init__(\n",
    "        self, model: str, num_questions_per_document: int, prompt_template: str\n",
    "    ) -> None:\n",
    "        self.llm = LLM(model=model)\n",
    "        self.prompt_template = prompt_template\n",
    "        self.num_questions_per_document = num_questions_per_document\n",
    "\n",
    "    def build(self, document: str) -> str:\n",
    "        prompt = prompt_template_evaluate.format(\n",
    "            num_questions_per_document=self.num_questions_per_document,\n",
    "            document=document[\"text\"],\n",
    "        )\n",
    "        response = self.llm.generate(user_prompt=prompt)\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f7dfb",
   "metadata": {},
   "source": [
    "Let's load some documents and generate questions/anwers for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = DATA_DIR / \"full_scale\"\n",
    "df_docs = ray.data.read_json(data_dir / \"02_sections\").to_pandas()\n",
    "df_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb10475",
   "metadata": {},
   "source": [
    "We choose a sample document about ray-logging to generate questions and answers from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = df_docs[\n",
    "    df_docs[\"section_url\"]\n",
    "    == \"https://docs.ray.io/en/master/serve/monitoring.html#ray-logging\"\n",
    "].iloc[0]\n",
    "print(sample_doc[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0482523d",
   "metadata": {},
   "source": [
    "We construct our evaluation dataset builder and attempt to generate questions from the sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_builder = EvaluationDatasetBuilder(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    num_questions_per_document=3,\n",
    "    prompt_template=prompt_template_evaluate,\n",
    ")\n",
    "output = dataset_builder.build(sample_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e87f8",
   "metadata": {},
   "source": [
    "The output looks promising it contains very relevant questions to the document indicating that our chosen model is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c64be",
   "metadata": {},
   "source": [
    "Unfortunately, this is not exactly the JSON we wanted. We would need to manually parse and fix the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ce0bb",
   "metadata": {},
   "source": [
    "#### JSON mode on Anyscale endpoints\n",
    "\n",
    "On anyscale endpoints, you can use the JSON mode to ensure the output is in a structured format of \"question\" and \"answer\". \n",
    "\n",
    "See the code below where we first define the desired response format `EvaluationExamples` via pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationExample(BaseModel):\n",
    "    question: str = Field(\n",
    "        \"What is the capital of France?\",\n",
    "        description=\"The question to ask the model.\",\n",
    "    )\n",
    "    answer: str = Field(\n",
    "        \"Paris\",\n",
    "        description=\"The expected answer to the question.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class EvaluationExamples(BaseModel):\n",
    "    examples: list[EvaluationExample] = Field(\n",
    "        ...,\n",
    "        description=\"A list of evaluation examples.\",\n",
    "    )\n",
    "\n",
    "\n",
    "EvaluationExamples.model_json_schema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e608d",
   "metadata": {},
   "source": [
    "We now define an `EvaluationDatasetBuilderJSONMode` to:\n",
    "- pass in a `response_format` parameter to the `generate` method.\n",
    "- load the returned response as a JSON object\n",
    "- update the list of examples with the `section_url` as the \"source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationDatasetBuilderJSONMode:\n",
    "    def __init__(\n",
    "        self, model: str, num_questions_per_document: int, prompt_template: str\n",
    "    ) -> None:\n",
    "        self.llm = LLM(model=model)\n",
    "        self.prompt_template = prompt_template\n",
    "        self.num_questions_per_document = num_questions_per_document\n",
    "\n",
    "    def build(self, document: str, response_format: dict) -> list:\n",
    "        prompt = self.prompt_template.format(\n",
    "            num_questions_per_document=self.num_questions_per_document,\n",
    "            document=document[\"text\"],\n",
    "        )\n",
    "        response = self.llm.generate(\n",
    "            user_prompt=prompt, response_format=response_format\n",
    "        )\n",
    "        out = json.loads(response.choices[0].message.content)\n",
    "        return [\n",
    "            {\n",
    "                \"question\": example[\"question\"],\n",
    "                \"answer\": example[\"answer\"],\n",
    "                \"source\": document[\"section_url\"],\n",
    "            }\n",
    "            for example in out[\"examples\"]\n",
    "        ]\n",
    "\n",
    "    __call__ = build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747231b",
   "metadata": {},
   "source": [
    "We try it out and see it works as expected now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_builder = EvaluationDatasetBuilderJSONMode(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    num_questions_per_document=3,\n",
    "    prompt_template=prompt_template_evaluate,\n",
    ")\n",
    "dataset_builder.build(\n",
    "    sample_doc,\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": EvaluationExamples.model_json_schema(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d76a9",
   "metadata": {},
   "source": [
    "To scale it up to generate questions and answers for all the documents, we can use Ray Data like so:\n",
    "\n",
    "Note for our purposes we have instituted a limit to avoid running for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1\n",
    "\n",
    "ds = (\n",
    "    ray.data.read_json(DATA_DIR / \"full_scale\" / \"02_sections\")\n",
    "    .limit(limit)\n",
    "    .materialize()\n",
    ")\n",
    "\n",
    "df = ds.flat_map(\n",
    "    EvaluationDatasetBuilderJSONMode,\n",
    "    concurrency=limit,\n",
    "    fn_constructor_kwargs={\n",
    "        \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        \"num_questions_per_document\": 3,\n",
    "        \"prompt_template\": prompt_template_evaluate,\n",
    "    },\n",
    "    fn_kwargs={\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": EvaluationExamples.model_json_schema(),\n",
    "        }\n",
    "    },\n",
    ").to_pandas()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101b187",
   "metadata": {},
   "source": [
    "We have went ahead and built a sample evaluation dataset which we will use for the rest of the guide.\n",
    "\n",
    "We will load it into a pandas dataframe and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_dataset = ray.data.read_json(DATA_DIR / \"evaluation\" / \"reference\").to_pandas()\n",
    "df_eval_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs.iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e985f45",
   "metadata": {},
   "source": [
    "### Taking it one step further\n",
    "\n",
    "When building an evaluation dataset, you can:\n",
    "- prompt LLMs to critique the generated questions and answers. \n",
    "  - This can be useful to ensure that the generated questions and answers are of high quality.\n",
    "- mimic complex queries by:\n",
    "  - generating the question and answers from multiple contexts\n",
    "  - mutating the question to be more complex by introducing:\n",
    "    - conditionals\n",
    "    - multi-hop reasoning\n",
    "\n",
    "This way we can produce an evaluation dataset with:\n",
    "- simple questions:\n",
    "- complex questions:\n",
    "  - multi-context questions\n",
    "  - multi-hop reasoning questions\n",
    "  - conditional questions\n",
    "\n",
    "This will provide more granular insights into where the RAG application is performing well and where it is not.\n",
    "\n",
    "\n",
    "### External References\n",
    "See the following third party references for more information on building an evaluation dataset:\n",
    "- [RAGAS evaluation dataset generation](https://docs.ragas.io/en/latest/concepts/testset_generation.html)\n",
    "- [Llamaindex evaluation dataset generation](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/e2e_evaluation.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3fc3dc",
   "metadata": {},
   "source": [
    "### Activity: Use JSON mode to generate evaluation examples\n",
    "\n",
    "Provide a prompt and response format to score the questions generated by the model.\n",
    "\n",
    "\n",
    "Here is some sample code to get you started:\n",
    "\n",
    "```python\n",
    "llm = LLM(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Your job is to score the quality of a generated question from a given context.\n",
    "\n",
    "Given this context:\n",
    "{context}\n",
    "\n",
    "Score the following question:\n",
    "{question}\n",
    "\n",
    "You must return JSON response with the following fields:\n",
    "- score: the score you give to the answer\n",
    "- reasoning: your reasoning for the score\n",
    "\"\"\"\n",
    "\n",
    "context = df_docs[df_docs[\"section_url\"].str.endswith(\"#ray-logging\")].iloc[0][\"text\"]\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    context=context,\n",
    "    question=\"How do I configure Ray logging?\",\n",
    ")\n",
    "\n",
    "# TODO - call llm.generate with the prompt using the JSON mode\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary> Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "class MyResponse(BaseModel):\n",
    "    score: int = Field(\n",
    "        ...,\n",
    "        description=\"The score you give to the answer.\",\n",
    "        ge=1,\n",
    "        le=10,\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"Your reasoning for the score.\",\n",
    "    )\n",
    "\n",
    "\n",
    "response = llm.generate(\n",
    "    user_prompt=prompt,\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": MyResponse.model_json_schema(),\n",
    "    },\n",
    ")\n",
    "\n",
    "json.loads(response.choices[0].message.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22166256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f18ec3",
   "metadata": {},
   "source": [
    "## Evaluate Retrieval Quality\n",
    "\n",
    "### What does it mean to evaluate Retrieval quality?\n",
    "- Assess usefulness of information retrieved in answering the query\n",
    "- Provide a score that correlates with maximizing usefulness of information retrieved\n",
    "\n",
    "### Why evaluate Retrieval on its own?\n",
    "- Enable modular design and development. \n",
    "    - i.e. enable Retrieval as a standalone service.\n",
    "- Easier to reason about than end-to-end metrics especially as system grows in complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49832126",
   "metadata": {},
   "source": [
    "### How to Evaluate Retrieval?\n",
    "\n",
    "We will perform the retrieval evaluation in two main steps:\n",
    "\n",
    "1. Run the retrieval process on the evaluation dataset\n",
    "2. Produce a simple retrieval score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1a8804",
   "metadata": {},
   "source": [
    "#### 1. Run the retrieval process on the evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94062fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieverRunner:\n",
    "    def __init__(self) -> None:\n",
    "        self.retriever = Retriever()\n",
    "\n",
    "    def run(self, record: dict, max_num_chunks: int) -> dict:\n",
    "        retrieval_response = self.retriever.retrieve(\n",
    "            query=record[\"question\"],\n",
    "            top_k=max_num_chunks,\n",
    "        )\n",
    "        retrieved_sources = retrieval_response[\"sources\"]\n",
    "        retrieved_contexts = retrieval_response[\"contexts\"]\n",
    "\n",
    "        for n in range(max_num_chunks + 1):\n",
    "            record[f\"retrieved_sources_at_{n}\"] = retrieved_sources[:n]\n",
    "            record[f\"retrieved_contexts_at_{n}\"] = retrieved_contexts[:n]\n",
    "\n",
    "            # see https://docs.pinecone.io/docs/understanding-cost#query \n",
    "            # depends on number of records, dimension, and if retrieving metadata\n",
    "            if n == 0:\n",
    "                record[f\"retrieval_usage_at_{n}\"] = 0\n",
    "            else:\n",
    "                record[f\"retrieval_usage_at_{n}\"] = 6 + (n - 1) // 10\n",
    "                \n",
    "        return record\n",
    "\n",
    "    __call__ = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28854a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_chunks = 20\n",
    "\n",
    "ds = ray.data.read_json(\n",
    "    DATA_DIR / \"evaluation\" / \"reference\"\n",
    ").materialize()\n",
    "\n",
    "df_eval_dataset_with_retrieval = ds.map(\n",
    "    RetrieverRunner,\n",
    "    fn_kwargs={\"max_num_chunks\": max_num_chunks},\n",
    "    concurrency=NUM_AVAILABLE_CPUS,\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_dataset_with_retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db575c4",
   "metadata": {},
   "source": [
    "#### 2. Produce a simple retrieval score\n",
    "\n",
    "We define a function to determine our retrieval score, which returns\n",
    "\n",
    "- 1 if the retrieved context overlaps with context used to generate question.\n",
    "- 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SourceURL = str\n",
    "\n",
    "def get_retrieval_score(\n",
    "    references: list[SourceURL], retrieved: list[list[SourceURL]], exact_match: bool\n",
    ") -> float:\n",
    "    matches = np.zeros(len(references))\n",
    "    for idx, (retrieved_sections, reference_section) in enumerate(\n",
    "        zip(retrieved, references)\n",
    "    ):\n",
    "        reference_page = reference_section.split(\"#\")[0]\n",
    "        for retrieved_section in retrieved_sections:\n",
    "            retrieved_page = retrieved_section.split(\"#\")[0]\n",
    "\n",
    "            match = (\n",
    "                retrieved_section == reference_section\n",
    "                if exact_match\n",
    "                else retrieved_page == reference_page\n",
    "            )\n",
    "\n",
    "            if match:\n",
    "                matches[idx] = 1\n",
    "                continue\n",
    "\n",
    "    retrieval_score = np.mean(matches)\n",
    "    return retrieval_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd105ad",
   "metadata": {},
   "source": [
    "Sanity check that at 0, the retrieval score is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_score(\n",
    "    references=df_eval_dataset_with_retrieval[\"source\"].tolist(),\n",
    "    retrieved=df_eval_dataset_with_retrieval[\"retrieved_sources_at_0\"].tolist(),\n",
    "    exact_match=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4b5c4",
   "metadata": {},
   "source": [
    "At a cutoff of `top_k=3`, the retrieval score is close to 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8be0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_score(\n",
    "    references=df_eval_dataset_with_retrieval[\"source\"].tolist(),\n",
    "    retrieved=df_eval_dataset_with_retrieval[\"retrieved_sources_at_3\"].tolist(),\n",
    "    exact_match=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9b875",
   "metadata": {},
   "source": [
    "At `top_k=13`, the retrieval score is 83%, the improvement in retrieval quality is by no means linear - there are diminishing returns as we increase n (the number of chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_score(\n",
    "    references=df_eval_dataset_with_retrieval[\"source\"].tolist(),\n",
    "    retrieved=df_eval_dataset_with_retrieval[\"retrieved_sources_at_13\"].tolist(),\n",
    "    exact_match=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d53d7",
   "metadata": {},
   "source": [
    "Let's visualize the retrieval score as a function of the number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba31242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrieval_scores = pd.DataFrame(\n",
    "    {\n",
    "        col_name: [\n",
    "            get_retrieval_score(\n",
    "                references=df_eval_dataset_with_retrieval[\"source\"].tolist(),\n",
    "                retrieved=df_eval_dataset_with_retrieval[\n",
    "                    f\"retrieved_sources_at_{n}\"\n",
    "                ].tolist(),\n",
    "                exact_match=exact_match,\n",
    "            )\n",
    "            for n in range(max_num_chunks + 1)\n",
    "        ]\n",
    "        for col_name, exact_match in zip(\n",
    "            [\"exact_match\", \"not_exact_match\"],\n",
    "            [True, False],\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "ax = df_retrieval_scores[\"exact_match\"].plot(marker=\"o\", label=\"Match on Section\")\n",
    "df_retrieval_scores[\"not_exact_match\"].plot(\n",
    "    ax=ax,\n",
    "    marker=\"o\",\n",
    "    label=\"Match on Page\",\n",
    "    title=\"Retrieval Score vs. Number of Retrieved Chunks\",\n",
    "    xlabel=\"Number of Retrieved Chunks\",\n",
    "    ylabel=\"Retrieval Score\",\n",
    "    grid=True,\n",
    "    xticks=range(0, 20, 2),\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4411337",
   "metadata": {},
   "source": [
    "The chart indicates two interesting points:\n",
    "- Retrieval improvement is not linear, there are diminishing returns as we increase n (the number of chunks).\n",
    "- If we expand to the entire parent page after retrieving a chunk which is a common strategy, we can expect to get a significant improvement in retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc21bf",
   "metadata": {},
   "source": [
    "### Taking it one step further\n",
    "\n",
    "**On context relevance**:\n",
    "\n",
    "So far our assessment of relevance was a binary decision in case the context overlaps with the context used to generate the question.\n",
    "\n",
    "We can make the score more granular by:\n",
    "- Prompting an LLM to extract relevant sentences from context given the question and either\n",
    "  - Ask the LLM to provide a score on a certain scale.\n",
    "  - Take the ratio of the number of extracted sentences to total number of sentences as the score.\n",
    "\n",
    "**On other measures of retrieval quality:**\n",
    "\n",
    "We can take a look at context attribution. In a more refined evaluation dataset where a given ground truth answer is associated with multiple contexts, we will want to measure the recall of the retrieval process.\n",
    "\n",
    "To do so, we can:\n",
    "- Prompt an LLM to attribute the context to the ground truth answer.\n",
    "\n",
    "### External References\n",
    "See the following third party references for more information on building an evaluation dataset:\n",
    "- [RAGAS Context Relevance](https://github.com/explodinggradients/ragas/blob/v0.0.22/src/ragas/metrics/_context_relevancy.py#L19)\n",
    "- [TrueLens Evaluation Context Relevance](https://github.com/truera/trulens/blob/trulens-eval-0.20.0/trulens_eval/trulens_eval/feedback/v2/feedback.py#L201)\n",
    "- [ARES Context Relevance](https://github.com/stanford-futuredata/ARES/blob/main/RAG_Automatic_Evaluation/LLMJudge_RAG_Compared_Scoring.py#L331)\n",
    "- [Llamaindex Context Relevance](https://github.com/run-llama/llama_index/blob/main/llama_index/evaluation/context_relevancy.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e3b25",
   "metadata": {},
   "source": [
    "### Cost Anaylsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af20c58",
   "metadata": {},
   "source": [
    "Let's assess our retrieval cost so far\n",
    "\n",
    "Retrieval cost is composed of two main components:\n",
    "- Cost of encoding the query\n",
    "- Cost of querying the search index\n",
    "\n",
    "\n",
    "Given the following pricing information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICING = {\n",
    "    # https://docs.endpoints.anyscale.com/pricing\n",
    "    \"embeddings\": {\n",
    "        # Pricing per 1M tokens\n",
    "        \"thenlper/gte-large\": 0.05,\n",
    "    },\n",
    "    # https://docs.pinecone.io/docs/understanding-cost#serverless-indexes\n",
    "    \"search\": {\n",
    "        # Pricing per 1M units\n",
    "        \"read_unit\": 8.25,  # A fetch request uses 1 RU for every 10 fetched records.\n",
    "        \"write_unit\": 2.00,\n",
    "        # Pricing per GB per month\n",
    "        \"storage\": 0.33,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913390f4",
   "metadata": {},
   "source": [
    "#### Cost of encoding the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb9890",
   "metadata": {},
   "source": [
    "We first compute the query embedding cost. To do so we need to count the number of tokens in each query and multiply by the cost per token.\n",
    "\n",
    "We implement the `count_tokens` function to count the number of tokens in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f97f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_tokens(text, model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f9dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\"Hello, how are you?\", EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de483507",
   "metadata": {},
   "source": [
    "We can now produce the number of tokens for all the queries in the evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ad4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = np.array([\n",
    "    count_tokens(question, EMBEDDING_MODEL_NAME)\n",
    "    for question in df_eval_dataset_with_retrieval[\"question\"].tolist()\n",
    "])\n",
    "\n",
    "total_tokens = token_count.sum()\n",
    "num_samples = token_count.size\n",
    "print(f\"Total tokens: {total_tokens} for a total of {num_samples} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e00e4",
   "metadata": {},
   "source": [
    "We can then compute the total embeddings cost by multiplying the total number of tokens by the cost per token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd97bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cost_per_token = PRICING[\"embeddings\"][EMBEDDING_MODEL_NAME] / 1e6\n",
    "query_encoding_costs = token_count * embeddings_cost_per_token\n",
    "total_query_encoding_cost = query_encoding_costs.sum()\n",
    "print(f\"Query encoding cost for {num_samples} questions: {total_query_encoding_cost * 100:.3f} cents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858d2a7",
   "metadata": {},
   "source": [
    "#### Cost of querying the search index\n",
    "\n",
    "We then compute the search cost based on the cost per read units used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a248887",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_cost_per_unit = PRICING[\"search\"][\"read_unit\"] / 1e6\n",
    "\n",
    "search_costs = {}\n",
    "for n in range(max_num_chunks + 1):\n",
    "    search_costs[f\"search_cost_at_{n}\"] = (\n",
    "        read_cost_per_unit\n",
    "        * df_eval_dataset_with_retrieval[f\"retrieval_usage_at_{n}\"]\n",
    "    )\n",
    "\n",
    "total_search_cost_for_20_chunks = search_costs[\"search_cost_at_20\"].sum()\n",
    "print(\n",
    "    f\"Total search cost for retrieving 20 chunks for {num_samples} questions: \"\n",
    "    f\"{total_search_cost_for_20_chunks * 100 :.3f} cents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce61e7d",
   "metadata": {},
   "source": [
    "### Total Retrieval Cost\n",
    "\n",
    "We finally put everything together to compute the total retrieval cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrieval_costs_per_question = pd.DataFrame(\n",
    "    {\n",
    "        **{\n",
    "            \"encoding_cost\": query_encoding_costs,\n",
    "        },\n",
    "        **search_costs,\n",
    "    }\n",
    ")\n",
    "df_retrieval_costs_per_question.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0128b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_cost_totals = defaultdict(list)\n",
    "\n",
    "for n in range(max_num_chunks + 1):\n",
    "    if n == 0:\n",
    "        retrieval_cost_totals[\"encoding\"].append(0)\n",
    "    else:\n",
    "        retrieval_cost_totals[\"encoding\"].append(\n",
    "            df_retrieval_costs_per_question[\"encoding_cost\"].sum() * 100\n",
    "        )\n",
    "    retrieval_cost_totals[\"search\"].append(\n",
    "        df_retrieval_costs_per_question[f\"search_cost_at_{n}\"].sum() * 100\n",
    "    )\n",
    "    retrieval_cost_totals[\"retrieval\"].append(\n",
    "        retrieval_cost_totals[\"encoding\"][-1] + retrieval_cost_totals[\"search\"][-1]\n",
    "    )\n",
    "    retrieval_cost_totals[\"num_chunks\"].append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrieval_costs = pd.DataFrame(retrieval_cost_totals)\n",
    "\n",
    "ax = df_retrieval_costs[\"encoding\"].plot(\n",
    "    marker=\"o\",\n",
    "    label=\"Query Encoding\",\n",
    ")\n",
    "df_retrieval_costs[\"search\"].plot(marker=\"o\", label=\"Search Index\", ax=ax)\n",
    "df_retrieval_costs[\"retrieval\"].plot(\n",
    "    marker=\"o\",\n",
    "    label=\"Retrieval (Total)\",\n",
    "    ax=ax,\n",
    "    grid=True,\n",
    "    title=f\"Total Cost vs. Number of Retrieved Chunks - {num_samples} Samples\",\n",
    "    xlabel=\"Number of Retrieved Chunks\",\n",
    "    ylabel=\"Cost (US Cents)\",\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c4abd",
   "metadata": {},
   "source": [
    "### Assessing Trade-offs\n",
    "\n",
    "Let's visualize the trade-offs between the retrieval score and the cost of retrieving the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrieval_costs_and_score = pd.merge(\n",
    "    df_retrieval_costs, df_retrieval_scores, left_on=\"num_chunks\", right_index=True\n",
    ")\n",
    "\n",
    "df_retrieval_costs_and_score.plot(\n",
    "    x=\"exact_match\",\n",
    "    y=\"retrieval\",\n",
    "    marker=\"o\",\n",
    "    title=f\"Retrieval Score vs. Retrieval Cost - {num_samples} samples\",\n",
    "    xlabel=\"Retrieval Score\",\n",
    "    ylabel=\"Cost (Cents)\",\n",
    "    grid=True,\n",
    ")\n",
    "for n in range(20):\n",
    "    plt.text(\n",
    "        df_retrieval_costs_and_score[\"exact_match\"].iloc[n],\n",
    "        df_retrieval_costs_and_score[\"retrieval\"].iloc[n],\n",
    "        str(n),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1e00f",
   "metadata": {},
   "source": [
    "Retrieving 1 document or 10 documents carries the same cost but the retrieval score is higher for 10 documents. Therefore if all we care about is retrieval score, we should retrieve 10 documents if we can afford it and it gets us to an \"acceptable retrieval score\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7bc99",
   "metadata": {},
   "source": [
    "## Evaluate Overall Quality\n",
    "\n",
    "We will perform the overall quality evaluation in two main steps:\n",
    "\n",
    "1. Run the generation process on the evaluation dataset to produce responses\n",
    "2. Produce an overall quality score by comparing the generated responses to the ground truth answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4cdf2",
   "metadata": {},
   "source": [
    "### 1. Run the generation process on the evaluation dataset to produce responses\n",
    "\n",
    "We construct a `QARunner` that encapsulates the entire generation process. It performs the following steps for a list of num_chunks to try:\n",
    "1. Augment the user prompt with the retrieved context\n",
    "2. Generate a response to the augmented query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QARunner:\n",
    "    def __init__(self, model: str):\n",
    "        self.llm = LLM(model=model)\n",
    "\n",
    "    def run(self, record: dict, num_chunks_to_try: list) -> dict:\n",
    "        for n in num_chunks_to_try:\n",
    "            response = self.llm.generate(\n",
    "                user_prompt=augment_prompt(\n",
    "                    record[\"question\"], record[f\"retrieved_contexts_at_{n}\"]\n",
    "                )\n",
    "            )\n",
    "            record[f\"response_at_{n}\"] = response.choices[0].message.content\n",
    "            record[f\"usage_completion_tokens_at_{n}\"] = (\n",
    "                response.usage.completion_tokens\n",
    "            )\n",
    "            record[f\"usage_prompt_tokens_at_{n}\"] = (\n",
    "                response.usage.prompt_tokens\n",
    "            )\n",
    "        return record\n",
    "\n",
    "    __call__ = run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02df9cc",
   "metadata": {},
   "source": [
    "We will proceed to limit the dataset size so we don't wait too long for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1\n",
    "\n",
    "ds = (\n",
    "    ray.data.read_parquet(\n",
    "        DATA_DIR / \"evaluation\" / \"reference_with_retrieval\",\n",
    "    )\n",
    "    .limit(limit)\n",
    "    .materialize()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976e1e4",
   "metadata": {},
   "source": [
    "We choose to experiment with the following subset of number of chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks_to_try = [0, 2, 3, 5, 10, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50b370",
   "metadata": {},
   "source": [
    "We run the QARunner by mapping it over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddea143",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.map(\n",
    "    QARunner,\n",
    "    fn_constructor_kwargs={\"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\"},\n",
    "    fn_kwargs={\"num_chunks_to_try\": num_chunks_to_try},\n",
    "    concurrency=(NUM_AVAILABLE_CPUS, NUM_AVAILABLE_CPUS),\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd42a8",
   "metadata": {},
   "source": [
    "### 2. Produce an overall quality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d24f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_scoring = \"\"\"\n",
    "Your job is to score the quality and correctness by providing an integer score between 1 and 5 of the following suggested answer:\n",
    "{generated_answer}\n",
    "\n",
    "given the following query:\n",
    "{query} \n",
    "\n",
    "and given the following ground truth answer:\n",
    "{reference_answer}.\n",
    "\n",
    "You must return JSON response with the following fields:\n",
    "- score: the score you give to the answer\n",
    "- reasoning: your reasoning for the score\n",
    "You must be very strict and avoid speculation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25a253",
   "metadata": {},
   "source": [
    "We create an `OverallQualityEvaluator` to evaluate the quality of the generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f237d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverallQualityEvaluator:\n",
    "    def __init__(self, prompt_template: str, model: str):\n",
    "        self.llm = LLM(model=model)\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        record: dict,\n",
    "        response_format: dict,\n",
    "        generated_answer_key: str,\n",
    "    ) -> dict:\n",
    "        prompt = self.prompt_template.format(\n",
    "            query=record[\"question\"],\n",
    "            generated_answer=record[generated_answer_key],\n",
    "            reference_answer=record[\"answer\"],\n",
    "        )\n",
    "        response = self.llm.generate(\n",
    "            user_prompt=prompt, response_format=response_format, temperature=0\n",
    "        )\n",
    "        try:\n",
    "            record.update(json.loads(response.choices[0].message.content))\n",
    "        except json.JSONDecodeError:\n",
    "            record[\"score\"] = -999\n",
    "            record[\"reasoning\"] = \"Invalid JSON response.\"\n",
    "        return record\n",
    "\n",
    "    __call__ = evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3022e",
   "metadata": {},
   "source": [
    "We will make use of JSON mode and accordingly define the JSON schema of the intended response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreResponse(BaseModel):\n",
    "    score: int = Field(\n",
    "        ...,\n",
    "        description=\"The score you give to the answer.\",\n",
    "        ge=1,\n",
    "        le=5,\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"Your reasoning for the score.\",\n",
    "    )\n",
    "\n",
    "ScoreResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea3ee6",
   "metadata": {},
   "source": [
    "We run a few examples to introspect the scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_quality_evaluator = OverallQualityEvaluator(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", prompt_template=prompt_template_scoring\n",
    ")\n",
    "overall_quality_evaluator.evaluate(\n",
    "    record={\n",
    "        \"question\": \"How can I deploy Ray Serve to Kubernetes?\",\n",
    "        \"answer\": \"You can deploy Ray Serve to Kubernetes using the KubeRay operator.\",\n",
    "        \"response_at_0\": \"I don't know.\",\n",
    "    },\n",
    "    generated_answer_key=\"response_at_0\",\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": ScoreResponse.model_json_schema(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_quality_evaluator = OverallQualityEvaluator(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", prompt_template=prompt_template_scoring\n",
    ")\n",
    "overall_quality_evaluator.evaluate(\n",
    "    record={\n",
    "        \"question\": \"How can I deploy Ray Serve to Kubernetes?\",\n",
    "        \"answer\": \"You can deploy Ray Serve to Kubernetes using the KubeRay operator and the Service Controller.\",\n",
    "        \"response_at_1\": \"Use the KubeRay operator.\",\n",
    "    },\n",
    "    generated_answer_key=\"response_at_1\",\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": ScoreResponse.model_json_schema(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4f18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_quality_evaluator = OverallQualityEvaluator(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", prompt_template=prompt_template_scoring\n",
    ")\n",
    "overall_quality_evaluator.evaluate(\n",
    "    record={\n",
    "        \"question\": \"How can I deploy Ray Serve to Kubernetes?\",\n",
    "        \"answer\": \"You can deploy Ray Serve to Kubernetes using the KubeRay operator and the Service Controller.\",\n",
    "        \"response_at_5\": \"Use the KubeRay operator and the Service Controller.\",\n",
    "    },\n",
    "    generated_answer_key=\"response_at_5\",\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": ScoreResponse.model_json_schema(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca01bd",
   "metadata": {},
   "source": [
    "To scale up the evaluation, we can use Ray to parallelize the evaluation of the generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1\n",
    "\n",
    "ds = (\n",
    "    ray.data.read_parquet(DATA_DIR / \"evaluation\" / \"reference_with_answer\")\n",
    "    .limit(limit)\n",
    "    .materialize()\n",
    ")\n",
    "\n",
    "out = ds.map(\n",
    "    OverallQualityEvaluator,\n",
    "    concurrency=limit,\n",
    "    fn_constructor_kwargs={\n",
    "        \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        \"prompt_template\": prompt_template_scoring,\n",
    "    },\n",
    "    fn_kwargs={\n",
    "        \"generated_answer_key\": \"response_at_20\",\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": ScoreResponse.model_json_schema(),\n",
    "        },\n",
    "    },\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86014f11",
   "metadata": {},
   "source": [
    "We have went ahead and ran the above code for the entire evaluation dataset over many chunks. We will go ahead and load the file to view how the scores look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50157093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quality_scores_per_prompt = pd.read_parquet(\n",
    "    DATA_DIR / \"evaluation\" / \"reference_with_scores\" / \"data.parquet\"\n",
    ")\n",
    "for n in num_chunks_to_try:\n",
    "    df_quality_scores_per_prompt = df_quality_scores_per_prompt[\n",
    "        df_quality_scores_per_prompt[f\"score_at_{n}\"] != -999\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5b6ed",
   "metadata": {},
   "source": [
    "We plot the mean generation scores for each number of retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_quality_scores = pd.DataFrame(\n",
    "    {\n",
    "        \"num_chunks\": num_chunks_to_try,\n",
    "        \"overall_quality_score\": [\n",
    "            df_quality_scores_per_prompt[f\"score_at_{num_chunks}\"].mean()\n",
    "            for num_chunks in num_chunks_to_try\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "df_overall_quality_scores.plot(\n",
    "    x=\"num_chunks\",\n",
    "    y=\"overall_quality_score\",\n",
    "    marker=\"o\",\n",
    "    grid=True,\n",
    "    title=\"Overall Mean Quality Score vs. Number of Retrieved Chunks\",\n",
    "    xlabel=\"Number of Retrieved Chunks\",\n",
    "    ylabel=\"Overall Mean Quality Score (1-5)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d7764",
   "metadata": {},
   "source": [
    "### Overall Cost Analysis\n",
    "\n",
    "Next we move to analyzing the cost of the overall quality evaluation.\n",
    "\n",
    "The total cost of the overall quality evaluation is composed of two main components:\n",
    "- Cost of Retrieval\n",
    "- Cost of Generation\n",
    "\n",
    "The cost of retrieval is the same as we computed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e66c6",
   "metadata": {},
   "source": [
    "### Generation cost\n",
    "\n",
    "Generation cost is computed as the total cost of input tokens and output tokens. In the case of anyscale endpoints input and output tokens are priced the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac64fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.endpoints.anyscale.com/pricing\n",
    "\n",
    "PRICING[\"llms\"] = {\n",
    "    # Pricing per 1M tokens\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\": {\"input\": 0.15, \"output\": 0.15},\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\": {\"input\": 0.25, \"output\": 0.25},\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\": {\"input\": 1, \"output\": 1},\n",
    "    \"codellama/CodeLlama-70b-Instruct-hf\": {\"input\": 1, \"output\": 1},\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\": {\"input\": 0.15, \"output\": 0.15},\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\": {\"input\": 0.50, \"output\": 0.50},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec1bf67",
   "metadata": {},
   "source": [
    "We compute the total cost for generating responses given 20 retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4018cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cost_at_20 = (\n",
    "    df_quality_scores_per_prompt[\"usage_completion_tokens_at_20\"]\n",
    "    * PRICING[\"llms\"][model][\"output\"]\n",
    "    / 1e6\n",
    "    + df_quality_scores_per_prompt[\"usage_prompt_tokens_at_20\"]\n",
    "    * PRICING[\"llms\"][model][\"input\"]\n",
    "    / 1e6\n",
    ").sum()\n",
    "\n",
    "print(\n",
    "    f\"Total cost at 20: {total_cost_at_20 * 100:.3f} cents for {num_samples} questions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfa50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cost_at_13 = (\n",
    "    df_quality_scores_per_prompt[\"usage_completion_tokens_at_13\"]\n",
    "    * PRICING[\"llms\"][model][\"output\"]\n",
    "    / 1e6\n",
    "    + df_quality_scores_per_prompt[\"usage_prompt_tokens_at_13\"]\n",
    "    * PRICING[\"llms\"][model][\"input\"]\n",
    "    / 1e6\n",
    ").sum()\n",
    "\n",
    "print(\n",
    "    f\"Total cost at 13: {total_cost_at_13 * 100:.3f} cents for {num_samples} queries.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cost_at_10 = (\n",
    "    df_quality_scores_per_prompt[\"usage_completion_tokens_at_10\"]\n",
    "    * PRICING[\"llms\"][model][\"output\"]\n",
    "    / 1e6\n",
    "    + df_quality_scores_per_prompt[\"usage_prompt_tokens_at_10\"]\n",
    "    * PRICING[\"llms\"][model][\"input\"]\n",
    "    / 1e6\n",
    ").sum()\n",
    "\n",
    "print(\n",
    "    f\"Total cost at 10: {total_cost_at_10 * 100:.3f} cents for {num_samples} queries.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cost_at_0 = (\n",
    "    df_quality_scores_per_prompt[\"usage_completion_tokens_at_0\"]\n",
    "    * PRICING[\"llms\"][model][\"output\"]\n",
    "    / 1e6\n",
    "    + df_quality_scores_per_prompt[\"usage_prompt_tokens_at_0\"]\n",
    "    * PRICING[\"llms\"][model][\"input\"]\n",
    "    / 1e6\n",
    ").sum()\n",
    "\n",
    "print(f\"Total cost at 0: {total_cost_at_0 * 100:.3f} cents for {num_samples} queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cost_totals = defaultdict(list)\n",
    "\n",
    "for num_chunks in num_chunks_to_try:\n",
    "    generation_cost = (\n",
    "        df_quality_scores_per_prompt[f\"usage_completion_tokens_at_{num_chunks}\"]\n",
    "        * PRICING[\"llms\"][model][\"output\"]\n",
    "        / 1e6\n",
    "        + df_quality_scores_per_prompt[f\"usage_prompt_tokens_at_{num_chunks}\"]\n",
    "        * PRICING[\"llms\"][model][\"input\"]\n",
    "        / 1e6\n",
    "    ).sum() * 100\n",
    "\n",
    "    gen_cost_totals[\"generation_cost\"].append(generation_cost)\n",
    "    gen_cost_totals[\"num_chunks\"].append(num_chunks)\n",
    "\n",
    "df_generation_costs = pd.DataFrame(gen_cost_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c8de3",
   "metadata": {},
   "source": [
    "### Overall cost\n",
    "\n",
    "Now that we computed both generation and retrieval costs, we can compute the overall cost for the entire RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2526fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_costs = pd.merge(\n",
    "    df_retrieval_costs,\n",
    "    df_generation_costs,\n",
    "    on=\"num_chunks\",\n",
    ")\n",
    "df_overall_costs[\"total_cost\"] = (\n",
    "    df_overall_costs[\"generation_cost\"] + df_overall_costs[\"retrieval\"]\n",
    ")\n",
    "df_overall_costs.plot(\n",
    "    x=\"num_chunks\", y=[\"retrieval\", \"generation_cost\", \"total_cost\"], marker=\"o\"\n",
    ")\n",
    "plt.grid()\n",
    "plt.title(f\"Total Cost vs. Number of Retrieved Chunks - {num_samples} samples\")\n",
    "plt.xlabel(\"Number of Retrieved Chunks\")\n",
    "plt.ylabel(\"Cost (US Cents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8b935",
   "metadata": {},
   "source": [
    "As you can see above LLM generation costs dominate the overall cost. The cost of retrieval is relatively low compared to the cost of generation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fac164",
   "metadata": {},
   "source": [
    "### Assess trade-offs between score and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10184826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_score_and_cost = pd.merge(\n",
    "    df_overall_quality_scores,\n",
    "    df_overall_costs,\n",
    "    on=\"num_chunks\",\n",
    ")\n",
    "\n",
    "df_overall_score_and_cost.plot(\n",
    "    x=\"total_cost\",\n",
    "    y=\"overall_quality_score\",\n",
    "    marker=\"o\",\n",
    "    grid=True,\n",
    "    title=f\"Overall Quality Score vs. Overall Cost - {num_samples} samples\",\n",
    "    xlabel=\"Overall Cost (US Cents)\",\n",
    "    ylabel=\"Overall Mean Score (1-5)\",\n",
    ")\n",
    "for idx, num_chunks in enumerate(num_chunks_to_try):\n",
    "    plt.text(\n",
    "        df_overall_score_and_cost[\"total_cost\"].iloc[idx],\n",
    "        df_overall_score_and_cost[\"overall_quality_score\"].iloc[idx],\n",
    "        str(num_chunks),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ac340",
   "metadata": {},
   "source": [
    "Based on the above chart we might want to choose `top_k=5` where the cost is relatively low and the overall quality score is relatively high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
