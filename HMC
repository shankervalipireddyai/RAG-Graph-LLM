import fitz  # PyMuPDF

def extract_paragraphs_from_pdf(pdf_path):
    paragraphs = []
    with fitz.open(pdf_path) as doc:
        for page in doc:
            # Extract text blocks with positional data
            blocks = page.get_text("blocks")
            
            for block in blocks:
                x0, y0, x1, y1, text, _, _ = block
                # Clean and normalize the text
                clean_text = text.strip()
                if clean_text:  # Ignore empty blocks
                    paragraphs.append(clean_text)
    return paragraphs

# Usage
pdf_file = "example.pdf"
paragraphs = extract_paragraphs_from_pdf(pdf_file)

# Display the paragraphs
for i, para in enumerate(paragraphs, start=1):
    print(f"Paragraph {i}:\n{para}\n")






pip install azure-ai-formrecognizer


import os
import json
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

# Set up Azure Document Intelligence client
endpoint = "https://<your-form-recognizer-endpoint>.cognitiveservices.azure.com/"
api_key = "<your-api-key>"
document_analysis_client = DocumentAnalysisClient(endpoint, AzureKeyCredential(api_key))

def parse_and_chunk_document(file_path):
    # Load PDF file
    with open(file_path, "rb") as f:
        poller = document_analysis_client.begin_analyze_document("prebuilt-layout", document=f)
        result = poller.result()
    
    # Initialize a list to hold structured content
    structured_content = []

    # Iterate through pages and extract headers, sub-headers, and paragraphs
    for page in result.pages:
        section_data = {
            "topic_name": None,       # Header or main section
            "section_name": None,     # Sub-header or subsection
            "section_description": [] # Paragraphs or sentences
        }
        
        for line in page.lines:
            text = line.content.strip()
            
            # Simple heuristics to identify headers and sections
            if text.isupper():  # Assume all-uppercase as a main header
                if section_data["topic_name"]:
                    # Save previous section before starting a new one
                    section_data["section_description"] = " ".join(section_data["section_description"])
                    structured_content.append(section_data)
                    section_data = {"topic_name": text, "section_name": None, "section_description": []}
                else:
                    section_data["topic_name"] = text
            elif text.endswith(":"):  # Assume text ending with ':' is a sub-header
                section_data["section_name"] = text
            else:
                # Treat as paragraph content
                section_data["section_description"].append(text)

        # Append the last section on each page
        if section_data["topic_name"]:
            section_data["section_description"] = " ".join(section_data["section_description"])
            structured_content.append(section_data)

    # Convert structured content to JSON format
    json_output = json.dumps(structured_content, indent=4)
    return json_output

# Example usage
pdf_file = r"C:\Users\shank\OneDrive\Desktop\RCM\RCM1\2024-10hsgml.pdf"

print("Parsing PDF with Azure Document Intelligence and converting to JSON:")
json_content = parse_and_chunk_document(pdf_file)
print(json_content)

# Optionally, save JSON output to a file
with open("output_azure_document_intelligence.json", "w") as f:
    f.write(json_content)




















import fitz  # PyMuPDF
import re

# Load the PDF document
pdf_path = "sample.pdf"
highlighted_phrases = [
    '- Cyber Incident Reporting',
    '- SEBI circular No: SEBI/HO/MIRSD/TPD/P/CIR/2022/93 dated June 30, 2022',
    '- Cyber-attacks, threats, cyber-incidents and breaches experienced by Stock Brokers / Depositories Participants shall be reported to Stock Exchanges / Depositories & SEBI within 6 hours',
    '- The incident shall also be reported to Indian Computer Emergency Response team (CERT-In)',
    '- Stock Brokers / Depository Participants, whose systems have been identified as “Protected system” by National Critical Information Infrastructure Protection Centre (NCIIPC) shall also report the incident to NCIIPC',
    '- Participants shall report about such incidents to NSDL through the dedicated e-mail id: dpincidents@nsdl.com',
    '- Investor Grievance Report (Monthly) By 10th of the following month Through e-PASS',
    '- Para 20 of NSDL Master Circular for Participants on ‘Grievance Redressal’ chapter.',
    '- Compliance Certificate (half yearly)',
    '- Para 17 of NSDL Master Circular for Participants on ‘Internal Controls/Reporting to NSDL/SEBI’ chapter.'
]

# Open the PDF document for highlighting
doc = fitz.open(pdf_path)

# Approach: Try to find each phrase by exact match first, then partial match
for page_num in range(doc.page_count):
    page = doc[page_num]
    page_text = page.get_text()

    for phrase in highlighted_phrases:
        # Attempt exact match first
        exact_locations = page.search_for(phrase)
        if exact_locations:
            print(f"Exact match found for phrase on page {page_num + 1}: {phrase}")
            for loc in exact_locations:
                highlight = page.add_highlight_annot(loc)
                highlight.update()
        else:
            # If exact match fails, attempt partial match
            print(f"No exact match, trying partial matches for phrase on page {page_num + 1}: {phrase}")
            parts = phrase.split()  # Split phrase into words or parts to search individually

            for part in parts:
                partial_locations = page.search_for(part)
                if partial_locations:
                    print(f"Partial match found for '{part}' on page {page_num + 1}")
                    for loc in partial_locations:
                        highlight = page.add_highlight_annot(loc)
                        highlight.update()
                else:
                    print(f"No match found for '{part}' on page {page_num + 1}")

# Save the modified PDF with highlights
output_path = "highlighted_sample_phrases_attempted.pdf"
doc.save(output_path)
doc.close()
print(f"Saved highlighted PDF to {output_path}")























import fitz  # PyMuPDF
import re
import time
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

# Function to extract sentences from text
def extract_sentences(text):
    # Regex pattern to extract sentences
    sentence_pattern = r'([A-Z][^.!?]*[.!?]+)'  # Matches sentences
    sentences = re.findall(sentence_pattern, text)
    return [s.strip() for s in sentences if s.strip()]

# Load the PDF document
pdf_path = "/content/data/2024-10hsgml.pdf"
doc = fitz.open(pdf_path)

# Initialize the language model
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0,
    max_tokens=1000
)

# Iterate through each page
for page_num in range(doc.page_count):
    page = doc[page_num]
    page_text = page.get_text()

    # Extract sentences from the page
    sentences = extract_sentences(page_text)
    print(f"Page {page_num + 1} - Extracted Sentences:")
    
    # Find and print the location of each sentence
    for sentence in sentences:
        # Search for the sentence in the page
        locations = page.search_for(sentence)
        if locations:
            print(f"Found sentence: '{sentence}' at locations: {locations}")

            # Prepare the prompt for the LLM
            prompt = f"""You are a regulatory finance technical specialist. Analyze the following text from a PDF document and identify if it indicates financial technical regulatory content. Respond with 'Yes' if it is regulatory and 'No' otherwise.

            Text:
            {sentence}

            Is it regulatory content?"""

            try:
                # Get the LLM response
                response = llm([HumanMessage(content=prompt)])
                is_regulatory = response.content.strip().lower()

                # Highlight the sentence only if it is regulatory
                if is_regulatory == 'yes':
                    for loc in locations:
                        highlight = page.add_highlight_annot(loc)
                        highlight.update()
                        print(f"Highlighted regulatory sentence: '{sentence}'")
                else:
                    print(f"Non-regulatory sentence: '{sentence}'")

                time.sleep(1)  # Add a delay to avoid rate limiting

            except Exception as e:
                print(f"Error processing sentence: {e}")
                time.sleep(60)  # Wait longer if we hit an error

# Save the modified PDF with highlights
output_path = "/content/data/highlighted_phrases_and_2024-10hsgml.pdf"
doc.save(output_path)
doc.close()
print(f"Saved highlighted PDF to {output_path}")































!pip install langchain langchain-community openai PyPDF2 PyMuPDF langchain-openai



import pymupdf as fitz  # PyMuPDF
import os
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import time

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = ""

# Path to your PDF file
pdf_path = "/content/data/CELEX_32019R0881_EN_TXT.pdf"

# Open the PDF file
doc = fitz.open(pdf_path)

# Function to split text into chunks
def split_text(text, chunk_size=2000):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

# Initialize the language model
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0,
    max_tokens=1000
)

# Process the PDF in chunks
highlighted_phrases = []
for page in doc:
    text = page.get_text()
    chunks = split_text(text)
    
    for chunk in chunks:
        prompt = f"""You are a regulatory finance technical specialist. Analyze the following text from a PDF document and identify phrases or sentences that indicate financial technical regulatory content. List only the relevant phrases or sentences, separated by newlines.

        Text:
        {chunk}

        Relevant phrases:"""

        try:
            response = llm([HumanMessage(content=prompt)])
            highlighted_phrases.extend(response.content.split('\n'))
            time.sleep(1)  # Add a delay to avoid rate limiting
        except Exception as e:
            print(f"Error processing chunk: {e}")
            time.sleep(60)  # Wait longer if we hit an error

# Highlight sections in the PDF and extract highlighted text
extracted_highlights = []
for page_number, page in enumerate(doc):
    for phrase in highlighted_phrases:
        if phrase.strip():
            areas = page.search_for(phrase.strip())
            for area in areas:
                highlight = page.add_highlight_annot(area)
                highlight.update()
                
                # Extract the highlighted text and its context
                words = page.get_text("words")
                highlight_rect = fitz.Rect(area)
                context_rect = highlight_rect + (-50, -20, 50, 20)  # Expand the area slightly
                context_text = " ".join(word[4] for word in words if fitz.Rect(word[:4]).intersects(context_rect))
                
                extracted_highlights.append({
                    'page': page_number + 1,
                    'text': context_text,
                    'rect': highlight_rect
                })

# Save the modified PDF with highlights
output_pdf_path = "/content/data/highlighted_CELEX_32019R0881_EN_TXT.pdf"
doc.save(output_pdf_path)
print(f"Highlighted PDF saved at {output_pdf_path}")

# Save extracted highlights to a text file
output_text_path = "/content/data/extracted_highlights.xls"
with open(output_text_path, 'w', encoding='utf-8') as f:
    for highlight in extracted_highlights:
        f.write(f"Page {highlight['page']}:\n")
        f.write(f"Text: {highlight['text']}\n")
        f.write(f"Location: {highlight['rect']}\n\n")

print(f"Extracted highlights saved at {output_text_path}")

# Close the document
doc.close()









import os
from pathlib import Path
from PyPDF2 import PdfReader
from langchain_community.chat_models import ChatOpenAI
from langchain_core.messages import HumanMessage
import time

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = ""

# Path to your PDF file
pdf_path = Path("/content/CELEX_32019R0881_EN_TXT.pdf")

# Function to read the entire PDF content
def read_pdf(file_path):
    with open(file_path, 'rb') as file:
        pdf_reader = PdfReader(file)
        text = ''
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

# Read the entire PDF content
pdf_content = read_pdf(pdf_path)

# Create a ChatOpenAI instance with GPT-4 and 128k context
llm = ChatOpenAI(
    model_name="gpt-4-1106-preview",
    temperature=0,
    max_tokens=1000
)

# Function to split text into chunks
def split_text(text, chunk_size=10000):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

# Split the PDF content into chunks
chunks = split_text(pdf_content)

# Analyze chunks
results = []
for i, chunk in enumerate(chunks):
    prompt = f"""You are a regulatory finance technical specialist. Analyze the following text (chunk {i+1} of {len(chunks)}) from a PDF document and determine if it contains financial technical regulatory content.
    Provide a brief explanation of your analysis, highlighting key indicators of financial technical regulatory content if present.
    If the content is not related to financial technical regulations, briefly explain why.

    Text chunk:
    {chunk}

    Question: Does this text chunk contain financial technical regulatory content?

    Answer:"""

    response = llm([HumanMessage(content=prompt)])
    results.append(response.content)

    # Sleep to avoid rate limiting
    time.sleep(20)

# Combine and analyze results
final_prompt = f"""You are a regulatory finance technical specialist. You have analyzed multiple chunks of a PDF document for financial technical regulatory content. Based on the following analysis results, provide a final determination on whether the entire document contains financial technical regulatory content. Summarize the key findings and explain your conclusion.

Analysis results:
{' '.join(results)}

Question: Based on these analyses, does the entire document contain financial technical regulatory content?

Answer:"""

final_response = llm([HumanMessage(content=final_prompt)])

print("Final Analysis:")
print(final_response.content)





















August 30 – September 2: Balanced Dataset for Procedures
•	Objective: Prepare a balanced dataset by restricting sentences per procedure to 20 rows.
•	Details:
o	Identified data imbalance with 200 procedures, some highly underrepresented.
o	Wrote code to restrict large classes to 20 sentences per procedure.
o	Verified distribution using visual charts (U-shaped imbalance vs. balanced backslash-shaped).
•	Challenge: Struggled to handle label overlap where sentences mapped to multiple labels.
•	Outcome: Model ready for initial training with a balanced subset.
________________________________________
September 2 – September 22: Fine-Tuning RoBERTa/BERT Models
•	Objective: Train RoBERTa/BERT models with 5k sentences and 200 procedures.
•	Details:
o	Used MultiLabelBinarizer (MLB) to handle multi-label sentences.
o	Encountered error: Labels column size mismatch (expected 22k, only got 221 unique procedures).
o	Trained with multiple settings, but F1 score remained very low (~0.003).
•	Challenge: Struggled to improve predictions even with balanced datasets and multiple hyperparameters.
•	Solution Attempts:
1.	Considered using NER and knowledge graphs to improve semantic understanding.
2.	Experimented with tokenization strategies but no meaningful impact on performance.


October 4: Tech/Non-Tech Classification using RoBERTa
•	Objective: Classify documents as tech or not tech.
•	Details:
o	Dataset: 259 tech docs vs. 59,000 non-tech PDFs.
o	Class imbalance identified as the primary challenge.
o	Trained with RoBERTa base model but faced accuracy issues due to the skew.
•	Challenge: Need for further balancing or resampling to address class imbalance.
•	Outcome: Task completed, but further experimentation needed to optimize results.
October 18: Debugging Prediction Scores with RoBERTa
•	Objective: Identify why RoBERTa is returning low prediction scores (~0.04).
•	Details:
o	Checked tokenization and input encoding using detailed logs.
o	Suggested using weighted loss functions (BCEWithLogitsLoss) to handle class imbalance.
o	Monitored model gradients to detect potential issues in parameter updates.
•	Challenge: Despite efforts, the F1 score remained low.
•	Next Steps: Implement knowledge graphs or NER for domain-specific improvements.




aiohttp==3.8.5
aiosignal==1.3.1
appnope==0.1.3
asttokens==2.2.1
async-timeout==4.0.2
attrs==22.2.0
backcall==0.2.0
beautifulsoup4==4.11.1
blobfile==2.0.1
bs4==0.0.1
certifi==2023.7.22
charset-normalizer==2.1.1
comm==0.1.2
contourpy==1.0.7
cycler==0.11.0
debugpy==1.6.5
decorator==5.1.1
docopt==0.6.2
entrypoints==0.4
executing==1.2.0
filelock==3.9.0
fonttools==4.38.0
frozenlist==1.3.3
huggingface-hub>=0.0.12
idna==3.4
ipykernel==6.20.1
ipython==8.10.0
jedi==0.18.2
joblib==1.2.0
jupyter_client==7.4.8
jupyter_core==5.1.3
kiwisolver==1.4.4
lxml==4.9.2
matplotlib==3.6.3
matplotlib-inline==0.1.6
multidict==6.0.4
nest-asyncio==1.5.6
numpy==1.24.1
openai==0.26.1
packaging==23.0
pandas==1.5.2
parso==0.8.3
pexpect==4.8.0
pickleshare==0.7.5
Pillow==9.4.0
pipreqs==0.4.12
platformdirs==2.6.2
plotly==5.12.0
prompt-toolkit==3.0.36
psutil==5.9.4
ptyprocess==0.7.0
pure-eval==0.2.2
pycryptodomex==3.17
Pygments==2.15.0
pyparsing==3.0.9
python-dateutil==2.8.2
pytz==2022.7.1
PyYAML==6.0
pyzmq==24.0.1
regex==2022.10.31
requests==2.31.0
scikit-learn==1.2.0
scipy==1.10.0
six==1.16.0
soupsieve==2.3.2.post1
stack-data==0.6.2
tenacity==8.1.0
threadpoolctl==3.1.0
tiktoken==0.1.2
tokenizers==0.13.2
tornado==6.3.3
tqdm==4.64.1
traitlets==5.8.1
transformers==4.30.0
typing_extensions==4.4.0
urllib3==1.26.13
wcwidth==0.2.5
yarg==0.1.9
yarl==1.8.2
















import requests
import re
import urllib.request
from bs4 import BeautifulSoup
from collections import deque
from html.parser import HTMLParser
from urllib.parse import urlparse
import os

# Regex pattern to match a URL
HTTP_URL_PATTERN = r'^http[s]*://.+'

# Define root domain to crawl
#domain = "openai.com"
#full_url = "https://openai.com/"

domain = "www.featur.ai"
full_url = "https://www.featur.ai/"

# Create a class to parse the HTML and get the hyperlinks
class HyperlinkParser(HTMLParser):
    def __init__(self):
        super().__init__()
        # Create a list to store the hyperlinks
        self.hyperlinks = []

    # Override the HTMLParser's handle_starttag method to get the hyperlinks
    def handle_starttag(self, tag, attrs):
        attrs = dict(attrs)

        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks
        if tag == "a" and "href" in attrs:
            self.hyperlinks.append(attrs["href"])

# Function to get the hyperlinks from a URL
def get_hyperlinks(url):

    # Try to open the URL and read the HTML
    try:
        # Open the URL and read the HTML
        with urllib.request.urlopen(url) as response:

            # If the response is not HTML, return an empty list
            if not response.info().get('Content-Type').startswith("text/html"):
                return []

            # Decode the HTML
            html = response.read().decode('utf-8')
    except Exception as e:
        print(e)
        return []

    # Create the HTML Parser and then Parse the HTML to get hyperlinks
    parser = HyperlinkParser()
    parser.feed(html)

    return parser.hyperlinks

# Function to get the hyperlinks from a URL that are within the same domain
def get_domain_hyperlinks(local_domain, url):
    clean_links = []
    for link in set(get_hyperlinks(url)):
        clean_link = None

        # If the link is a URL, check if it is within the same domain
        if re.search(HTTP_URL_PATTERN, link):
            # Parse the URL and check if the domain is the same
            url_obj = urlparse(link)
            if url_obj.netloc == local_domain:
                clean_link = link

        # If the link is not a URL, check if it is a relative link
        else:
            if link.startswith("/"):
                link = link[1:]
            elif link.startswith("#") or link.startswith("mailto:"):
                continue
            clean_link = "https://" + local_domain + "/" + link

        if clean_link is not None:
            if clean_link.endswith("/"):
                clean_link = clean_link[:-1]
            clean_links.append(clean_link)

    # Return the list of hyperlinks that are within the same domain
    return list(set(clean_links))


def crawl(url):
    # Parse the URL and get the domain
    local_domain = urlparse(url).netloc

    # Create a queue to store the URLs to crawl
    queue = deque([url])

    # Create a set to store the URLs that have already been seen (no duplicates)
    seen = set([url])

    # Create a directory to store the text files
    if not os.path.exists("text/"):
            os.mkdir("text/")

    if not os.path.exists("text/"+local_domain+"/"):
            os.mkdir("text/" + local_domain + "/")

    # Create a directory to store the csv files
    if not os.path.exists("processed"):
            os.mkdir("processed")

    # While the queue is not empty, continue crawling
    while queue:

        # Get the next URL from the queue
        url = queue.pop()
        print(url) # for debugging and to see the progress

        # Save text from the url to a <url>.txt file
        with open('text/'+local_domain+'/'+url[8:].replace("/", "_") + ".txt", "w") as f:

            # Get the text from the URL using BeautifulSoup
            soup = BeautifulSoup(requests.get(url).text, "html.parser")

            # Get the text but remove the tags
            text = soup.get_text()

            # If the crawler gets to a page that requires JavaScript, it will stop the crawl
            if ("You need to enable JavaScript to run this app." in text):
                print("Unable to parse page " + url + " due to JavaScript being required")

            # Otherwise, write the text to the file in the text directory
            f.write(text)

        # Get the hyperlinks from the URL and add them to the queue
        for link in get_domain_hyperlinks(local_domain, url):
            if link not in seen:
                queue.append(link)
                seen.add(link)

crawl(full_url)



!pip install -r requirements.txt

def remove_newlines(serie):
    serie = serie.str.replace('\n', ' ')
    serie = serie.str.replace('\\n', ' ')
    serie = serie.str.replace('  ', ' ')
    serie = serie.str.replace('  ', ' ')
    return serie
import pandas as pd

# Create a list to store the text files
texts=[]

# Get all the text files in the text directory
for file in os.listdir("text/" + domain + "/"):

    # Open the file and read the text
    with open("text/" + domain + "/" + file, "r") as f:
        text = f.read()

        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.
        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))

# Create a dataframe from the list of texts
df = pd.DataFrame(texts, columns = ['fname', 'text'])

# Set the text column to be the raw text with the newlines removed
df['text'] = df.fname + ". " + remove_newlines(df.text)
df.to_csv('processed/scraped.csv')
df.head()

fname	text



# Load the cl100k_base tokenizer which is designed to work with the ada-002 model
tokenizer = tiktoken.get_encoding("cl100k_base")

df = pd.read_csv('processed/scraped.csv', index_col=0)
df.columns = ['title', 'text']

# Tokenize the text and save the number of tokens to a new column
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))

# Visualize the distribution of the number of tokens per row using a histogram
df.n_tokens.hist()
<Axes: >

max_tokens = 500

# Function to split the text into chunks of a maximum number of tokens
def split_into_many(text, max_tokens = max_tokens):

    # Split the text into sentences
    sentences = text.split('. ')

    # Get the number of tokens for each sentence
    n_tokens = [len(tokenizer.encode(" " + sentence)) for sentence in sentences]

    chunks = []
    tokens_so_far = 0
    chunk = []

    # Loop through the sentences and tokens joined together in a tuple
    for sentence, token in zip(sentences, n_tokens):

        # If the number of tokens so far plus the number of tokens in the current sentence is greater
        # than the max number of tokens, then add the chunk to the list of chunks and reset
        # the chunk and tokens so far
        if tokens_so_far + token > max_tokens:
            chunks.append(". ".join(chunk) + ".")
            chunk = []
            tokens_so_far = 0

        # If the number of tokens in the current sentence is greater than the max number of
        # tokens, go to the next sentence
        if token > max_tokens:
            continue

        # Otherwise, add the sentence to the chunk and add the number of tokens to the total
        chunk.append(sentence)
        tokens_so_far += token + 1

    # Add the last chunk to the list of chunks
    if chunk:
        chunks.append(". ".join(chunk) + ".")

    return chunks


shortened = []

# Loop through the dataframe
for row in df.iterrows():

    # If the text is None, go to the next row
    if row[1]['text'] is None:
        continue

    # If the number of tokens is greater than the max number of tokens, split the text into chunks
    if row[1]['n_tokens'] > max_tokens:
        shortened += split_into_many(row[1]['text'])

    # Otherwise, add the text to the list of shortened texts
    else:
        shortened.append( row[1]['text'] )
df = pd.DataFrame(shortened, columns = ['text'])
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))
df.n_tokens.hist()


















web-qa.py FIleeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee


################################################################################
### Step 1
################################################################################

import requests
import re
import urllib.request
from bs4 import BeautifulSoup
from collections import deque
from html.parser import HTMLParser
from urllib.parse import urlparse
import os
import pandas as pd
import tiktoken
import openai
import numpy as np
from openai.embeddings_utils import distances_from_embeddings, cosine_similarity
from ast import literal_eval

# Regex pattern to match a URL
HTTP_URL_PATTERN = r'^http[s]{0,1}://.+$'

# Define OpenAI api_key
# openai.api_key = '<Your API Key>'

# Define root domain to crawl
domain = "openai.com"
full_url = "https://openai.com/"

# Create a class to parse the HTML and get the hyperlinks
class HyperlinkParser(HTMLParser):
    def __init__(self):
        super().__init__()
        # Create a list to store the hyperlinks
        self.hyperlinks = []

    # Override the HTMLParser's handle_starttag method to get the hyperlinks
    def handle_starttag(self, tag, attrs):
        attrs = dict(attrs)

        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks
        if tag == "a" and "href" in attrs:
            self.hyperlinks.append(attrs["href"])

################################################################################
### Step 2
################################################################################

# Function to get the hyperlinks from a URL
def get_hyperlinks(url):
    
    # Try to open the URL and read the HTML
    try:
        # Open the URL and read the HTML
        with urllib.request.urlopen(url) as response:

            # If the response is not HTML, return an empty list
            if not response.info().get('Content-Type').startswith("text/html"):
                return []
            
            # Decode the HTML
            html = response.read().decode('utf-8')
    except Exception as e:
        print(e)
        return []

    # Create the HTML Parser and then Parse the HTML to get hyperlinks
    parser = HyperlinkParser()
    parser.feed(html)

    return parser.hyperlinks

################################################################################
### Step 3
################################################################################

# Function to get the hyperlinks from a URL that are within the same domain
def get_domain_hyperlinks(local_domain, url):
    clean_links = []
    for link in set(get_hyperlinks(url)):
        clean_link = None

        # If the link is a URL, check if it is within the same domain
        if re.search(HTTP_URL_PATTERN, link):
            # Parse the URL and check if the domain is the same
            url_obj = urlparse(link)
            if url_obj.netloc == local_domain:
                clean_link = link

        # If the link is not a URL, check if it is a relative link
        else:
            if link.startswith("/"):
                link = link[1:]
            elif (
                link.startswith("#")
                or link.startswith("mailto:")
                or link.startswith("tel:")
            ):
                continue
            clean_link = "https://" + local_domain + "/" + link

        if clean_link is not None:
            if clean_link.endswith("/"):
                clean_link = clean_link[:-1]
            clean_links.append(clean_link)

    # Return the list of hyperlinks that are within the same domain
    return list(set(clean_links))


################################################################################
### Step 4
################################################################################

def crawl(url):
    # Parse the URL and get the domain
    local_domain = urlparse(url).netloc

    # Create a queue to store the URLs to crawl
    queue = deque([url])

    # Create a set to store the URLs that have already been seen (no duplicates)
    seen = set([url])

    # Create a directory to store the text files
    if not os.path.exists("text/"):
            os.mkdir("text/")

    if not os.path.exists("text/"+local_domain+"/"):
            os.mkdir("text/" + local_domain + "/")

    # Create a directory to store the csv files
    if not os.path.exists("processed"):
            os.mkdir("processed")

    # While the queue is not empty, continue crawling
    while queue:

        # Get the next URL from the queue
        url = queue.pop()
        print(url) # for debugging and to see the progress
        
        # Try extracting the text from the link, if failed proceed with the next item in the queue
        try:
            # Save text from the url to a <url>.txt file
            with open('text/'+local_domain+'/'+url[8:].replace("/", "_") + ".txt", "w", encoding="UTF-8") as f:

                # Get the text from the URL using BeautifulSoup
                soup = BeautifulSoup(requests.get(url).text, "html.parser")

                # Get the text but remove the tags
                text = soup.get_text()

                # If the crawler gets to a page that requires JavaScript, it will stop the crawl
                if ("You need to enable JavaScript to run this app." in text):
                    print("Unable to parse page " + url + " due to JavaScript being required")
            
                # Otherwise, write the text to the file in the text directory
                f.write(text)
        except Exception as e:
            print("Unable to parse page " + url)

        # Get the hyperlinks from the URL and add them to the queue
        for link in get_domain_hyperlinks(local_domain, url):
            if link not in seen:
                queue.append(link)
                seen.add(link)

crawl(full_url)

################################################################################
### Step 5
################################################################################

def remove_newlines(serie):
    serie = serie.str.replace('\n', ' ')
    serie = serie.str.replace('\\n', ' ')
    serie = serie.str.replace('  ', ' ')
    serie = serie.str.replace('  ', ' ')
    return serie


################################################################################
### Step 6
################################################################################

# Create a list to store the text files
texts=[]

# Get all the text files in the text directory
for file in os.listdir("text/" + domain + "/"):

    # Open the file and read the text
    with open("text/" + domain + "/" + file, "r", encoding="UTF-8") as f:
        text = f.read()

        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.
        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))

# Create a dataframe from the list of texts
df = pd.DataFrame(texts, columns = ['fname', 'text'])

# Set the text column to be the raw text with the newlines removed
df['text'] = df.fname + ". " + remove_newlines(df.text)
df.to_csv('processed/scraped.csv')
df.head()

################################################################################
### Step 7
################################################################################

# Load the cl100k_base tokenizer which is designed to work with the ada-002 model
tokenizer = tiktoken.get_encoding("cl100k_base")

df = pd.read_csv('processed/scraped.csv', index_col=0)
df.columns = ['title', 'text']

# Tokenize the text and save the number of tokens to a new column
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))

# Visualize the distribution of the number of tokens per row using a histogram
df.n_tokens.hist()

################################################################################
### Step 8
################################################################################

max_tokens = 500

# Function to split the text into chunks of a maximum number of tokens
def split_into_many(text, max_tokens = max_tokens):

    # Split the text into sentences
    sentences = text.split('. ')

    # Get the number of tokens for each sentence
    n_tokens = [len(tokenizer.encode(" " + sentence)) for sentence in sentences]
    
    chunks = []
    tokens_so_far = 0
    chunk = []

    # Loop through the sentences and tokens joined together in a tuple
    for sentence, token in zip(sentences, n_tokens):

        # If the number of tokens so far plus the number of tokens in the current sentence is greater 
        # than the max number of tokens, then add the chunk to the list of chunks and reset
        # the chunk and tokens so far
        if tokens_so_far + token > max_tokens:
            chunks.append(". ".join(chunk) + ".")
            chunk = []
            tokens_so_far = 0

        # If the number of tokens in the current sentence is greater than the max number of 
        # tokens, go to the next sentence
        if token > max_tokens:
            continue

        # Otherwise, add the sentence to the chunk and add the number of tokens to the total
        chunk.append(sentence)
        tokens_so_far += token + 1
        
    # Add the last chunk to the list of chunks
    if chunk:
        chunks.append(". ".join(chunk) + ".")

    return chunks
    

shortened = []

# Loop through the dataframe
for row in df.iterrows():

    # If the text is None, go to the next row
    if row[1]['text'] is None:
        continue

    # If the number of tokens is greater than the max number of tokens, split the text into chunks
    if row[1]['n_tokens'] > max_tokens:
        shortened += split_into_many(row[1]['text'])
    
    # Otherwise, add the text to the list of shortened texts
    else:
        shortened.append( row[1]['text'] )

################################################################################
### Step 9
################################################################################

df = pd.DataFrame(shortened, columns = ['text'])
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))
df.n_tokens.hist()

################################################################################
### Step 10
################################################################################

# Note that you may run into rate limit issues depending on how many files you try to embed
# Please check out our rate limit guide to learn more on how to handle this: https://platform.openai.com/docs/guides/rate-limits

df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])
df.to_csv('processed/embeddings.csv')
df.head()

################################################################################
### Step 11
################################################################################

df=pd.read_csv('processed/embeddings.csv', index_col=0)
df['embeddings'] = df['embeddings'].apply(literal_eval).apply(np.array)

df.head()

################################################################################
### Step 12
################################################################################

def create_context(
    question, df, max_len=1800, size="ada"
):
    """
    Create a context for a question by finding the most similar context from the dataframe
    """

    # Get the embeddings for the question
    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']

    # Get the distances from the embeddings
    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')


    returns = []
    cur_len = 0

    # Sort by distance and add the text to the context until the context is too long
    for i, row in df.sort_values('distances', ascending=True).iterrows():
        
        # Add the length of the text to the current length
        cur_len += row['n_tokens'] + 4
        
        # If the context is too long, break
        if cur_len > max_len:
            break
        
        # Else add it to the text that is being returned
        returns.append(row["text"])

    # Return the context
    return "\n\n###\n\n".join(returns)

def answer_question(
    df,
    model="text-davinci-003",
    question="Am I allowed to publish model outputs to Twitter, without a human review?",
    max_len=1800,
    size="ada",
    debug=False,
    max_tokens=150,
    stop_sequence=None
):
    """
    Answer a question based on the most similar context from the dataframe texts
    """
    context = create_context(
        question,
        df,
        max_len=max_len,
        size=size,
    )
    # If debug, print the raw model response
    if debug:
        print("Context:\n" + context)
        print("\n\n")

    try:
        # Create a completions using the questin and context
        response = openai.Completion.create(
            prompt=f"Answer the question based on the context below, and if the question can't be answered based on the context, say \"I don't know\"\n\nContext: {context}\n\n---\n\nQuestion: {question}\nAnswer:",
            temperature=0,
            max_tokens=max_tokens,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
            stop=stop_sequence,
            model=model,
        )
        return response["choices"][0]["text"].strip()
    except Exception as e:
        print(e)
        return ""

################################################################################
### Step 13
################################################################################

print(answer_question(df, question="What day is it?", debug=False))

print(answer_question(df, question="What is our newest embeddings model?"))































import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
import torch

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']
y = df['tech/non-tech']

# Create label dictionary
label_dict = {'Non-Tech': 0, 'Tech': 1}
reverse_label_dict = {v: k for k, v in label_dict.items()}

# Convert string labels to integer labels
y = y.map(label_dict)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=512,
    )

# Prepare datasets
train_data = Dataset.from_dict({'text': X_train.tolist(), 'label': y_train.tolist()})
test_data = Dataset.from_dict({'text': X_test.tolist(), 'label': y_test.tolist()})

# Tokenize datasets
train_data = train_data.map(tokenize_function, batched=True, remove_columns=['text'])
test_data = test_data.map(tokenize_function, batched=True, remove_columns=['text'])

# Rename 'label' to 'labels' as expected by the model
train_data = train_data.rename_column('label', 'labels')
test_data = test_data.rename_column('label', 'labels')

# Initialize model
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model='f1',
    greater_is_better=True
)

# Define compute_metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        'accuracy': (predictions == labels).mean(),
        'f1': f1_score(labels, predictions, average='weighted')
    }

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Make predictions and evaluate
predictions = trainer.predict(test_data)
pred_labels = np.argmax(predictions.predictions, axis=1)

# Print predictions for the first 20 rows
print("\nPredictions vs Actual (First 20 Rows):")
for i in range(20):
    print(f"Prediction: {reverse_label_dict[pred_labels[i]]}, Actual: {reverse_label_dict[y_test.iloc[i]]}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, pred_labels, target_names=list(label_dict.keys())))

# Confusion matrix
cm = confusion_matrix(y_test, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(label_dict.keys()), yticklabels=list(label_dict.keys()))
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Print class distribution
print("\nClass Distribution:")
print(y.map(reverse_label_dict).value_counts(normalize=True))
























import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
import torch

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']
y = df['tech/non-tech']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )

# Prepare datasets
train_data = Dataset.from_dict({'text': X_train.tolist(), 'label': y_train.tolist()})
test_data = Dataset.from_dict({'text': X_test.tolist(), 'label': y_test.tolist()})

# Tokenize datasets
train_data = train_data.map(tokenize_function, batched=True, remove_columns=['text'])
test_data = test_data.map(tokenize_function, batched=True, remove_columns=['text'])

# Convert labels to integers
train_data = train_data.map(lambda examples: {'labels': examples['label']}, remove_columns=['label'])
test_data = test_data.map(lambda examples: {'labels': examples['label']}, remove_columns=['label'])

# Set format for PyTorch
train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Initialize model
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model='f1',
    greater_is_better=True
)

# Define compute_metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        'accuracy': (predictions == labels).mean(),
        'f1': f1_score(labels, predictions, average='weighted')
    }

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Make predictions and evaluate
predictions = trainer.predict(test_data)
pred_labels = np.argmax(predictions.predictions, axis=1)

# Print predictions for the first 20 rows
print("\nPredictions vs Actual (First 20 Rows):")
for i in range(20):
    print(f"Prediction: {pred_labels[i]}, Actual: {y_test.iloc[i]}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, pred_labels, target_names=['Non-Tech', 'Tech']))

# Confusion matrix
cm = confusion_matrix(y_test, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Tech', 'Tech'], yticklabels=['Non-Tech', 'Tech'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Print class distribution
print("\nClass Distribution:")
print(y.value_counts(normalize=True))




















import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from collections import Counter
from sklearn.utils import resample
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
import torch

# Load Excel file (replace with your actual file path)
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']
y = df['tech/non-tech']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Concatenate training data back together into a DataFrame
df_train = pd.DataFrame({'summary': X_train, 'label': y_train}).reset_index(drop=True)

# Separate majority and minority classes
majority_class = df_train[df_train['label'] == 0]
minority_class = df_train[df_train['label'] == 1]

# Upsample minority class
minority_upsampled = resample(minority_class,
                              replace=True,
                              n_samples=len(majority_class),
                              random_state=42)

# Combine majority class with upsampled minority class
upsampled_train = pd.concat([majority_class, minority_upsampled])

# Separate features and labels
X_train_resampled, y_train_resampled = upsampled_train['summary'], upsampled_train['label']

# Initialize tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )

# Prepare datasets
train_data = Dataset.from_dict({'text': X_train_resampled.tolist(), 'label': y_train_resampled.tolist()})
test_data = Dataset.from_dict({'text': X_test.tolist(), 'label': y_test.tolist()})

# Tokenize datasets
train_data = train_data.map(tokenize_function, batched=True, remove_columns=['text'])
test_data = test_data.map(tokenize_function, batched=True, remove_columns=['text'])

# Convert labels to integers
train_data = train_data.map(lambda examples: {'labels': examples['label']}, remove_columns=['label'])
test_data = test_data.map(lambda examples: {'labels': examples['label']}, remove_columns=['label'])

# Set format for PyTorch
train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Initialize model
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model='f1',
    greater_is_better=True
)

# Define compute_metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        'accuracy': (predictions == labels).mean(),
        'f1': f1_score(labels, predictions, average='weighted')
    }

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Make predictions and evaluate
predictions = trainer.predict(test_data)
pred_labels = np.argmax(predictions.predictions, axis=1)

# Print predictions for the first 20 rows
print("\nPredictions vs Actual (First 20 Rows):")
for i in range(20):
    print(f"Prediction: {pred_labels[i]}, Actual: {y_test.iloc[i]}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, pred_labels, target_names=['Non-Tech', 'Tech']))

# Confusion matrix
cm = confusion_matrix(y_test, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Tech', 'Tech'], yticklabels=['Non-Tech', 'Tech'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()




import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from datasets import Dataset

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']  # Text column
y = df['tech/non-tech']  # Label column (tech or non-tech)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 1: Tokenize the text using RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenization function for Hugging Face datasets
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )

# Prepare dataset for Hugging Face Trainer
train_data = Dataset.from_dict({'text': X_train.tolist(), 'label': y_train.tolist()})
test_data = Dataset.from_dict({'text': X_test.tolist(), 'label': y_test.tolist()})

# Map tokenization function onto the datasets
train_data = train_data.map(tokenize_function, batched=True, remove_columns=['text'])
test_data = test_data.map(tokenize_function, batched=True, remove_columns=['text'])

# Set the format for PyTorch
train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Step 2: Define RoBERTa model and TrainingArguments
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    greater_is_better=True
)

# Step 3: Define compute_metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        'accuracy': (predictions == labels).mean(),
        'f1': f1_score(labels, predictions, average='weighted')
    }

# Step 4: Define Trainer and train the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Step 5: Make predictions and evaluate
predictions = trainer.predict(test_data)
pred_labels = np.argmax(predictions.predictions, axis=1)

# Print predictions for the first 20 rows and compare with actual labels
print("\nPredictions vs Actual (First 20 Rows):")
for i in range(20):
    print(f"Prediction: {pred_labels[i]}, Actual: {y_test.iloc[i]}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, pred_labels, target_names=['Non-Tech', 'Tech']))

# Confusion matrix
cm = confusion_matrix(y_test, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Tech', 'Tech'], yticklabels=['Non-Tech', 'Tech'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()



















import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from datasets import Dataset

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']  # Text column
y = df['tech/non-tech']  # Label column (tech or non-tech)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 1: Convert text data to TF-IDF features (Numerical representation)
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 2: Apply SMOTE to handle class imbalance on the numerical TF-IDF features
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# Step 3: Inverse transform the resampled TF-IDF features back into text
X_train_resampled_text = [' '.join(vectorizer.inverse_transform(doc)[0]) for doc in X_train_resampled]

# Step 4: Tokenize the text using RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenization function for Hugging Face datasets
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=True, truncation=True)

# Prepare dataset for Hugging Face Trainer
train_data = Dataset.from_dict({'text': X_train_resampled_text, 'label': y_train_resampled})
test_data = Dataset.from_dict({'text': X_test.tolist(), 'label': y_test.tolist()})

# Map tokenization function onto the datasets
train_data = train_data.map(tokenize_function, batched=True)
test_data = test_data.map(tokenize_function, batched=True)

# Set the format for PyTorch
train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Step 5: Define RoBERTa model and TrainingArguments
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True
)

# Step 6: Define Trainer and train the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    tokenizer=tokenizer,
    compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}
)

# Train the model
trainer.train()

# Step 7: Make predictions and evaluate
predictions = trainer.predict(test_data)
pred_labels = np.argmax(predictions.predictions, axis=1)

# Print predictions for the first 20 rows and compare with actual labels
print("\nPredictions vs Actual (First 20 Rows):")
for i in range(20):
    print(f"Prediction: {pred_labels[i]}, Actual: {y_test.iloc[i]}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, pred_labels, target_names=['Non-Tech', 'Tech']))

# Confusion matrix
cm = confusion_matrix(y_test, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Tech', 'Tech'], yticklabels=['Non-Tech', 'Tech'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()























import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from datasets import Dataset

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']  # Text column
y = df['tech/non-tech']  # Label column (tech or non-tech)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 1: Convert text data to TF-IDF features (Numerical representation)
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 2: Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# Step 3: Convert back the resampled TF-IDF matrix to text to feed into RoBERTa
X_train_resampled_text = vectorizer.inverse_transform(X_train_resampled)

# Load RoBERTa tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

# Tokenization function
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

# Prepare dataset for Hugging Face Trainer
train_data = Dataset.from_dict({'text': [' '.join(doc) for doc in X_train_resampled_text], 'label': y_train_resampled})
test_data = Dataset.from_dict({'text': X_test, 'label': y_test})

train_data = train_data.map(tokenize_function, batched=True)
test_data = test_data.map(tokenize_function, batched=True)

train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True
)

# Define the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    tokenizer=tokenizer,
    compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}
)

# Train the model
trainer.train()

# Make predictions
predictions = trainer.predict(test_data)
pred_labels = np.argmax(predictions.predictions, axis=1)

# Print predictions for the first 20 rows and compare with actual labels
print("\nPredictions vs Actual (First 20 Rows):")
for i in range(20):
    print(f"Prediction: {pred_labels[i]}, Actual: {y_test.iloc[i]}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, pred_labels, target_names=['Non-Tech', 'Tech']))

# Confusion matrix
cm = confusion_matrix(y_test, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Tech', 'Tech'], yticklabels=['Non-Tech', 'Tech'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()





























import pandas as pd
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import torch
from imblearn.over_sampling import SMOTE
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from datasets import Dataset

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']
y = df['tech/non-tech']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# SMOTE for handling class imbalance
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train.values.reshape(-1, 1), y_train)
X_train_resampled = X_train_resampled.flatten()

# Load RoBERTa tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

# Tokenization function
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

# Prepare dataset for Hugging Face Trainer
train_data = Dataset.from_dict({'text': X_train_resampled, 'label': y_train_resampled})
test_data = Dataset.from_dict({'text': X_test, 'label': y_test})

train_data = train_data.map(tokenize_function, batched=True)
test_data = test_data.map(tokenize_function, batched=True)

train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True
)

# Define the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    tokenizer=tokenizer,
    compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}
)

# Train the model
trainer.train()

# Make predictions
predictions = trainer.predict(test_data)
pred_labels = np.argmax(predictions.predictions, axis=1)

# Print predictions for the first 20 rows and compare with actual labels
print("\nPredictions vs Actual (First 20 Rows):")
for i in range(20):
    print(f"Prediction: {pred_labels[i]}, Actual: {y_test.iloc[i]}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, pred_labels, target_names=['Non-Tech', 'Tech']))

# Confusion matrix
cm = confusion_matrix(y_test, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Tech', 'Tech'], yticklabels=['Non-Tech', 'Tech'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()


















import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from cuml.ensemble import RandomForestClassifier as cuRF
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Analyze label imbalance
class_counts = df['tech/non-tech'].value_counts()
print("\nClass distribution (Tech vs Non-Tech):")
print(class_counts)

# Visualize class distribution
plt.figure(figsize=(6, 4))
sns.barplot(x=class_counts.index, y=class_counts.values, palette='Blues')
plt.title("Class Distribution of Tech vs Non-Tech")
plt.ylabel('Number of Documents')
plt.xlabel('Class')
plt.show()

# Handle the imbalance using SMOTE (oversample the minority class)
X = df['summary']  # Text column
y = df['tech/non-tech']  # Label column (tech or non-tech)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Convert text data to TF-IDF features
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Apply SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# Check the distribution after SMOTE
resampled_class_counts = pd.Series(y_train_resampled).value_counts()
print("\nClass distribution after SMOTE (Tech vs Non-Tech):")
print(resampled_class_counts)

# Visualize the class distribution after SMOTE
plt.figure(figsize=(6, 4))
sns.barplot(x=resampled_class_counts.index, y=resampled_class_counts.values, palette='Greens')
plt.title("Class Distribution After SMOTE")
plt.ylabel('Number of Documents')
plt.xlabel('Class')
plt.show()

# Naive Bayes (CPU-bound)
nb = MultinomialNB()

# GPU-based SVM using cuML
svm = SVC(probability=True, class_weight='balanced')  # Use balanced class weights to handle imbalance

# GPU-based Random Forest using cuML
rf = cuRF(n_estimators=100)

# PyTorch-based ANN (GPU)
class SimpleANN(nn.Module):
    def __init__(self, input_dim):
        super(SimpleANN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 100)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(100, 2)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Prepare PyTorch model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_dim = X_train_tfidf.shape[1]
model = SimpleANN(input_dim).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Convert TF-IDF sparse matrices to dense format for PyTorch
X_train_tensor = torch.tensor(X_train_resampled.toarray(), dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train_resampled.values, dtype=torch.long).to(device)

# Train ANN model
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

# Predictions from Naive Bayes, SVM, Random Forest, and ANN
X_test_dense = X_test_tfidf.toarray()  # Convert to dense format for PyTorch

# Predictions from each model
nb_pred = nb.fit(X_train_resampled.toarray(), y_train_resampled).predict(X_test_dense)
svm_pred = svm.fit(X_train_resampled, y_train_resampled).predict(X_test_tfidf)
rf_pred = rf.fit(X_train_resampled, y_train_resampled).predict(X_test_tfidf)

# PyTorch ANN predictions
model.eval()
X_test_tensor = torch.tensor(X_test_dense, dtype=torch.float32).to(device)
with torch.no_grad():
    ann_pred_proba = model(X_test_tensor)
ann_pred = torch.argmax(ann_pred_proba, dim=1).cpu().numpy()

# Combine predictions using majority voting
final_pred = np.array([np.bincount([nb_pred[i], svm_pred[i], rf_pred[i], ann_pred[i]]).argmax() for i in range(len(nb_pred))])

# Print predictions for the first 20 rows along with the actual labels
print("\nPredictions vs Actual (First 20 Rows):")
for i in range(20):
    print(f"Prediction: {final_pred[i]}, Actual: {y_test.iloc[i]}")

# Classification report and confusion matrix
print("\nClassification Report:")
print(classification_report(y_test, final_pred, target_names=['Non-Tech', 'Tech']))

# Confusion Matrix
cm = confusion_matrix(y_test, final_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Tech', 'Tech'], yticklabels=['Non-Tech', 'Tech'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

























import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from cuml.svm import SVC as cuSVC
from cuml.ensemble import RandomForestClassifier as cuRF
from sklearn.naive_bayes import MultinomialNB
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from cuml import UMAP

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']
y = df['tech/non-tech']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Convert text data to TF-IDF features
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# SMOTE for oversampling
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# Naive Bayes (CPU-bound)
nb = MultinomialNB()

# GPU-based SVM using cuML
svm = cuSVC(probability=True)

# GPU-based Random Forest using cuML
rf = cuRF(n_estimators=100)

# PyTorch-based ANN (GPU)
class SimpleANN(nn.Module):
    def __init__(self, input_dim):
        super(SimpleANN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 100)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(100, 2)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Prepare PyTorch model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_dim = X_train_tfidf.shape[1]
model = SimpleANN(input_dim).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Convert TF-IDF sparse matrices to dense format for PyTorch
X_train_tensor = torch.tensor(X_train_resampled.toarray(), dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train_resampled.values, dtype=torch.long).to(device)

# Train ANN model
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

# Predictions and ensemble would follow here, combining the outputs from all classifiers.

















import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
from transformers import pipeline

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']  # Text column
y = df['tech/non-tech']  # Label column (tech or non-tech)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Convert text data to TF-IDF features for traditional models
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Handle imbalance using SMOTE (oversample the minority class)
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# Define traditional models
nb = MultinomialNB()
svm = SVC(probability=True, class_weight='balanced')  # SVM with class weight adjustment
rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')  # Random Forest with class weight adjustment
ann = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300)

# Load RoBERTa model for text classification (using Hugging Face pipeline)
roberta_classifier = pipeline("text-classification", model="roberta-base", return_all_scores=True)

# Function to get RoBERTa predictions (transforming output to binary labels)
def roberta_predict(texts):
    preds = []
    for text in texts:
        roberta_output = roberta_classifier(text)[0]
        # Assuming 'tech' is class 1 and 'non-tech' is class 0
        pred_label = 1 if roberta_output[1]['score'] > roberta_output[0]['score'] else 0
        preds.append(pred_label)
    return preds

# Get RoBERTa predictions on training and test sets
X_train_roberta_preds = roberta_predict(X_train)
X_test_roberta_preds = roberta_predict(X_test)

# Add RoBERTa predictions as a feature to the traditional models
X_train_combined = pd.DataFrame(X_train_resampled.toarray())
X_train_combined['roberta_pred'] = X_train_roberta_preds

X_test_combined = pd.DataFrame(X_test_tfidf.toarray())
X_test_combined['roberta_pred'] = X_test_roberta_preds

# Create an ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[
    ('nb', nb),
    ('svm', svm),
    ('rf', rf),
    ('ann', ann)
], voting='soft')  # Use 'soft' voting to average probabilities

# Train the ensemble model on the resampled data with RoBERTa predictions
ensemble_model.fit(X_train_combined, y_train_resampled)

# Predict on the test set with RoBERTa predictions
y_pred = ensemble_model.predict(X_test_combined)

# Calculate accuracy and show classification report
accuracy = accuracy_score(y_test, y_pred)
print(f'Ensemble model accuracy: {accuracy:.4f}')
print(classification_report(y_test, y_pred, target_names=['Non-Tech', 'Tech']))








































import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.utils.class_weight import compute_class_weight

# Load Excel file
df = pd.read_excel('regulations.xlsx')

# Separate features and labels
X = df['summary']  # Text column
y = df['tech/non-tech']  # Label column (tech or non-tech)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Convert text data to TF-IDF features
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Handle imbalance using SMOTE (oversample the minority class)
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# Define individual models
nb = MultinomialNB()
svm = SVC(probability=True, class_weight='balanced')  # SVM with class weight adjustment
rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')  # Random Forest with class weight adjustment
ann = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300)

# Create an ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[
    ('nb', nb),
    ('svm', svm),
    ('rf', rf),
    ('ann', ann)
], voting='soft')  # Use 'soft' voting to average probabilities

# Train the ensemble model on the resampled data
ensemble_model.fit(X_train_resampled, y_train_resampled)

# Predict on the test set
y_pred = ensemble_model.predict(X_test_tfidf)

# Calculate accuracy and show classification report
accuracy = accuracy_score(y_test, y_pred)
print(f'Ensemble model accuracy: {accuracy:.4f}')
print(classification_report(y_test, y_pred, target_names=['Non-Tech', 'Tech']))



pip install imbalanced-learn















def debug_model(question):
    print("### Step 1: Analyzing the Input Question ###")
    
    # Tokenize the input question
    text_enc = Bert_tokenizer.encode_plus(
        question,
        None,
        add_special_tokens=True,
        max_length=MAX_LEN,
        padding='max_length',
        return_token_type_ids=False,
        return_attention_mask=True,
        truncation=True,
        return_tensors='pt'
    )

    # Check tokenization details
    input_ids = text_enc['input_ids']
    attention_mask = text_enc['attention_mask']
    print("Tokenized Input IDs:", input_ids)
    print("Attention Mask:", attention_mask)
    
    decoded_question = Bert_tokenizer.decode(input_ids[0], skip_special_tokens=True)
    print("Decoded Question:", decoded_question)
    
    # Determine the device of the model
    device = next(QTmodel.parameters()).device

    # Move input tensors to the same device as the model
    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    print("\n### Step 2: Checking Model Predictions ###")
    
    # Perform the forward pass
    with torch.no_grad():
        outputs = QTmodel(input_ids, attention_mask)
    
    # Move the output back to CPU for numpy conversion
    pred_out = outputs[0].detach().cpu().numpy()
    print("Raw Model Output (Logits):", pred_out)

    # Apply softmax/sigmoid if needed (depending on whether it's single-label or multi-label)
    probabilities = torch.sigmoid(torch.tensor(pred_out)) if is_multilabel else torch.softmax(torch.tensor(pred_out), dim=-1)
    print("Probabilities:", probabilities)
    
    # If it's multi-label classification, convert predictions based on threshold
    threshold = 0.5
    predicted_labels = (probabilities > threshold).int().numpy()
    print("Predicted Labels (with threshold 0.5):", predicted_labels)

    print("\n### Step 3: Investigating Dataset Label Distribution ###")
    # Optionally, visualize the label distribution if applicable
    plt.figure(figsize=(12, 6))
    sns.countplot(x=filtered_df['procedure'])
    plt.xticks(rotation=90)
    plt.title("Procedure Label Distribution")
    plt.show()

    print("\n### Step 4: Checking Model Confidence ###")
    print("Model Confidence per Class:", probabilities)

    print("\n### Step 5: Checking Tokenization and Padding ###")
    print("Padded Input IDs:", input_ids)
    print("Decoded Question after Padding/Truncation:", decoded_question)

    print("\n### Step 6: Checking Training History ###")
    # Assuming you have training and validation loss history
    plt.plot(train_loss, label="Training Loss")
    plt.plot(val_loss, label="Validation Loss")
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    print("\n### Step 7: Label-Specific Performance ###")
    # If you have true labels and predictions, you can compute metrics like precision, recall, and F1 score
    y_true = np.array(true_labels)  # Replace with actual ground truth
    y_pred = predicted_labels  # Replace with actual model predictions

    print(classification_report(y_true, y_pred, target_names=mlb.classes_))

    print("\n### Step 8: Adjusting Prediction Threshold ###")
    # Adjusting the threshold if needed
    threshold = 0.3  # Try different thresholds
    predictions = (probabilities > threshold).int().numpy()
    print("Predictions with Threshold", threshold, ":", predictions)

# Example usage:
debug_model("How can I implement network access control?")






















import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re

# Load your dataset
filtered_df = pd.read_csv('your_dataset.csv')  # Replace with actual dataset path

# 1. Check for Missing Values and Plot
missing_values = filtered_df.isnull().sum()
plt.figure(figsize=(8, 6))
missing_values.plot(kind='bar', color='orange')
plt.title("Missing Values per Column")
plt.ylabel("Count")
plt.show()
print("Missing values per column:\n", missing_values)

# 2. Check for Duplicate Entries and Show Count
duplicates = filtered_df.duplicated(subset=['extended_text', 'procedure']).sum()
print(f"Number of duplicate rows: {duplicates}")

# 3. Check for Inconsistent Procedures (e.g., spelling mistakes, whitespace issues)
procedure_unique = filtered_df['procedure'].unique()
plt.figure(figsize=(12, 6))
sns.countplot(y=filtered_df['procedure'], order=filtered_df['procedure'].value_counts().index)
plt.title("Count of Each Procedure Label")
plt.show()
print(f"Unique procedures:\n{procedure_unique}")

# Optional: Manual mapping to fix known inconsistent procedures (if applicable)
# filtered_df['procedure'] = filtered_df['procedure'].replace({'inconsistent_name':'correct_name'})

# 4. Analyze Sentence Length Distribution
sentence_lengths = filtered_df['extended_text'].apply(lambda x: len(x.split()))
plt.figure(figsize=(10, 6))
sns.histplot(sentence_lengths, kde=True, color='blue')
plt.title("Sentence Length Distribution")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()

# Identify sentences that are too short or too long
short_sentences = filtered_df[filtered_df['extended_text'].apply(lambda x: len(x.split()) < 5)]
long_sentences = filtered_df[filtered_df['extended_text'].apply(lambda x: len(x.split()) > 100)]
print(f"Number of short sentences (<5 words): {len(short_sentences)}")
print(f"Number of long sentences (>100 words): {len(long_sentences)}")

# 5. Detect Noise (Special characters, numbers, etc.) and Plot
def has_noise(text):
    return bool(re.search(r'[^a-zA-Z\s]', text))

noisy_sentences = filtered_df[filtered_df['extended_text'].apply(has_noise)]
print(f"Number of noisy sentences: {len(noisy_sentences)}")

# Plot sentence length distribution for noisy vs non-noisy sentences
filtered_df['has_noise'] = filtered_df['extended_text'].apply(has_noise)
plt.figure(figsize=(10, 6))
sns.histplot(data=filtered_df, x='extended_text', hue='has_noise', kde=True, palette="Set1")
plt.title("Sentence Length Distribution (Noisy vs Non-Noisy)")
plt.show()

# 6. Plot Distribution of Sentences per Procedure
procedure_counts = filtered_df['procedure'].value_counts()
plt.figure(figsize=(12, 6))
sns.barplot(x=procedure_counts.index, y=procedure_counts.values, palette="viridis")
plt.xticks(rotation=90)
plt.title("Number of Sentences per Procedure")
plt.ylabel("Count")
plt.show()

# 7. Text Cleaning (Optional: remove extra whitespace, symbols, etc.)
def clean_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

filtered_df['extended_text_clean'] = filtered_df['extended_text'].apply(clean_text)

# Display a sample of cleaned text vs original text
sample_data = filtered_df[['extended_text', 'extended_text_clean']].sample(5)
print("Original vs Cleaned Text Sample:")
print(sample_data)

# Save the cleaned dataset
filtered_df.to_csv('cleaned_dataset.csv', index=False)
print("Cleaned dataset saved as 'cleaned_dataset.csv'")
















import pandas as pd

# Assuming 'filtered_df' contains 'extended_text' and 'procedure' columns
# Step 1: Group by 'procedure' and limit to 20 sentences only for procedures with 30 or more sentences
max_sentences_per_procedure = 20
min_sentences_threshold = 30

# Group by 'procedure' and apply a limit of 20 rows only if the procedure has 30 or more sentences
balanced_df = filtered_df.groupby('procedure').apply(lambda x: x.sample(n=max_sentences_per_procedure) 
                                                     if len(x) >= min_sentences_threshold else x)

# Reset the index of the dataframe
balanced_df = balanced_df.reset_index(drop=True)

# Print some basic info about the balanced dataframe
print(f"Original dataset size: {len(filtered_df)}")
print(f"Balanced dataset size: {len(balanced_df)}")
print(balanced_df['procedure'].value_counts())

# Save the balanced dataset to a new CSV file if needed
balanced_df.to_csv('balanced_filtered_df.csv', index=False)














import pandas as pd
import matplotlib.pyplot as plt

# Assuming filtered_df contains 'extended_text' and 'procedure' columns

# Step 1: Sentence Distribution
sentence_counts = filtered_df['extended_text'].value_counts()

# Display the most and least frequent sentences
print("Top 10 most frequent sentences:")
print(sentence_counts.head(10))

print("\nTop 10 least frequent sentences:")
print(sentence_counts.tail(10))

# Plot sentence distribution
plt.figure(figsize=(12, 6))
sentence_counts.plot(kind='bar', color='lightgreen')
plt.title('Sentence Distribution')
plt.xlabel('Sentences')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

# Step 2: Procedure Distribution as in previous analysis
procedure_counts = filtered_df['procedure'].value_counts()

# Plot procedure distribution again for reference
plt.figure(figsize=(12, 6))
procedure_counts.plot(kind='bar', color='skyblue')
plt.title('Procedure Label Distribution')
plt.xlabel('Procedure Labels')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

# Step 3: Procedure-Sentence Relationship
# Group the data by sentence and count the unique procedures per sentence
sentence_procedure_counts = filtered_df.groupby('extended_text')['procedure'].nunique()

# Display the most and least frequent sentences by number of unique procedures they map to
print("Top 10 sentences with the most unique procedures:")
print(sentence_procedure_counts.sort_values(ascending=False).head(10))

print("\nTop 10 sentences with the least unique procedures:")
print(sentence_procedure_counts.sort_values(ascending=True).head(10))

# Plot the number of unique procedures per sentence
plt.figure(figsize=(12, 6))
sentence_procedure_counts.plot(kind='bar', color='orange')
plt.title('Number of Unique Procedures per Sentence')
plt.xlabel('Sentences')
plt.ylabel('Number of Unique Procedures')
plt.xticks(rotation=90)
plt.show()

# Step 4: Procedure Distribution per Sentence
# Check how many times each procedure is mapped to different sentences
procedure_sentence_counts = filtered_df.groupby('procedure')['extended_text'].nunique()

# Display the most and least frequent procedures in terms of mapping to sentences
print("Top 10 procedures with the most unique sentences:")
print(procedure_sentence_counts.sort_values(ascending=False).head(10))

print("\nTop 10 procedures with the least unique sentences:")
print(procedure_sentence_counts.sort_values(ascending=True).head(10))

# Plot procedure-sentence relationship
plt.figure(figsize=(12, 6))
procedure_sentence_counts.plot(kind='bar', color='purple')
plt.title('Number of Unique Sentences per Procedure')
plt.xlabel('Procedure')
plt.ylabel('Number of Unique Sentences')
plt.xticks(rotation=90)
plt.show()



























import pandas as pd
import matplotlib.pyplot as plt

# Assuming filtered_df contains 'extended_text' and 'procedure' columns

# Step 1: Check the distribution of the procedures
procedure_counts = filtered_df['procedure'].value_counts()

# Step 2: Display the most frequent and least frequent procedures
print("Top 10 most frequent procedures:")
print(procedure_counts.head(10))

print("\nTop 10 least frequent procedures:")
print(procedure_counts.tail(10))

# Step 3: Plot the distribution of the procedures
plt.figure(figsize=(12, 6))
procedure_counts.plot(kind='bar', color='skyblue')
plt.title('Procedure Label Distribution')
plt.xlabel('Procedure Labels')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

# Step 4: Calculate the proportion of each label
procedure_proportion = procedure_counts / len(filtered_df)
print("\nProcedure Proportions:")
print(procedure_proportion.head(10))

# Step 5: Identify potential imbalance (e.g., procedures that are highly underrepresented)
imbalance_threshold = 0.01  # Example: Define threshold as 1% of the data
underrepresented_procedures = procedure_proportion[procedure_proportion < imbalance_threshold]

print(f"\nNumber of underrepresented procedures (less than {imbalance_threshold * 100}% of the data): {len(underrepresented_procedures)}")
print("Underrepresented procedures:")
print(underrepresented_procedures)


















# Ensure that labels_procedures are in the correct format
# Each procedure should be treated as a list, even if there is only one procedure per sentence
labels_procedures = [[label] if isinstance(label, str) else label for label in labels_procedures]

# Encode labels using MultiLabelBinarizer
mlb = MultiLabelBinarizer()
encoded_labels = mlb.fit_transform(labels_procedures)

# Ensure that the number of labels matches the number of sentences
assert len(sentences) == len(encoded_labels), "Mismatch between sentences and encoded labels!"

# Define number of labels (unique procedures)
num_labels = len(mlb.classes_)
print(f"Number of unique procedures (num_labels): {num_labels}")







import torch
import torch.nn as nn
from transformers import RobertaTokenizer, RobertaModel, AdamW, Trainer, TrainingArguments
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
import numpy as np
import optuna
from transformers import RobertaForSequenceClassification
from transformers.trainer_utils import EvalPrediction

# Assuming 'filtered_df' contains the sentence and procedure columns
sentences = filtered_df['sentence'].tolist()  # List of sentences
procedures = filtered_df['procedure'].tolist()  # List of single procedures

# LabelBinarizer to one-hot encode the procedure labels
lb = LabelBinarizer()
encoded_labels = lb.fit_transform(procedures)

# Tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

def tokenize_sentences(sentences, tokenizer, max_len=512):
    inputs = tokenizer(sentences, padding=True, truncation=True, max_length=max_len, return_tensors="pt")
    return inputs['input_ids'], inputs['attention_mask']

# Tokenize the input sentences
input_ids, attention_mask = tokenize_sentences(sentences, tokenizer)

# Convert labels to tensor
labels_procedures = torch.tensor(encoded_labels, dtype=torch.float32)

# Dataset and DataLoader
train_dataset = TensorDataset(input_ids, attention_mask, labels_procedures)

# Define the Model
class RobertaSingleLabelClassifier(nn.Module):
    def __init__(self, num_labels):
        super(RobertaSingleLabelClassifier, self).__init__()
        self.roberta = RobertaModel.from_pretrained('roberta-base')
        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output  # Get the pooled output
        logits = self.classifier(pooled_output)
        
        if labels is not None:
            loss_fn = nn.BCEWithLogitsLoss()
            loss = loss_fn(logits, labels)
            return loss, logits
        return logits

# Initialize the model
num_procedures = len(lb.classes_)
model = RobertaSingleLabelClassifier(num_labels=num_procedures)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,  # You can adjust this based on your needs
    save_total_limit=2,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
)

# Compute metrics for evaluation
def compute_metrics(p: EvalPrediction):
    preds = np.argmax(p.predictions, axis=1)
    true_labels = np.argmax(p.label_ids, axis=1)

    accuracy = accuracy_score(true_labels, preds)
    f1 = f1_score(true_labels, preds, average='weighted')
    precision = precision_score(true_labels, preds, average='weighted')
    recall = recall_score(true_labels, preds, average='weighted')

    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'confusion_matrix': confusion_matrix(true_labels, preds)
    }

# Prepare data for Trainer
train_data = TensorDataset(input_ids, attention_mask, labels_procedures)

train_loader = DataLoader(train_data, batch_size=16, shuffle=True)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=train_data,  # Here we use the train data for simplicity, use separate val set in real use
    compute_metrics=compute_metrics,
)

# Run training
trainer.train()

# Predict function
def predict(model, sentence):
    model.eval()
    tokens = tokenizer(sentence, return_tensors="pt", padding='max_length', truncation=True, max_length=512)
    input_ids = tokens['input_ids'].to('cuda')

    with torch.no_grad():
        logits = model(input_ids)

    # Apply sigmoid to get probabilities
    procedure_probs = torch.sigmoid(logits).squeeze().cpu().numpy()

    # Convert probabilities to binary predictions (threshold can be adjusted)
    procedure_preds = procedure_probs > 0.5

    # Map binary predictions to class names
    procedures = lb.inverse_transform([procedure_preds])[0]

    return {
        "sentence": sentence,
        "procedures": list(procedures)
    }

# Example usage
example_sentence = "Implement network access control."
prediction = predict(model, example_sentence)
print(prediction)

# Optuna hyperparameter optimization (optional)
def objective(trial):
    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)

    model = RobertaSingleLabelClassifier(num_labels=num_procedures)

    training_args = TrainingArguments(
        output_dir='./results',
        evaluation_strategy="epoch",
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=3,
        learning_rate=lr,
        save_total_limit=2,
        logging_dir='./logs',
        logging_steps=10,
        load_best_model_at_end=True,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        eval_dataset=train_data,
        compute_metrics=compute_metrics,
    )

    result = trainer.train()
    eval_result = trainer.evaluate()
    
    return eval_result['eval_loss']

# Optuna study
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=5)  # You can set n_trials to the number you want

best_trial = study.best_trial
print(f"Best trial: {best_trial.params}")

# Save the best model
model.save_pretrained('./best_model')


















import torch
import numpy as np
import pandas as pd
from datasets import Dataset
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import f1_score, precision_score, recall_score
from transformers import (
    RobertaTokenizer,
    RobertaForSequenceClassification,
    Trainer,
    TrainingArguments
)
import torch.nn as nn
import optuna
from optuna.integration import PyTorchLightningPruningCallback
from optuna.trial import TrialState

# Load your dataset
# Assuming 'filtered_df' is your DataFrame containing 'extended_text' and 'procedure' columns
filtered_df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path

# Data Preparation
sentences = filtered_df['extended_text'].tolist()
labels_procedures = filtered_df['procedure'].tolist()

# Convert labels to list of lists
labels_procedures = [eval(label) if isinstance(label, str) else label for label in labels_procedures]

# Encode labels using MultiLabelBinarizer
mlb = MultiLabelBinarizer()
encoded_labels = mlb.fit_transform(labels_procedures)

# Define number of labels
num_labels = len(mlb.classes_)
print(f"Number of unique procedures (num_labels): {num_labels}")

# Initialize tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenization function
def tokenize_sentences(examples):
    return tokenizer(
        examples['sentence'],
        padding='max_length',
        truncation=True,
        max_length=512
    )

# Create HuggingFace Dataset
dataset = Dataset.from_dict({"sentence": sentences, "labels": list(encoded_labels)})

# Tokenize dataset
tokenized_dataset = dataset.map(tokenize_sentences, batched=True)
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Split dataset into train and validation sets
train_test_split = tokenized_dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
val_dataset = train_test_split['test']

# Custom Trainer Class with BCEWithLogitsLoss
class CustomTrainer(Trainer):
    def __init__(self, loss_fn, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_fn = loss_fn  # Loss function passed during initialization

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop('labels')
        outputs = model(**inputs)
        logits = outputs.logits

        # Compute loss using BCEWithLogitsLoss
        loss = self.loss_fn(logits, labels.float())

        return (loss, outputs) if return_outputs else loss

# Evaluation Metrics Function
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions

    # Apply sigmoid to logits to get probabilities
    preds = torch.sigmoid(torch.tensor(preds)).numpy()
    preds = (preds > 0.5).astype(int)

    # Calculate metrics
    f1 = f1_score(labels, preds, average='micro')
    precision = precision_score(labels, preds, average='micro')
    recall = recall_score(labels, preds, average='micro')

    return {
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Optuna Objective Function
def objective(trial):
    # Hyperparameter space
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-4)
    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])
    num_train_epochs = trial.suggest_int('num_train_epochs', 2, 5)
    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)
    warmup_steps = trial.suggest_int('warmup_steps', 0, 500)

    # Define the model
    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)

    # Define the loss function
    loss_fn = nn.BCEWithLogitsLoss()

    # Training arguments
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        warmup_steps=warmup_steps,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        logging_dir='./logs',
        evaluation_strategy="epoch",
        save_strategy="no",
        disable_tqdm=True,
        logging_steps=50,
    )

    # Initialize custom trainer
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        loss_fn=loss_fn,
    )

    # Train the model
    trainer.train()

    # Evaluate the model
    eval_results = trainer.evaluate()

    # Set the objective to maximize F1 score
    f1 = eval_results['eval_f1']
    trial.report(f1, step=0)

    # Handle pruning (optional)
    if trial.should_prune():
        raise optuna.exceptions.TrialPruned()

    return f1

# Create an Optuna study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=10)

# Best trial
print("Best trial:")
trial = study.best_trial

print(f"  F1 Score: {trial.value}")
print("  Params:")
for key, value in trial.params.items():
    print(f"    {key}: {value}")

# Train the final model with the best hyperparameters
best_params = trial.params

# Define the model
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)

# Define the loss function
loss_fn = nn.BCEWithLogitsLoss()

# Training arguments with best hyperparameters
training_args = TrainingArguments(
    output_dir='./best_model',
    num_train_epochs=best_params['num_train_epochs'],
    per_device_train_batch_size=best_params['batch_size'],
    per_device_eval_batch_size=best_params['batch_size'],
    warmup_steps=best_params['warmup_steps'],
    learning_rate=best_params['learning_rate'],
    weight_decay=best_params['weight_decay'],
    logging_dir='./logs',
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_steps=50,
)

# Initialize custom trainer with best hyperparameters
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    loss_fn=loss_fn,
)

# Train the final model
trainer.train()

# Save the best model
trainer.save_model('./best_model')

# Example prediction function
def predict(sentence):
    inputs = tokenizer(
        sentence,
        return_tensors="pt",
        padding='max_length',
        truncation=True,
        max_length=512
    )
    inputs = {key: value.to(trainer.model.device) for key, value in inputs.items()}

    with torch.no_grad():
        logits = model(**inputs).logits

    # Apply sigmoid to get probabilities
    probs = torch.sigmoid(logits).cpu().numpy()

    # Convert probabilities to binary predictions
    preds = (probs > 0.5).astype(int)

    # Map binary predictions to class names
    procedures = mlb.inverse_transform(preds)[0]

    return {
        "sentence": sentence,
        "predicted_procedures": list(procedures)
    }

# Example usage
example_sentence = "Implement network access control."
prediction = predict(example_sentence)
print(prediction)


































import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import MultiLabelBinarizer
from datasets import Dataset
from sklearn.metrics import f1_score, precision_score, recall_score
import optuna

# Define dataset
def load_data(filtered_df):
    sentences = filtered_df['extended_text'].tolist()
    labels_procedures = filtered_df['procedure'].tolist()

    # Convert lists of strings to list of lists
    labels_procedures = [eval(label) if isinstance(label, str) else label for label in labels_procedures]

    # MultiLabelBinarizer to one-hot encode the procedure labels
    mlb = MultiLabelBinarizer()
    encoded_labels = mlb.fit_transform(labels_procedures)

    # Create a dataset for transformers
    data = {'sentence': sentences, 'labels': encoded_labels.tolist()}
    dataset = Dataset.from_dict(data)

    return dataset, mlb

# Define the metric function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = (logits > 0.5).astype(int)
    precision = precision_score(labels, predictions, average='micro')
    recall = recall_score(labels, predictions, average='micro')
    f1 = f1_score(labels, predictions, average='micro')
    return {"precision": precision, "recall": recall, "f1": f1}

# Define the model init function for the Trainer
def model_init():
    return RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_procedures)

# Tokenizer and padding function
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

def tokenize_function(examples):
    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=512)

# Load and tokenize the dataset
filtered_df = # Your filtered DataFrame here
dataset, mlb = load_data(filtered_df)
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Split into train and validation
train_test_split = tokenized_dataset.train_test_split(test_size=0.1)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

# Define hyperparameter search space
def hp_space(trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True),
        "num_train_epochs": trial.suggest_int("num_train_epochs", 3, 10),
        "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [8, 16, 32]),
        "weight_decay": trial.suggest_float("weight_decay", 0.01, 0.1, log=True),
    }

# Define the objective function for optuna
def objective(trial):
    # Define training arguments
    training_args = TrainingArguments(
        output_dir='./results',
        evaluation_strategy='epoch',
        save_strategy="epoch",
        learning_rate=trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True),
        num_train_epochs=trial.suggest_int("num_train_epochs", 3, 10),
        per_device_train_batch_size=trial.suggest_categorical("per_device_train_batch_size", [8, 16, 32]),
        weight_decay=trial.suggest_float("weight_decay", 0.01, 0.1, log=True),
        logging_dir='./logs',
        logging_steps=10,
        metric_for_best_model="f1",
        load_best_model_at_end=True,
    )

    # Initialize Trainer
    trainer = Trainer(
        model_init=model_init,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    # Train the model
    trainer.train()

    # Evaluate on validation set
    eval_result = trainer.evaluate(eval_dataset=eval_dataset)
    return eval_result['eval_f1']

# Optuna hyperparameter search
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

# Best hyperparameters
print("Best hyperparameters:", study.best_params)

# Train with the best hyperparameters
best_trial = study.best_trial
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    save_strategy="epoch",
    learning_rate=best_trial.params['learning_rate'],
    num_train_epochs=best_trial.params['num_train_epochs'],
    per_device_train_batch_size=best_trial.params['per_device_train_batch_size'],
    weight_decay=best_trial.params['weight_decay'],
    logging_dir='./logs',
    logging_steps=10,
    metric_for_best_model="f1",
    load_best_model_at_end=True,
)

# Initialize final Trainer
trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Final training with the best hyperparameters
trainer.train()

# Evaluate final model
final_eval = trainer.evaluate(eval_dataset=eval_dataset)
print(f"Final evaluation: {final_eval}")















import torch
import numpy as np
from datasets import Dataset
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
import torch.nn as nn

# Assuming 'filtered_df' contains the sentence and procedure columns
sentences = filtered_df['extended_text'].tolist()
labels_procedures = filtered_df['procedure'].tolist()

# Convert lists of strings to list of lists (if it's a string representation of lists, use eval)
labels_procedures = [eval(label) if isinstance(label, str) else label for label in labels_procedures]

# MultiLabelBinarizer to one-hot encode the procedure labels
mlb = MultiLabelBinarizer()
encoded_labels = mlb.fit_transform(labels_procedures)

# Tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenize sentences
def tokenize_sentences(examples):
    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=512)

# Convert to HuggingFace Dataset
dataset = Dataset.from_dict({"sentence": sentences, "labels": list(encoded_labels)})

# Tokenize dataset
tokenized_dataset = dataset.map(tokenize_sentences, batched=True)
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Split dataset into train and validation (80/20 split)
train_test_split = tokenized_dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
val_dataset = train_test_split['test']

# Custom Trainer Class with BCEWithLogitsLoss
class CustomTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with Logits Loss

    # Override compute_loss to include BCEWithLogitsLoss for multi-label classification
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop('labels')  # Labels
        outputs = model(**inputs)      # Forward pass to get logits
        logits = outputs.logits

        # Compute loss using BCEWithLogitsLoss
        loss = self.loss_fn(logits, labels.float())

        return (loss, outputs) if return_outputs else loss

# Model definition: Use RobertaForSequenceClassification for multi-label classification
num_labels = len(mlb.classes_)
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)

# Custom metrics for evaluation
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions
    
    # Convert logits to probabilities and apply threshold
    preds = torch.sigmoid(torch.tensor(preds)).numpy()
    preds = (preds > 0.5).astype(int)

    # Calculate F1 Score, Precision, Recall
    f1 = f1_score(labels, preds, average='micro')
    precision = precision_score(labels, preds, average='micro')
    recall = recall_score(labels, preds, average='micro')
    
    # Confusion matrix (optional for multi-label classification)
    conf_matrix = confusion_matrix(labels.argmax(axis=1), preds.argmax(axis=1))
    
    return {
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'confusion_matrix': conf_matrix
    }

# Define Training Arguments
training_args = TrainingArguments(
    output_dir='./results',          # Output directory
    num_train_epochs=5,              # Number of training epochs
    per_device_train_batch_size=16,  # Batch size for training
    per_device_eval_batch_size=16,   # Batch size for evaluation
    warmup_steps=500,                # Number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # Strength of weight decay
    logging_dir='./logs',            # Directory for storing logs
    evaluation_strategy="epoch",     # Evaluate after every epoch
    save_strategy="epoch",           # Save model after each epoch
    load_best_model_at_end=True,     # Load best model at the end of training
    metric_for_best_model="f1",      # Metric to use to select the best model
    logging_steps=10,                # Log every 10 steps
)

# Initialize the CustomTrainer object with BCEWithLogitsLoss
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics  # Custom metrics function for evaluation
)

# Train the model
trainer.train()

# Evaluate the model on the validation dataset
eval_results = trainer.evaluate()

print(f"Evaluation Results: {eval_results}")

# Example prediction function
def predict(sentence):
    inputs = tokenizer(sentence, return_tensors="pt", padding='max_length', truncation=True, max_length=512)
    inputs = {key: value.to(trainer.model.device) for key, value in inputs.items()}

    with torch.no_grad():
        logits = model(**inputs).logits

    # Apply sigmoid to get probabilities
    probs = torch.sigmoid(logits).cpu().numpy()

    # Convert probabilities to binary predictions
    preds = (probs > 0.5).astype(int)

    # Map binary predictions to class names
    procedures = mlb.inverse_transform(preds)[0]

    return {
        "sentence": sentence,
        "predicted_procedures": list(procedures)
    }

# Example usage
example_sentence = "Implement network access control."
prediction = predict(example_sentence)
print(prediction)































#comet ML


import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from peft import LoraConfig, get_peft_model
import bitsandbytes as bnb
from sklearn.preprocessing import MultiLabelBinarizer
import time
from torch.cuda.amp import autocast, GradScaler
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from comet_ml import Experiment

# Tokenization and Preprocessing
def tokenize_sentences(sentences, tokenizer, max_len=256):
    print("Tokenizing sentences...")
    start_time = time.time()
    inputs = [tokenizer(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt") for sentence in sentences]
    input_ids = torch.cat([input['input_ids'] for input in inputs], dim=0)
    print(f"Tokenization completed in {time.time() - start_time:.2f} seconds.")
    return input_ids

# Data Preparation
def prepare_data(sentences, labels_procedures, tokenizer):
    input_ids = tokenize_sentences(sentences, tokenizer)
    train_dataset = TensorDataset(input_ids, labels_procedures)
    return train_dataset

# Custom Model with Multi-Label Classification Head
class MistralMultiLabelClassifier(nn.Module):
    def __init__(self, num_labels):
        super(MistralMultiLabelClassifier, self).__init__()
        self.mistral = AutoModel.from_pretrained(
            "mistralai/Mistral-7B",
            load_in_8bit=True,  # Quantize with bitsandbytes
            device_map='auto',
            quantization_config=bnb.QuantizationConfig(load_in_8bit=True)
        )
        self.classifier = nn.Linear(self.mistral.config.hidden_size, num_labels)  # Custom linear layer for multi-label classification

    def forward(self, input_ids):
        outputs = self.mistral(input_ids=input_ids)
        pooled_output = outputs.pooler_output  # Use the pooled output from the model
        logits = self.classifier(pooled_output)
        return logits

# Function to train and evaluate the model
def run_experiment(hyperparams, train_loader, val_loader, num_labels, experiment):
    # Log hyperparameters
    experiment.log_parameters(hyperparams)
    
    print(f"Starting experiment with hyperparameters: {hyperparams}")
    start_time = time.time()
    
    # Model setup
    model = MistralMultiLabelClassifier(num_labels=num_labels)
    
    # Apply LoRA configuration
    lora_config = LoraConfig(
        r=hyperparams['lora_rank'],  
        lora_alpha=16,
        target_modules=["query", "key", "value"],
        lora_dropout=hyperparams['lora_dropout'],
        bias="none"
    )
    model = get_peft_model(model, lora_config)
    
    optimizer = AdamW(model.parameters(), lr=hyperparams['learning_rate'])
    num_training_steps = len(train_loader) * hyperparams['epochs']
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)
    loss_fn = nn.BCEWithLogitsLoss()
    scaler = GradScaler()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    # Log model architecture
    experiment.set_model_graph(str(model))

    # Training Loop
    model.train()
    total_train_time = 0
    total_loss = 0
    for epoch in range(hyperparams['epochs']):
        print(f"Starting Epoch {epoch + 1}/{hyperparams['epochs']}...")
        epoch_start_time = time.time()
        total_epoch_loss = 0
        for batch_idx, batch in enumerate(train_loader):
            batch_input_ids, batch_labels = [x.to(device) for x in batch]

            optimizer.zero_grad()

            # Mixed precision forward pass
            with autocast():
                logits = model(batch_input_ids)
                loss = loss_fn(logits, batch_labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            total_epoch_loss += loss.item()
        
        avg_epoch_loss = total_epoch_loss / len(train_loader)
        total_loss += avg_epoch_loss
        epoch_duration = time.time() - epoch_start_time
        total_train_time += epoch_duration
        print(f"Epoch {epoch+1}/{hyperparams['epochs']} completed in {epoch_duration:.2f} seconds. Average Loss: {avg_epoch_loss:.4f}")
        
        # Log metrics to Comet ML
        experiment.log_metric("train_loss", avg_epoch_loss, step=epoch)
        experiment.log_metric("epoch_duration", epoch_duration, step=epoch)
    
    avg_loss = total_loss / hyperparams['epochs']
    print(f"Training completed in {total_train_time:.2f} seconds. Average Loss across all epochs: {avg_loss:.4f}")

    # Evaluate the model
    model.eval()
    total_eval_loss = 0
    true_labels = []
    predictions = []
    with torch.no_grad():
        for batch in val_loader:
            batch_input_ids, batch_labels = [x.to(device) for x in batch]
            with autocast():
                logits = model(batch_input_ids)
            loss = loss_fn(logits, batch_labels)
            total_eval_loss += loss.item()

            # Sigmoid and threshold for predictions
            probs = torch.sigmoid(logits)
            preds = (probs > 0.5).float().cpu().numpy()
            predictions.extend(preds)
            true_labels.extend(batch_labels.cpu().numpy())

    avg_eval_loss = total_eval_loss / len(val_loader)
    accuracy = accuracy_score(true_labels, predictions)
    print(f"Evaluation Loss: {avg_eval_loss:.4f}, Accuracy: {accuracy:.4f}")
    print(f"Experiment completed in {time.time() - start_time:.2f} seconds.\n")
    
    # Log final metrics to Comet ML
    experiment.log_metric("final_train_loss", avg_loss)
    experiment.log_metric("eval_loss", avg_eval_loss)
    experiment.log_metric("accuracy", accuracy)
    experiment.log_metric("total_train_time", total_train_time)

    # Log confusion matrix
    cm = confusion_matrix(true_labels, predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    experiment.log_figure(figure_name="confusion_matrix", figure=plt)
    plt.close()
    
    return avg_loss, avg_eval_loss, accuracy

# Hyperparameter sets
experiments = [
    {'learning_rate': 3e-4, 'batch_size': 8, 'lora_rank': 8, 'epochs': 3, 'lora_dropout': 0.1},
    {'learning_rate': 1e-4, 'batch_size': 8, 'lora_rank': 8, 'epochs': 5, 'lora_dropout': 0.1},
    {'learning_rate': 3e-4, 'batch_size': 16, 'lora_rank': 16, 'epochs': 3, 'lora_dropout': 0.1},
    {'learning_rate': 3e-4, 'batch_size': 8, 'lora_rank': 8, 'epochs': 3, 'lora_dropout': 0.3},
    {'learning_rate': 5e-4, 'batch_size': 8, 'lora_rank': 8, 'epochs': 3, 'lora_dropout': 0.1}
]

# Example data (replace with your actual data)
sentences = filtered_df['extended_text'].tolist()
labels_procedures = filtered_df['procedure'].tolist()
labels_procedures = [eval(label) if isinstance(label, str) else label for label in labels_procedures]
mlb = MultiLabelBinarizer()
encoded_labels = mlb.fit_transform(labels_procedures)
labels_procedures = torch.tensor(encoded_labels, dtype=torch.float32)

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B")

# Tokenize sentences
train_dataset = prepare_data(sentences, labels_procedures, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)

# Validation dataset (you should split part of your data for validation)
val_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)

# Run experiments
results = []
for hyperparams in experiments:
    # Create a new Comet ML experiment for each hyperparameter set
    with Experiment(
        api_key="YOUR_API_KEY",
        project_name="your-project-name",
        workspace="your-workspace"
    ) as experiment:
        experiment.set_name(f"Experiment_{len(results) + 1}")
        
        # Log dataset details
        experiment.log_parameter("num_samples", len(train_dataset))
        experiment.log_parameter("num_labels", len(mlb.classes_))
        
        result = run_experiment(hyperparams, train_loader, val_loader, num_labels=len(mlb.classes_), experiment=experiment)
        results.append((hyperparams, result))

# Print results
for i, (hyperparams, (train_loss, eval_loss, accuracy)) in enumerate(results):
    print(f"Experiment {i+1}:")
    print(f"Hyperparameters: {hyperparams}")
    print(f"Train Loss: {train_loss:.4f}")
    print(f"Eval Loss: {eval_loss:.4f}")
    print(f"Accuracy: {accuracy:.4f}")
    print()
































pip install transformers bitsandbytes


pip install bitsandbytes-cuda117  # CUDA 11.7



from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Path to the local directory where Mistral 7B is stored
model_path = "/path/to/local/mistral-7b"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load the model in FP16 precision with 8-bit quantization
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    torch_dtype=torch.float16,  # Enable FP16 precision
    load_in_8bit=True,          # Enable 8-bit quantization
    device_map="auto"           # Automatically load model onto available GPUs
)

# Ensure the model is loaded onto the GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Example usage: Tokenize a sample input and run the model
input_text = "Once upon a time, there was a mysterious AI model."
inputs = tokenizer(input_text, return_tensors="pt").to(device)

# Generate output
with torch.no_grad():
    outputs = model.generate(**inputs, max_length=50)

# Decode the generated output
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(generated_text)





















import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from peft import LoraConfig, get_peft_model
import bitsandbytes as bnb
from sklearn.preprocessing import MultiLabelBinarizer
import time
from torch.cuda.amp import autocast, GradScaler
from sklearn.metrics import accuracy_score

# Tokenization and Preprocessing
def tokenize_sentences(sentences, tokenizer, max_len=256):
    print("Tokenizing sentences...")
    start_time = time.time()
    inputs = [tokenizer(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt") for sentence in sentences]
    input_ids = torch.cat([input['input_ids'] for input in inputs], dim=0)
    print(f"Tokenization completed in {time.time() - start_time:.2f} seconds.")
    return input_ids

# Data Preparation
def prepare_data(sentences, labels_procedures, tokenizer):
    input_ids = tokenize_sentences(sentences, tokenizer)
    train_dataset = TensorDataset(input_ids, labels_procedures)
    return train_dataset

# Custom Model with Multi-Label Classification Head
class MistralMultiLabelClassifier(nn.Module):
    def __init__(self, num_labels):
        super(MistralMultiLabelClassifier, self).__init__()
        self.mistral = AutoModel.from_pretrained(
            "mistralai/Mistral-7B",
            load_in_8bit=True,  # Quantize with bitsandbytes
            device_map='auto',
            quantization_config=bnb.QuantizationConfig(load_in_8bit=True)
        )
        self.classifier = nn.Linear(self.mistral.config.hidden_size, num_labels)  # Custom linear layer for multi-label classification

    def forward(self, input_ids):
        outputs = self.mistral(input_ids=input_ids)
        pooled_output = outputs.pooler_output  # Use the pooled output from the model
        logits = self.classifier(pooled_output)
        return logits

# Function to train and evaluate the model
def run_experiment(hyperparams, train_loader, val_loader, num_labels):
    print(f"Starting experiment with hyperparameters: {hyperparams}")
    start_time = time.time()
    
    # Model setup
    model = MistralMultiLabelClassifier(num_labels=num_labels)
    
    # Apply LoRA configuration
    lora_config = LoraConfig(
        r=hyperparams['lora_rank'],  
        lora_alpha=16,
        target_modules=["query", "key", "value"],
        lora_dropout=hyperparams['lora_dropout'],
        bias="none"
    )
    model = get_peft_model(model, lora_config)
    
    optimizer = AdamW(model.parameters(), lr=hyperparams['learning_rate'])
    num_training_steps = len(train_loader) * hyperparams['epochs']
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)
    loss_fn = nn.BCEWithLogitsLoss()
    scaler = GradScaler()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    # Training Loop
    model.train()
    total_train_time = 0
    total_loss = 0
    for epoch in range(hyperparams['epochs']):
        print(f"Starting Epoch {epoch + 1}/{hyperparams['epochs']}...")
        epoch_start_time = time.time()
        total_epoch_loss = 0
        for batch_idx, batch in enumerate(train_loader):
            batch_input_ids, batch_labels = [x.to(device) for x in batch]

            optimizer.zero_grad()

            # Mixed precision forward pass
            with autocast():
                logits = model(batch_input_ids)
                loss = loss_fn(logits, batch_labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            total_epoch_loss += loss.item()
        
        avg_epoch_loss = total_epoch_loss / len(train_loader)
        total_loss += avg_epoch_loss
        epoch_duration = time.time() - epoch_start_time
        total_train_time += epoch_duration
        print(f"Epoch {epoch+1}/{hyperparams['epochs']} completed in {epoch_duration:.2f} seconds. Average Loss: {avg_epoch_loss:.4f}")
    
    avg_loss = total_loss / hyperparams['epochs']
    print(f"Training completed in {total_train_time:.2f} seconds. Average Loss across all epochs: {avg_loss:.4f}")

    # Evaluate the model
    model.eval()
    total_eval_loss = 0
    true_labels = []
    predictions = []
    with torch.no_grad():
        for batch in val_loader:
            batch_input_ids, batch_labels = [x.to(device) for x in batch]
            with autocast():
                logits = model(batch_input_ids)
            loss = loss_fn(logits, batch_labels)
            total_eval_loss += loss.item()

            # Sigmoid and threshold for predictions
            probs = torch.sigmoid(logits)
            preds = (probs > 0.5).float().cpu().numpy()
            predictions.extend(preds)
            true_labels.extend(batch_labels.cpu().numpy())

    avg_eval_loss = total_eval_loss / len(val_loader)
    accuracy = accuracy_score(true_labels, predictions)
    print(f"Evaluation Loss: {avg_eval_loss:.4f}, Accuracy: {accuracy:.4f}")
    print(f"Experiment completed in {time.time() - start_time:.2f} seconds.\n")
    return avg_loss, avg_eval_loss, accuracy

# Hyperparameter sets
experiments = [
    {'learning_rate': 3e-4, 'batch_size': 8, 'lora_rank': 8, 'epochs': 3, 'lora_dropout': 0.1},
    {'learning_rate': 1e-4, 'batch_size': 8, 'lora_rank': 8, 'epochs': 5, 'lora_dropout': 0.1},
    {'learning_rate': 3e-4, 'batch_size': 16, 'lora_rank': 16, 'epochs': 3, 'lora_dropout': 0.1},
    {'learning_rate': 3e-4, 'batch_size': 8, 'lora_rank': 8, 'epochs': 3, 'lora_dropout': 0.3},
    {'learning_rate': 5e-4, 'batch_size': 8, 'lora_rank': 8, 'epochs': 3, 'lora_dropout': 0.1}
]

# Example data
sentences = filtered_df['extended_text'].tolist()
labels_procedures = filtered_df['procedure'].tolist()
labels_procedures = [eval(label) if isinstance(label, str) else label for label in labels_procedures]
mlb = MultiLabelBinarizer()
encoded_labels = mlb.fit_transform(labels_procedures)
labels_procedures = torch.tensor(encoded_labels, dtype=torch.float32)

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B")

# Tokenize sentences
train_dataset = prepare_data(sentences, labels_procedures, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)

# Validation dataset (you should split part of your data for validation)
val_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)

# Run experiments
results = []
for hyperparams in experiments:
    result = run_experiment(hyperparams, train_loader, val_loader, num_labels=len(mlb.classes_))
    results.append((hyperparams, result))

















































pip install torch sentence-transformers scikit-learn cohere


pip install chromadb

pip install langchain PyPDF2 ray fitz PyMuPDF


import chromadb
import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import uuid
import json
import pandas as pd

# Initialize ChromaDB client
client = chromadb.Client()

# Create or load a collection in ChromaDB
collection_name = "knowledge_base"
collection = client.get_or_create_collection(name=collection_name)


# Initialize Sentence Embedding Model (GTE-Large or any Sentence Transformer model)
embed_model = SentenceTransformer('thenlper/gte-large')

# Sample knowledge base data: Replace this with your actual data
knowledge_base = [
    {"sentence": "What is the capital of France?", "domain": "Geography", "standard": "General Knowledge", "objective": "Capital City", "procedure": "Find Capital"},
    {"sentence": "Describe photosynthesis.", "domain": "Biology", "standard": "Botany", "objective": "Plant Process", "procedure": "Explain Photosynthesis"},
    {"sentence": "How does gravity work?", "domain": "Physics", "standard": "Classical Mechanics", "objective": "Fundamental Force", "procedure": "Explain Gravity"},
]

# Generate Embeddings and Store in ChromaDB
for item in knowledge_base:
    sentence = item["sentence"]

    # Generate embedding for the sentence
    embedding = embed_model.encode(sentence).tolist()

    # Add embedding to ChromaDB along with metadata
    collection.add(
        ids=[str(uuid.uuid4())],
        documents=[sentence],
        embeddings=[embedding],
        metadatas=[{
            "domain": item["domain"],
            "standard": item["standard"],
            "objective": item["objective"],
            "procedure": item["procedure"]
        }]
    )

print("Data stored in ChromaDB.")







df_data = pd.read_json('/content/data/sentences_input.json')
print(df_data.columns)
print(df_data["data"]["sentences"])

list_sentences = df_data["data"]["sentences"]

#print(list_sentences)

df = pd.DataFrame(list_sentences)
#print(df)

try:
    # Ensure the DataFrame exists and has the required columns
    if 'df' not in globals():
        raise NameError("DataFrame 'df' is not defined")
    
    required_columns = ['sentence', 'domain', 'standard', 'objective', 'procedure']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"Missing required columns in DataFrame: {', '.join(missing_columns)}")

    # Generate Embeddings and Store in ChromaDB
    for _, item in df.iterrows():
        sentence = item.get('sentence')

        if pd.isna(sentence):
            print(f"Warning: Skipping item due to missing 'sentence' field: {item}")
            continue

        # Generate embedding for the sentence
        embedding = embed_model.encode(sentence).tolist()

        # Add embedding to ChromaDB along with metadata
        collection.add(
            ids=[str(uuid.uuid4())],
            documents=[sentence],
            embeddings=[embedding],
            metadatas=[{
                "domain": item.get('domain', ''),
                "standard": item.get('standard', ''),
                "objective": item.get('objective', ''),
                "procedure": item.get('procedure', '')
            }]
        )

    print("Data stored in ChromaDB.")

except NameError as e:
    print(f"Error: {str(e)}")
except ValueError as e:
    print(f"Error: {str(e)}")
except Exception as e:
    print(f"An unexpected error occurred: {str(e)}")
	
	
	
	
	





import fitz  # PyMuPDF
import os
import re

# Define keywords associated with each category
category_keywords = {
    'Control Domain': ['domain', 'control domain', 'cyber defense', 'security framework'],
    'Control Objective': ['objective', 'control objective', 'aim', 'purpose'],
    'Standard': ['standard', 'compliance', 'regulation', 'policy'],
    'Procedure': ['procedure', 'process', 'action', 'step', 'method']
}

# Function to extract text from a single PDF file
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    
    # Iterate through all pages
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text += page.get_text("text")
    
    doc.close()
    return text

# Function to classify sentences based on category keywords
def classify_sentence(sentence):
    sentence_lower = sentence.lower()
    
    for category, keywords in category_keywords.items():
        for keyword in keywords:
            if keyword in sentence_lower:
                return category
    
    return None  # Return None if no category is matched

# Function to extract valid sentences and classify them
def extract_valid_sentences(text):
    sentences = re.split(r'(?<=[.!?]) +', text)  # Split the text into sentences
    classified_sentences = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > 10:  # Ensure sentence is of a reasonable length
            category = classify_sentence(sentence)
            if category:
                classified_sentences.append((category, sentence))
    
    return classified_sentences

# Function to process multiple PDFs in a directory
def process_pdfs_in_directory(directory_path):
    extracted_data = []
    
    for filename in os.listdir(directory_path):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(directory_path, filename)
            print(f"Processing file: {pdf_path}")
            
            # Extract text from the PDF
            text = extract_text_from_pdf(pdf_path)
            
            # Extract valid sentences and classify them
            classified_sentences = extract_valid_sentences(text)
            
            # Append the extracted sentences with the source (filename)
            for category, sentence in classified_sentences:
                extracted_data.append({
                    'source': filename,
                    'category': category,
                    'sentence': sentence
                })
    
    return extracted_data

# Directory path containing the PDF files
pdf_directory = '/content/data/'

# Extract data from all PDFs in the directory
extracted_info = process_pdfs_in_directory(pdf_directory)

# Output the extracted information
for entry in extracted_info:
    print(f"Source: {entry['source']}\nCategory: {entry['category']}\nSentence: {entry['sentence']}\n---\n")
	
	










import uuid
import chromadb
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path
import ray
import ray.data
import shutil
from PyPDF2 import PdfReader

# Initialize the Sentence Embedding Model (GTE-Large)
embed_model = SentenceTransformer('thenlper/gte-large')

# Initialize ChromaDB client and load the knowledge base
client = chromadb.Client()
collection_name = "knowledge_base"
collection = client.get_collection(name=collection_name)  # Assume the collection is already created

# PDF reading and chunking
#DATA_DIR = Path("/content/data")
#shutil.copytree(Path("./"), DATA_DIR, dirs_exist_ok=True)

pdf_path = Path("/content/data/CELEX_32019R0881_EN_TXT.pdf")  # Path to your PDF file

# Read the PDF using PyPDF2
reader = PdfReader(str(pdf_path))
pdf_text = ""
for page in reader.pages:
    pdf_text += page.extract_text()

# Chunk the text using RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Adjust chunk size if needed
    chunk_overlap=50   # Small overlap between chunks for context
)

chunks = splitter.split_text(pdf_text)

# We'll use the first 100 chunks for embeddings and queries
query_chunks = chunks[:50]

print(extracted_info)

query_embeddings = [embed_model.encode(chunk['sentence']).tolist() for chunk in extracted_info[:50]]


# Generate embeddings for the chunks
query_embeddings = [embed_model.encode(chunk).tolist() for chunk in query_chunks]

# Query the knowledge base using the embeddings
def query_knowledge_base(embeddings):
    results = collection.query(
        query_embeddings=embeddings,  # Query embeddings
        n_results=3,  # Number of similar results to return per embedding
        include=["metadatas", "documents", "distances"]
    )

    mappings = []

    # Calculate similarity and prepare mappings
    for i, result in enumerate(results['documents']):
        for j, document in enumerate(result):
            metadata = results['metadatas'][i][j]
            distance = results['distances'][i][j]
            similarity = 1 - distance  # Convert distance to similarity

            mappings.append({
                "query_chunk": query_chunks[i],  # Original chunk that was queried
                "matched_sentence": document,
                "domain": metadata["domain"],
                "standard": metadata["standard"],
                "objective": metadata["objective"],
                "procedure": metadata["procedure"],
                "similarity": similarity
            })

    return mappings

# Perform the query on the first 100 embeddings
results = query_knowledge_base(query_embeddings)

# Display the results (for first 5 query mappings)
for mapping in results[:5]:
    print(f"Query Chunk: {mapping['query_chunk'][:100]}...")  # Truncate to show first 100 chars
    print(f"Matched Sentence: {mapping['matched_sentence']}")
    print(f"Domain: {mapping['domain']}, Standard: {mapping['standard']}")
    print(f"Objective: {mapping['objective']}, Procedure: {mapping['procedure']}")
    print(f"Similarity: {mapping['similarity']:.4f}")
    print()
	
	







import uuid
import chromadb
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path
import shutil
from PyPDF2 import PdfReader
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Initialize the Sentence Embedding Model (GTE-Large)
embed_model = SentenceTransformer('thenlper/gte-large')

# Initialize ChromaDB client and load the knowledge base
client = chromadb.Client()
collection_name = "knowledge_base"
collection = client.get_collection(name=collection_name)  # Assume the collection is already created

# PDF reading and chunking
#DATA_DIR = Path("/content/data")
#shutil.copytree(Path("./"), DATA_DIR, dirs_exist_ok=True)

pdf_path = Path("/content/data/CELEX_32019R0881_EN_TXT.pdf")  # Path to your PDF file

# Read the PDF using PyPDF2
reader = PdfReader(str(pdf_path))
pdf_text = ""
for page in reader.pages:
    pdf_text += page.extract_text()

# Chunk the text using RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Adjust chunk size if needed
    chunk_overlap=50   # Small overlap between chunks for context
)

chunks = splitter.split_text(pdf_text)

# We'll use the first 100 chunks for embeddings and queries
query_chunks = chunks[:100]

print(extracted_info)

query_embeddings = [embed_model.encode(chunk['sentence']).tolist() for chunk in extracted_info[:120]]
# Generate embeddings for the chunks
#query_embeddings = [embed_model.encode(chunk).tolist() for chunk in query_chunks]

# Convert query_embeddings to a numpy array for cosine similarity calculation
query_embeddings_np = np.array(query_embeddings)

# Function to query the knowledge base and use cosine similarity
def query_knowledge_base(query_embeddings_np):
    # Retrieve all stored embeddings and metadata from ChromaDB
    stored_data = collection.get(include=["embeddings", "metadatas", "documents"])

    stored_embeddings = np.array(stored_data['embeddings'])  # Extract stored embeddings
    stored_metadatas = stored_data['metadatas']  # Extract metadata
    stored_documents = stored_data['documents']  # Extract stored documents

    # Compute cosine similarity between query embeddings and stored embeddings
    cosine_sim_matrix = cosine_similarity(query_embeddings_np, stored_embeddings)

    mappings = []

    # Process each query embedding and its cosine similarities to stored embeddings
    for i, query_chunk in enumerate(query_chunks):
        # Get the top 3 most similar stored embeddings
        top_indices = np.argsort(cosine_sim_matrix[i])[::-1][:3]  # Sort in descending order

        for idx in top_indices:
            similarity = cosine_sim_matrix[i][idx]
            metadata = stored_metadatas[idx]

            mappings.append({
                "query_chunk": query_chunk,  # Original chunk that was queried
                "matched_sentence": stored_documents[idx],
                "domain": metadata["domain"],
                "standard": metadata["standard"],
                "objective": metadata["objective"],
                "procedure": metadata["procedure"],
                "similarity": similarity
            })

    return mappings

# Perform the query on the first 100 embeddings
results = query_knowledge_base(query_embeddings_np)

# Display the results (for first 5 query mappings)
for mapping in results[:5]:
    print(f"Query Chunk: {mapping['query_chunk'][:100]}...")  # Truncate to show first 100 chars
    print(f"Matched Sentence: {mapping['matched_sentence']}")
    print(f"Domain: {mapping['domain']}, Standard: {mapping['standard']}")
    print(f"Objective: {mapping['objective']}, Procedure: {mapping['procedure']}")
    print(f"Similarity: {mapping['similarity']:.4f}")
    print()
























































pip install pymupdf


import fitz  # PyMuPDF
import os
import re

# Function to extract text from a single PDF file
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    
    # Iterate through all pages
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text += page.get_text("text")
    
    doc.close()
    return text

# Function to extract sections and sentences from the text
def extract_sections_and_sentences(text):
    # Regular expression to identify section headings (Assuming they're in ALL CAPS or followed by a number or bullet points)
    section_pattern = re.compile(r'^[A-Z][A-Z0-9\s,.\-]+(?=\n)', re.MULTILINE)
    
    sections = section_pattern.findall(text)
    sentences = re.split(r'(?<=[.!?]) +', text)  # Split the text into sentences
    description_sentences = []
    
    for sentence in sentences:
        # Check if sentence is part of a section (by looking at surrounding text)
        for section in sections:
            if section in sentence:
                description_sentences.append((section.strip(), sentence.strip()))
                break
    
    return description_sentences

# Function to process multiple PDFs in a directory
def process_pdfs_in_directory(directory_path):
    extracted_data = []
    
    for filename in os.listdir(directory_path):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(directory_path, filename)
            print(f"Processing file: {pdf_path}")
            
            # Extract text from the PDF
            text = extract_text_from_pdf(pdf_path)
            
            # Extract sections and sentences from the text
            extracted_sections = extract_sections_and_sentences(text)
            
            # Append the extracted sections and sentences with the source (filename)
            for section, sentence in extracted_sections:
                extracted_data.append({
                    'source': filename,
                    'section': section,
                    'sentence': sentence
                })
    
    return extracted_data

# Directory path containing the PDF files
pdf_directory = '/content/data'

# Extract data from all PDFs in the directory
extracted_info = process_pdfs_in_directory(pdf_directory)

# Output the extracted information
for entry in extracted_info:
    print(f"Source: {entry['source']}\nSection: {entry['section']}\nSentence: {entry['sentence']}\n---\n")




































Here’s the complete code for fine-tuning the GTE large embedding model and performing cosine similarity-based search using your dataset:

### 1. **Dataset Preparation and Sentence Pairing**
```python
import random

# Sample knowledge base data (Add more as needed)
knowledge_base = [
    {"sentence": "Regularly review access privileges to ensure they align with job roles.", "domain": "Access Control", "standard": "Access Reviews", "objective": "Least Privilege Enforcement", "procedure": "Periodic Access Review"},
    {"sentence": "Apply security patches as soon as they are released.", "domain": "Risk Management", "standard": "Patch Management", "objective": "Vulnerability Mitigation", "procedure": "Patch Deployment Process"},
    {"sentence": "Use strong encryption algorithms for all sensitive data.", "domain": "Encryption", "standard": "Data Protection", "objective": "Data Confidentiality", "procedure": "Encryption Policy Enforcement"},
    {"sentence": "Enable logging for all security-related events.", "domain": "Monitoring and Logging", "standard": "Audit Logging", "objective": "Security Event Tracking", "procedure": "Log Management Configuration"},
    {"sentence": "Implement least privilege access for all system users.", "domain": "Access Control", "standard": "Role-Based Access Control (RBAC)", "objective": "Access Restriction", "procedure": "RBAC Implementation"},
    # Add more sentences from your dataset (prepare 100+ more)
]

# Create sentence pairs (positive and negative pairs)
pairs = []
labels = []

for i, item1 in enumerate(knowledge_base):
    for j, item2 in enumerate(knowledge_base):
        if i != j:
            # Define similar pairs (same domain and objective)
            if item1["domain"] == item2["domain"] and item1["objective"] == item2["objective"]:
                pairs.append((item1["sentence"], item2["sentence"]))
                labels.append(1)  # Similar pair
            else:
                # Randomly select dissimilar pairs
                if random.random() > 0.8:
                    pairs.append((item1["sentence"], item2["sentence"]))
                    labels.append(0)  # Dissimilar pair
```

### 2. **Fine-Tune GTE Large Model**
You will fine-tune the GTE large model using `SentenceTransformer` and contrastive learning (`CosineSimilarityLoss`).

```python
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

# Load the pre-trained GTE large model
model = SentenceTransformer('thenlper/gte-large')

# Convert sentence pairs into InputExamples
train_examples = [InputExample(texts=[sent1, sent2], label=float(label)) for (sent1, sent2), label in zip(pairs, labels)]

# Create DataLoader for training
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

# Define the loss function (CosineSimilarityLoss)
train_loss = losses.CosineSimilarityLoss(model)

# Fine-tune the model
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=1,  # You can increase this for better results
    warmup_steps=100,
    output_path='./fine-tuned-gte-large'  # Save the fine-tuned model
)
```

### 3. **Generate Query Embeddings and Knowledge Base Embeddings**
After fine-tuning, generate embeddings for both the queries and knowledge base sentences, and use cosine similarity for retrieval.

```python
from sklearn.metrics.pairwise import cosine_similarity

# Reload the fine-tuned model
model = SentenceTransformer('./fine-tuned-gte-large')

# Generate embeddings for the knowledge base
knowledge_embeddings = [model.encode(item["sentence"]) for item in knowledge_base]

# Sample query sentences (you can load real queries here)
query_sentences = [
    "How can we manage access control?",
    "Explain encryption algorithms.",
    "What is the best practice for access reviews?"
]

# Generate embeddings for the query sentences
query_embeddings = model.encode(query_sentences)

# Compute cosine similarity between query embeddings and knowledge base embeddings
for query_embedding in query_embeddings:
    similarities = cosine_similarity([query_embedding], knowledge_embeddings)
    top_k = similarities[0].argsort()[-3:][::-1]  # Get top 3 similar sentences

    for idx in top_k:
        print(f"Query: {query_sentences}")
        print(f"Matched Sentence: {knowledge_base[idx]['sentence']}")
        print(f"Domain: {knowledge_base[idx]['domain']}, Objective: {knowledge_base[idx]['objective']}")
        print(f"Cosine Similarity: {similarities[0][idx]:.4f}")
        print()
```

### 4. **Evaluation**
After running the fine-tuning and query embedding comparison, you can evaluate the retrieval performance of your fine-tuned model. Here's a breakdown of the key steps:
1. **Dataset Preparation**: The dataset consists of sentence pairs from the knowledge base, labeled as similar or dissimilar.
2. **Model Fine-tuning**: The GTE large model is fine-tuned using the contrastive loss function (CosineSimilarityLoss).
3. **Embedding Generation**: Both the knowledge base sentences and the query sentences are embedded using the fine-tuned model.
4. **Cosine Similarity Search**: Cosine similarity is calculated between the query embeddings and the knowledge base embeddings to find the most relevant results.

### Summary
This setup allows you to fine-tune the GTE large model specifically for your use case, where you need to match queries with sentences from a knowledge base. By improving the quality of the embeddings through fine-tuning, the cosine similarity search becomes more effective for retrieving relevant information.

You can iterate on this by adjusting the training data, adding more sentences, and performing evaluations using metrics like NDCG or MRR to further improve the system.
















pip install torch sentence-transformers scikit-learn cohere

pip install chromadb

pip install langchain PyPDF2 ray

# Initialize ChromaDB client
client = chromadb.Client()

# Create or load a collection in ChromaDB
collection_name = "knowledge_base"
collection = client.get_or_create_collection(name=collection_name)


# Initialize Sentence Embedding Model (GTE-Large or any Sentence Transformer model)
embed_model = SentenceTransformer('thenlper/gte-large')

# Sample knowledge base data: Replace this with your actual data
knowledge_base = [
    {"sentence": "What is the capital of France?", "domain": "Geography", "standard": "General Knowledge", "objective": "Capital City", "procedure": "Find Capital"},
    {"sentence": "Describe photosynthesis.", "domain": "Biology", "standard": "Botany", "objective": "Plant Process", "procedure": "Explain Photosynthesis"},
    {"sentence": "How does gravity work?", "domain": "Physics", "standard": "Classical Mechanics", "objective": "Fundamental Force", "procedure": "Explain Gravity"},
]

# Generate Embeddings and Store in ChromaDB
for item in knowledge_base:
    sentence = item["sentence"]

    # Generate embedding for the sentence
    embedding = embed_model.encode(sentence).tolist()

    # Add embedding to ChromaDB along with metadata
    collection.add(
        ids=[str(uuid.uuid4())],
        documents=[sentence],
        embeddings=[embedding],
        metadatas=[{
            "domain": item["domain"],
            "standard": item["standard"],
            "objective": item["objective"],
            "procedure": item["procedure"]
        }]
    )

print("Data stored in ChromaDB.")










import chromadb
import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import uuid
import json

# Initialize ChromaDB client
client = chromadb.Client()

# Create or load a collection in ChromaDB
collection_name = "knowledge_base"
collection = client.get_or_create_collection(name=collection_name)

# Initialize Sentence Embedding Model (GTE-Large or any Sentence Transformer model)
embed_model = SentenceTransformer('thenlper/gte-large')

# Read data from sentences_input.json
try:
    with open('/content/data/sentences_input.json', 'r') as file:
        knowledge_base = json.load(file)
except FileNotFoundError:
    print("Error: sentences_input.json file not found.")
    exit(1)
except json.JSONDecodeError:
    print("Error: Invalid JSON format in sentences_input.json.")
    exit(1)

# Generate Embeddings and Store in ChromaDB
for item in knowledge_base["data"]["sentences"]:
    sentence = item.get("sentence")

    if not sentence:
        print(f"Warning: Skipping item due to missing 'sentence' field: {item}")
        continue

    # Generate embedding for the sentence
    embedding = embed_model.encode(sentence).tolist()

    # Add embedding to ChromaDB along with metadata
    collection.add(
        ids=[str(uuid.uuid4())],
        documents=[sentence],
        embeddings=[embedding],
        metadatas=[{
            "domain": item.get("domain", ""),
            "standard": item.get("standard", ""),
            "objective": item.get("objective", ""),
            "procedure": item.get("procedure", "")
        }]
    )

print("Data stored in ChromaDB.")



# Query the Knowledge Base with a new sentence
query_sentence = "Coordinate the event logging function with other organizational entities requiring audit-related information to guide and inform the selection criteria for events to be logged?"

# Generate embedding for the query
query_embedding = embed_model.encode(query_sentence).reshape(1, -1)

# Fetch all stored embeddings and metadata from ChromaDB
results = collection.get(include=["embeddings", "metadatas","documents"])
stored_embeddings = np.array(results['embeddings'])
stored_metadata = results['metadatas']

# Compute cosine similarity between query embedding and all stored sentence embeddings
similarities = cosine_similarity(query_embedding, stored_embeddings)

print(similarities)

# Find the top 3 most similar sentences based on cosine similarity
top_k = 3
top_indices = np.argsort(similarities[0])[-top_k:][::-1]  # Sort in descending order

#print(top_indices)
print(results['documents'])
print(stored_metadata)

# Print top similar results with their associated metadata
print("\nTop Similar Sentences:")
for idx in top_indices:
    print(f"Sentence: {results['documents'][idx]}")
    print(f"Cosine Similarity: {similarities[0][idx]:.4f}")
    print(f"Metadata: {stored_metadata[idx]}")
    print()
	
	
	
import pandas as pd
import chromadb
from sentence_transformers import SentenceTransformer
import uuid

# Initialize ChromaDB client
client = chromadb.Client()

# Create or load a collection in ChromaDB
collection_name = "knowledge_base"
collection = client.get_or_create_collection(name=collection_name)

# Initialize Sentence Embedding Model (GTE-Large or any Sentence Transformer model)
embed_model = SentenceTransformer('thenlper/gte-large')

# Read data from pandas DataFrame
# Assuming the DataFrame is already loaded or created elsewhere
# If not, you can load it like this:
# df = pd.read_csv('your_data.csv')  # or any other method to load your data

try:
    # Ensure the DataFrame exists and has the required columns
    if 'df' not in globals():
        raise NameError("DataFrame 'df' is not defined")
    
    required_columns = ['sentence', 'domain', 'standard', 'objective', 'procedure']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"Missing required columns in DataFrame: {', '.join(missing_columns)}")

    # Generate Embeddings and Store in ChromaDB
    for _, item in df.iterrows():
        sentence = item.get('sentence')

        if pd.isna(sentence):
            print(f"Warning: Skipping item due to missing 'sentence' field: {item}")
            continue

        # Generate embedding for the sentence
        embedding = embed_model.encode(sentence).tolist()

        # Add embedding to ChromaDB along with metadata
        collection.add(
            ids=[str(uuid.uuid4())],
            documents=[sentence],
            embeddings=[embedding],
            metadatas=[{
                "domain": item.get('domain', ''),
                "standard": item.get('standard', ''),
                "objective": item.get('objective', ''),
                "procedure": item.get('procedure', '')
            }]
        )

    print("Data stored in ChromaDB.")

except NameError as e:
    print(f"Error: {str(e)}")
except ValueError as e:
    print(f"Error: {str(e)}")
except Exception as e:
    print(f"An unexpected error occurred: {str(e)}")


	


import uuid
import chromadb
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path
import ray
import ray.data
import shutil
from PyPDF2 import PdfReader

# Initialize the Sentence Embedding Model (GTE-Large)
embed_model = SentenceTransformer('thenlper/gte-large')

# Initialize ChromaDB client and load the knowledge base
client = chromadb.Client()
collection_name = "knowledge_base"
collection = client.get_collection(name=collection_name)  # Assume the collection is already created

# PDF reading and chunking
#DATA_DIR = Path("/content/data")
#shutil.copytree(Path("./"), DATA_DIR, dirs_exist_ok=True)

pdf_path = Path("/content/data/CELEX_32019R0881_EN_TXT.pdf")  # Path to your PDF file

# Read the PDF using PyPDF2
reader = PdfReader(str(pdf_path))
pdf_text = ""
for page in reader.pages:
    pdf_text += page.extract_text()

# Chunk the text using RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Adjust chunk size if needed
    chunk_overlap=50   # Small overlap between chunks for context
)

chunks = splitter.split_text(pdf_text)

# We'll use the first 100 chunks for embeddings and queries
query_chunks = chunks[:50]

# Generate embeddings for the chunks
query_embeddings = [embed_model.encode(chunk).tolist() for chunk in query_chunks]

# Query the knowledge base using the embeddings
def query_knowledge_base(embeddings):
    results = collection.query(
        query_embeddings=embeddings,  # Query embeddings
        n_results=3,  # Number of similar results to return per embedding
        include=["metadatas", "documents", "distances"]
    )

    mappings = []

    # Calculate similarity and prepare mappings
    for i, result in enumerate(results['documents']):
        for j, document in enumerate(result):
            metadata = results['metadatas'][i][j]
            distance = results['distances'][i][j]
            similarity = 1 - distance  # Convert distance to similarity

            mappings.append({
                "query_chunk": query_chunks[i],  # Original chunk that was queried
                "matched_sentence": document,
                "domain": metadata["domain"],
                "standard": metadata["standard"],
                "objective": metadata["objective"],
                "procedure": metadata["procedure"],
                "similarity": similarity
            })

    return mappings

# Perform the query on the first 100 embeddings
results = query_knowledge_base(query_embeddings)

# Display the results (for first 5 query mappings)
for mapping in results[:5]:
    print(f"Query Chunk: {mapping['query_chunk'][:100]}...")  # Truncate to show first 100 chars
    print(f"Matched Sentence: {mapping['matched_sentence']}")
    print(f"Domain: {mapping['domain']}, Standard: {mapping['standard']}")
    print(f"Objective: {mapping['objective']}, Procedure: {mapping['procedure']}")
    print(f"Similarity: {mapping['similarity']:.4f}")
    print()



import uuid
import chromadb
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path
import shutil
from PyPDF2 import PdfReader
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Initialize the Sentence Embedding Model (GTE-Large)
embed_model = SentenceTransformer('thenlper/gte-large')

# Initialize ChromaDB client and load the knowledge base
client = chromadb.Client()
collection_name = "knowledge_base"
collection = client.get_collection(name=collection_name)  # Assume the collection is already created

# PDF reading and chunking
#DATA_DIR = Path("/content/data")
#shutil.copytree(Path("./"), DATA_DIR, dirs_exist_ok=True)

pdf_path = Path("/content/data/CELEX_32019R0881_EN_TXT.pdf")  # Path to your PDF file

# Read the PDF using PyPDF2
reader = PdfReader(str(pdf_path))
pdf_text = ""
for page in reader.pages:
    pdf_text += page.extract_text()

# Chunk the text using RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Adjust chunk size if needed
    chunk_overlap=50   # Small overlap between chunks for context
)

chunks = splitter.split_text(pdf_text)

# We'll use the first 100 chunks for embeddings and queries
query_chunks = chunks[:100]

# Generate embeddings for the chunks
query_embeddings = [embed_model.encode(chunk).tolist() for chunk in query_chunks]

# Convert query_embeddings to a numpy array for cosine similarity calculation
query_embeddings_np = np.array(query_embeddings)

# Function to query the knowledge base and use cosine similarity
def query_knowledge_base(query_embeddings_np):
    # Retrieve all stored embeddings and metadata from ChromaDB
    stored_data = collection.get(include=["embeddings", "metadatas", "documents"])

    stored_embeddings = np.array(stored_data['embeddings'])  # Extract stored embeddings
    stored_metadatas = stored_data['metadatas']  # Extract metadata
    stored_documents = stored_data['documents']  # Extract stored documents

    # Compute cosine similarity between query embeddings and stored embeddings
    cosine_sim_matrix = cosine_similarity(query_embeddings_np, stored_embeddings)

    mappings = []

    # Process each query embedding and its cosine similarities to stored embeddings
    for i, query_chunk in enumerate(query_chunks):
        # Get the top 3 most similar stored embeddings
        top_indices = np.argsort(cosine_sim_matrix[i])[::-1][:3]  # Sort in descending order

        for idx in top_indices:
            similarity = cosine_sim_matrix[i][idx]
            metadata = stored_metadatas[idx]

            mappings.append({
                "query_chunk": query_chunk,  # Original chunk that was queried
                "matched_sentence": stored_documents[idx],
                "domain": metadata["domain"],
                "standard": metadata["standard"],
                "objective": metadata["objective"],
                "procedure": metadata["procedure"],
                "similarity": similarity
            })

    return mappings

# Perform the query on the first 100 embeddings
results = query_knowledge_base(query_embeddings_np)

# Display the results (for first 5 query mappings)
for mapping in results[:5]:
    print(f"Query Chunk: {mapping['query_chunk'][:100]}...")  # Truncate to show first 100 chars
    print(f"Matched Sentence: {mapping['matched_sentence']}")
    print(f"Domain: {mapping['domain']}, Standard: {mapping['standard']}")
    print(f"Objective: {mapping['objective']}, Procedure: {mapping['procedure']}")
    print(f"Similarity: {mapping['similarity']:.4f}")
    print()




























































































pip install torch sentence-transformers scikit-learn cohere ray
pip install chromadb,pyPDF2

import uuid 
import chromadb
from sentence_transformers import SentenceTransformer
import numpy as np
from pathlib import Path
from PyPDF2 import PdfReader
import os

# Initialize Sentence Embedding Model (GTE-Large)
embed_model = SentenceTransformer('thenlper/gte-large')

# Initialize ChromaDB client
client = chromadb.Client()
collection_name = "pdf_embeddings"
collection = client.get_or_create_collection(name=collection_name)

def read_pdf(file_path):
    try:
        with open(file_path, 'rb') as file:
            pdf = PdfReader(file)
            text = ""
            for page in pdf.pages:
                text += page.extract_text()
        print(f"Successfully read PDF: {file_path}")
        return text
    except Exception as e:
        print(f"Error reading PDF {file_path}: {e}")
        return ""

def chunk_text(text, chunk_size=1000, overlap=200):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        if end > len(text):
            end = len(text)
        chunks.append(text[start:end])
        start = end - overlap
    return chunks

def process_pdf(file_path):
    text = read_pdf(file_path)
    if not text:
        print(f"No text extracted from {file_path}")
        return
    
    print(f"Extracted text length: {len(text)}")
    chunks = chunk_text(text)
    print(f"Number of chunks: {len(chunks)}")
    
    for i, chunk in enumerate(chunks):
        try:
            embedding = embed_model.encode(chunk).tolist()
            collection.add(
                ids=[str(uuid.uuid4())],
                documents=[chunk],
                embeddings=[embedding],
                metadatas=[{"source": str(file_path)}]
            )
            print(f"Added chunk {i+1}/{len(chunks)} with embedding of length {len(embedding)}")
        except Exception as e:
            print(f"Error processing chunk {i+1} from {file_path}: {e}")

# Directory containing PDF files
DATA_DIR = Path("/content/data")

# Process all PDF files in the directory

try:
    for file in os.listdir(DATA_DIR):
        if file.endswith(".pdf"):
            file_path = DATA_DIR / file
            print(f"Processing: {file}")
            process_pdf(file_path)
            print(f"Finished processing: {file}")
    print("All PDF files have been processed.")
except Exception as e:
    print(f"Error during PDF processing: {e}")

print(f"Total items in collection: {collection.count()}")
print("Available collections:", client.list_collections())
print("Collection info:", collection.get())

def query_knowledge_base(query_text, n_results=3):
    try:
        query_embedding = embed_model.encode(query_text).tolist()
        print(f"Query embedding length: {len(query_embedding)}")
        print("Query parameters:")
        print(f"Embedding: {query_embedding[:5]}... (length: {len(query_embedding)})")
        print(f"n_results: {n_results}")
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            include=["metadatas", "documents", "distances"]
        )
        print("Raw results:", results)
        
        if not results['documents'][0]:
            print("No matching documents found.")
            return
        
        for i, doc in enumerate(results['documents'][0]):
            print(f"Result {i+1}:")
            print(f"Text: {doc[:200]}...")  # Print first 200 characters
            print(f"Source: {results['metadatas'][0][i]['source']}")
            print(f"Similarity: {1 - results['distances'][0][i]:.4f}")
            print()
    except Exception as e:
        print(f"Error during querying: {e}")

# Example usage
print("\nPerforming example query:")
query_knowledge_base("What is the main topic of the document?")










































1. Install Required Libraries

Ensure you have the necessary libraries installed:

pip install transformers torch

2. Load the Locally Saved RoBERTa Model

Assuming you’ve already downloaded the model and its tokenizer, you can load them from the local directory:

from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch

# Load the tokenizer and model from the local directory
model_dir = "path/to/your/local/roberta/model"

tokenizer = RobertaTokenizer.from_pretrained(model_dir)
model = RobertaForSequenceClassification.from_pretrained(model_dir)

# Ensure the model is in evaluation mode
model.eval()

Replace "path/to/your/local/roberta/model" with the actual directory where your model and tokenizer are saved.

3. Perform Inference

You can now use the model to perform inference on a given text. Here’s an example of how to do this:

# Example input text
text = "This is a sample sentence for testing."

# Tokenize the input
inputs = tokenizer(text, return_tensors="pt")

# Run the model on the input
with torch.no_grad():  # Disable gradient calculation for inference
    outputs = model(**inputs)

# Get the predicted logits
logits = outputs.logits

# Optionally, convert logits to probabilities
probabilities = torch.softmax(logits, dim=-1)

# Get predicted class (for classification models)
predicted_class = torch.argmax(logits, dim=-1)

print(f"Logits: {logits}")
print(f"Probabilities: {probabilities}")
print(f"Predicted class: {predicted_class.item()}")

Key Poin




















import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer, AutoModel
from peft import LoraConfig, get_peft_model
import bitsandbytes as bnb
from sklearn.preprocessing import MultiLabelBinarizer
import time
from torch.cuda.amp import autocast
from torch.nn.functional import cosine_similarity

# Tokenization and Preprocessing
def tokenize_sentences(sentences, tokenizer, max_len=256):
    # Tokenize the list of raw strings (sentences)
    inputs = tokenizer(sentences, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt")
    return inputs['input_ids']

# Ensure filtered_df is defined before this point
# Data Preparation (Make sure the sentences and procedures are lists of strings)
sentences = filtered_df['extended_text'].astype(str).tolist()  # Ensure sentences are strings
procedures = filtered_df['procedure'].astype(str).tolist()  # Ensure procedures are strings

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

# Tokenize sentences and procedures
sentence_input_ids = tokenize_sentences(sentences, tokenizer)  # Tokenize the sentence strings
procedure_input_ids = tokenize_sentences(procedures, tokenizer)  # Tokenize the procedure strings

# Custom Model to Extract Embeddings
class MistralEmbeddingExtractor(nn.Module):
    def __init__(self):
        super(MistralEmbeddingExtractor, self).__init__()
        self.mistral = AutoModel.from_pretrained(
            "mistralai/Mistral-7B-v0.1",
            load_in_8bit=True,
            device_map='auto',
            trust_remote_code=True
        )

    def forward(self, input_ids):
        outputs = self.mistral(input_ids=input_ids)
        embeddings = outputs.last_hidden_state.mean(dim=1)  # Extract embeddings by averaging the hidden states
        return embeddings

# Apply LoRA PEFT
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Updated target modules for Mistral
    lora_dropout=0.1,
    bias="none"
)

# Initialize the model and apply LoRA
model = MistralEmbeddingExtractor()
model = get_peft_model(model, lora_config)

# Load model onto the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Extract Embeddings for Sentences and Procedures
def extract_embeddings(input_ids):
    input_ids = input_ids.to(device)
    with torch.no_grad():
        with autocast():
            embeddings = model(input_ids)
    return embeddings

# Extract embeddings for all sentences and procedures
sentence_embeddings = extract_embeddings(sentence_input_ids)
procedure_embeddings = extract_embeddings(procedure_input_ids)

# Compute Cosine Similarity between Sentence Embeddings and Procedure Embeddings
def compute_cosine_similarity(sentence_embeddings, procedure_embeddings):
    # Normalize embeddings to ensure cosine similarity is accurate
    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
    procedure_embeddings = torch.nn.functional.normalize(procedure_embeddings, p=2, dim=1)
    
    similarities = cosine_similarity(sentence_embeddings.unsqueeze(1), procedure_embeddings.unsqueeze(0), dim=-1)
    return similarities

# Compute similarities
cosine_similarities = compute_cosine_similarity(sentence_embeddings, procedure_embeddings)

# Map Sentences to Procedures based on the Highest Cosine Similarity
similarity_scores, predicted_procedure_indices = torch.max(cosine_similarities, dim=1)

# Convert indices to procedures
predicted_procedures = [procedures[idx] for idx in predicted_procedure_indices.tolist()]

# Example: Print the first 5 sentence mappings
for i in range(5):
    print(f"Sentence: {sentences[i]}")
    print(f"Predicted Procedure: {predicted_procedures[i]}")
    print(f"Similarity Score: {similarity_scores[i].item():.4f}")
    print()

























import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from peft import LoraConfig, get_peft_model
import bitsandbytes as bnb
from torch.cuda.amp import autocast
from torch.nn.functional import cosine_similarity

# Tokenization and Preprocessing
def tokenize_sentences(sentences, tokenizer, max_len=256):
    """Tokenize a list of sentences, ensuring the output is in the expected format."""
    inputs = tokenizer(sentences, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt")
    input_ids = inputs['input_ids']
    attention_mask = inputs['attention_mask']
    return input_ids, attention_mask

# Data Preparation
sentences = filtered_df['extended_text'].tolist()
procedures = filtered_df['procedure'].tolist()

# Tokenize sentences and procedures
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B")
sentence_input_ids, sentence_attention_mask = tokenize_sentences(sentences, tokenizer)
procedure_input_ids, procedure_attention_mask = tokenize_sentences(procedures, tokenizer)

# Custom Model to Extract Embeddings
class MistralEmbeddingExtractor(nn.Module):
    def __init__(self):
        super(MistralEmbeddingExtractor, self).__init__()
        self.mistral = AutoModel.from_pretrained(
            "mistralai/Mistral-7B",
            load_in_8bit=True,
            device_map='auto',
            quantization_config=bnb.QuantizationConfig(load_in_8bit=True)
        )

    def forward(self, input_ids, attention_mask=None):
        outputs = self.mistral(input_ids=input_ids, attention_mask=attention_mask)
        # Take the mean of the last hidden state to represent the sentence embedding
        embeddings = outputs.last_hidden_state.mean(dim=1)
        return embeddings

# Apply LoRA PEFT
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # target the appropriate modules in Mistral
    lora_dropout=0.1,
    bias="none"
)

# Initialize the model and apply LoRA
model = MistralEmbeddingExtractor()
model = get_peft_model(model, lora_config)

# Load model onto the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Extract Embeddings for Sentences and Procedures
def extract_embeddings(input_ids, attention_mask):
    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)
    with torch.no_grad():
        with autocast():
            embeddings = model(input_ids, attention_mask)
    return embeddings

# Extract embeddings for all sentences and procedures
sentence_embeddings = extract_embeddings(sentence_input_ids, sentence_attention_mask)
procedure_embeddings = extract_embeddings(procedure_input_ids, procedure_attention_mask)

# Compute Cosine Similarity between Sentence Embeddings and Procedure Embeddings
def compute_cosine_similarity(sentence_embeddings, procedure_embeddings):
    similarities = cosine_similarity(sentence_embeddings.unsqueeze(1), procedure_embeddings.unsqueeze(0), dim=-1)
    return similarities

# Compute similarities
cosine_similarities = compute_cosine_similarity(sentence_embeddings, procedure_embeddings)

# Map Sentences to Procedures based on the Highest Cosine Similarity
similarity_scores, predicted_procedure_indices = torch.max(cosine_similarities, dim=1)

# Convert indices to procedures
predicted_procedures = [procedures[idx] for idx in predicted_procedure_indices.tolist()]

# Example: Print the first 5 sentence mappings
for i in range(5):
    print(f"Sentence: {sentences[i]}")
    print(f"Predicted Procedure: {predicted_procedures[i]}")
    print(f"Similarity Score: {similarity_scores[i].item():.4f}")
    print()





























import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from peft import LoraConfig, get_peft_model
import bitsandbytes as bnb
from torch.cuda.amp import autocast
from torch.nn.functional import cosine_similarity

# Tokenization and Preprocessing
def tokenize_sentences(sentences, tokenizer, max_len=256):
    inputs = tokenizer(sentences, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt")
    input_ids = inputs['input_ids']
    return input_ids

# Data Preparation
sentences = filtered_df['extended_text'].tolist()
procedures = filtered_df['procedure'].tolist()

# Tokenize sentences and procedures
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B")
sentence_input_ids = tokenize_sentences(sentences, tokenizer)
procedure_input_ids = tokenize_sentences(procedures, tokenizer)

# Custom Model to Extract Embeddings
class MistralEmbeddingExtractor(nn.Module):
    def __init__(self):
        super(MistralEmbeddingExtractor, self).__init__()
        self.mistral = AutoModel.from_pretrained(
            "mistralai/Mistral-7B",
            load_in_8bit=True,
            device_map='auto',
            quantization_config=bnb.QuantizationConfig(load_in_8bit=True)
        )

    def forward(self, input_ids, attention_mask=None):
        outputs = self.mistral(input_ids=input_ids, attention_mask=attention_mask)
        # Take the mean of the last hidden state to represent the sentence embedding
        embeddings = outputs.last_hidden_state.mean(dim=1)
        return embeddings

# Apply LoRA PEFT
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # target the appropriate modules in Mistral
    lora_dropout=0.1,
    bias="none"
)

# Initialize the model and apply LoRA
model = MistralEmbeddingExtractor()
model = get_peft_model(model, lora_config)

# Load model onto the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Extract Embeddings for Sentences and Procedures
def extract_embeddings(input_ids):
    input_ids = input_ids.to(device)
    attention_mask = (input_ids != tokenizer.pad_token_id).to(device)  # Create attention mask to avoid padding tokens
    with torch.no_grad():
        with autocast():
            embeddings = model(input_ids, attention_mask)
    return embeddings

# Extract embeddings for all sentences and procedures
sentence_embeddings = extract_embeddings(sentence_input_ids)
procedure_embeddings = extract_embeddings(procedure_input_ids)

# Compute Cosine Similarity between Sentence Embeddings and Procedure Embeddings
def compute_cosine_similarity(sentence_embeddings, procedure_embeddings):
    similarities = cosine_similarity(sentence_embeddings.unsqueeze(1), procedure_embeddings.unsqueeze(0), dim=-1)
    return similarities

# Compute similarities
cosine_similarities = compute_cosine_similarity(sentence_embeddings, procedure_embeddings)

# Map Sentences to Procedures based on the Highest Cosine Similarity
similarity_scores, predicted_procedure_indices = torch.max(cosine_similarities, dim=1)

# Convert indices to procedures
predicted_procedures = [procedures[idx] for idx in predicted_procedure_indices.tolist()]

# Example: Print the first 5 sentence mappings
for i in range(5):
    print(f"Sentence: {sentences[i]}")
    print(f"Predicted Procedure: {predicted_procedures[i]}")
    print(f"Similarity Score: {similarity_scores[i].item():.4f}")
    print()

































import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer, AutoModel
from peft import LoraConfig, get_peft_model
import bitsandbytes as bnb
from sklearn.preprocessing import MultiLabelBinarizer
import time
from torch.cuda.amp import autocast
from torch.nn.functional import cosine_similarity

# Tokenization and Preprocessing
def tokenize_sentences(sentences, tokenizer, max_len=256):
    inputs = [tokenizer(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt") for sentence in sentences]
    input_ids = torch.cat([input['input_ids'] for input in inputs], dim=0)
    return input_ids

# Data Preparation
sentences = filtered_df['extended_text'].tolist()
procedures = filtered_df['procedure'].tolist()

# Tokenize sentences and procedures
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B")
sentence_input_ids = tokenize_sentences(sentences, tokenizer)
procedure_input_ids = tokenize_sentences(procedures, tokenizer)

# Custom Model to Extract Embeddings
class MistralEmbeddingExtractor(nn.Module):
    def __init__(self):
        super(MistralEmbeddingExtractor, self).__init__()
        self.mistral = AutoModel.from_pretrained(
            "mistralai/Mistral-7B",
            load_in_8bit=True,
            device_map='auto',
            quantization_config=bnb.QuantizationConfig(load_in_8bit=True)
        )

    def forward(self, input_ids):
        outputs = self.mistral(input_ids=input_ids)
        embeddings = outputs.pooler_output  # Extract the pooled output as embeddings
        return embeddings

# Apply LoRA PEFT
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["query", "key", "value"],
    lora_dropout=0.1,
    bias="none"
)

# Initialize the model and apply LoRA
model = MistralEmbeddingExtractor()
model = get_peft_model(model, lora_config)

# Load model onto the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Extract Embeddings for Sentences and Procedures
def extract_embeddings(input_ids):
    input_ids = input_ids.to(device)
    with torch.no_grad():
        with autocast():
            embeddings = model(input_ids)
    return embeddings

# Extract embeddings for all sentences and procedures
sentence_embeddings = extract_embeddings(sentence_input_ids)
procedure_embeddings = extract_embeddings(procedure_input_ids)

# Compute Cosine Similarity between Sentence Embeddings and Procedure Embeddings
def compute_cosine_similarity(sentence_embeddings, procedure_embeddings):
    similarities = cosine_similarity(sentence_embeddings.unsqueeze(1), procedure_embeddings.unsqueeze(0), dim=-1)
    return similarities

# Compute similarities
cosine_similarities = compute_cosine_similarity(sentence_embeddings, procedure_embeddings)

# Map Sentences to Procedures based on the Highest Cosine Similarity
# For each sentence, find the procedure with the highest cosine similarity
similarity_scores, predicted_procedure_indices = torch.max(cosine_similarities, dim=1)

# Convert indices to procedures
predicted_procedures = [procedures[idx] for idx in predicted_procedure_indices.tolist()]

# Example: Print the first 5 sentence mappings
for i in range(5):
    print(f"Sentence: {sentences[i]}")
    print(f"Predicted Procedure: {predicted_procedures[i]}")
    print(f"Similarity Score: {similarity_scores[i].item():.4f}")
    print()






















Install required libraries
!pip install transformers peft bitsandbytes torchdatasets

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from peft import LoraConfig, get_peft_model
import bitsandbytes as bnb
from sklearn.preprocessing import MultiLabelBinarizer
import time
from torch.cuda.amp import autocast, GradScaler

# Tokenization and Preprocessing
def tokenize_sentences(sentences, tokenizer, max_len=256):
    print("Tokenizing sentences...")
    start_time = time.time()
    inputs = [tokenizer(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt") for sentence in sentences]
    input_ids = torch.cat([input['input_ids'] for input in inputs], dim=0)
    print(f"Tokenization completed in {time.time() - start_time:.2f} seconds.")
    return input_ids

# Data Preparation
# Assuming 'filtered_df' contains the sentence and procedure columns
sentences = filtered_df['extended_text'].tolist()
labels_procedures = filtered_df['procedure'].tolist()
labels_procedures = [eval(label) if isinstance(label, str) else label for label in labels_procedures]
mlb = MultiLabelBinarizer()
encoded_labels = mlb.fit_transform(labels_procedures)
labels_procedures = torch.tensor(encoded_labels, dtype=torch.float32)

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B")

# Tokenize sentences
input_ids = tokenize_sentences(sentences, tokenizer)

# Dataset and DataLoader
train_dataset = TensorDataset(input_ids, labels_procedures)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)

# Custom Model with Multi-Label Classification Head
class MistralMultiLabelClassifier(nn.Module):
    def __init__(self, num_labels):
        super(MistralMultiLabelClassifier, self).__init__()
        self.mistral = AutoModel.from_pretrained(
            "mistralai/Mistral-7B",
            load_in_8bit=True,  # Quantize with bitsandbytes
            device_map='auto',
            quantization_config=bnb.QuantizationConfig(load_in_8bit=True)
        )
        self.classifier = nn.Linear(self.mistral.config.hidden_size, num_labels)  # Custom linear layer for multi-label classification

    def forward(self, input_ids):
        outputs = self.mistral(input_ids=input_ids)
        pooled_output = outputs.pooler_output  # Use the pooled output from the model
        logits = self.classifier(pooled_output)
        return logits

# Apply LoRA PEFT
lora_config = LoraConfig(
    r=8,  # Rank for LoRA layers
    lora_alpha=16,
    target_modules=["query", "key", "value"],  # Layers to apply LoRA (usually attention layers)
    lora_dropout=0.1,
    bias="none"
)

# Number of procedures (labels)
num_procedures = len(mlb.classes_)

# Initialize model with custom classifier
model = MistralMultiLabelClassifier(num_labels=num_procedures)
model = get_peft_model(model, lora_config)

# Optimizer, Scheduler, and Loss Function
optimizer = AdamW(model.parameters(), lr=2e-5)
num_training_steps = len(train_loader) * 3  # 3 epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)
loss_fn = nn.BCEWithLogitsLoss()

# Mixed Precision Training
scaler = GradScaler()

# Training Loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

epochs = 3
for epoch in range(epochs):
    print(f"Starting Epoch {epoch + 1}/{epochs}...")
    epoch_start_time = time.time()
    model.train()
    total_loss = 0

    for batch_idx, batch in enumerate(train_loader):
        batch_start_time = time.time()
        batch_input_ids, batch_labels = [x.to(device) for x in batch]

        optimizer.zero_grad()

        # Mixed precision forward pass
        with autocast():
            logits = model(batch_input_ids)
            loss = loss_fn(logits, batch_labels)

        # Backward pass with scaling
        scaler.scale(loss).backward()

        # Gradient clipping (optional)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Optimizer step and scaler update
        scaler.step(optimizer)
        scaler.update()

        scheduler.step()

        total_loss += loss.item()

        # Print progress for every batch
        print(f"Batch {batch_idx + 1}/{len(train_loader)} completed in {time.time() - batch_start_time:.2f} seconds. Loss: {loss.item():.4f}")

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{epochs} completed in {time.time() - epoch_start_time:.2f} seconds. Average Loss: {avg_loss:.4f}\n")


# Prediction Step
def tokenize_for_prediction(sentences, tokenizer, max_len=512):
    inputs = tokenizer(sentences, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt")
    return inputs['input_ids']

# Example sentence for prediction
sentence = ["Sample legal text that you want to classify."]
input_ids = tokenize_for_prediction(sentence, tokenizer)

# Move to device
input_ids = input_ids.to(device)

# Prediction with quantized model
model.eval()
with torch.no_grad():
    with autocast():  # Use mixed precision for inference
        logits = model(input_ids)

# Apply sigmoid to get probabilities
probabilities = torch.sigmoid(logits)

# Define a threshold to get the final labels (e.g., 0.5)
threshold = 0.5
predicted_labels = (probabilities > threshold).int()

# Convert predictions to readable format (using MultiLabelBinarizer)
predicted_labels = predicted_labels.cpu()





















Fine-tuning large models like Mistral 7B can be resource-intensive, but by using techniques like LoRA (Low-Rank Adaptation) and PEFT (Parameter-Efficient Fine-Tuning), you can significantly reduce memory consumption and speed up the process. These methods update only a small subset of parameters, making the process much lighter.

I’ll walk you through the code to fine-tune Mistral 7B using LoRA and PEFT on AWS SageMaker. LoRA focuses on injecting low-rank matrices into the attention layers, while PEFT enables tuning only a few layers.

We’ll use the peft library with LoRA on top of Mistral 7B.

Required Libraries:

	•	peft: For efficient parameter tuning.
	•	transformers: For loading the base Mistral model.

You can install the required libraries in your SageMaker notebook environment as follows:

!pip install peft transformers accelerate datasets

Updated Code Using LoRA

Here’s how you can fine-tune Mistral 7B with LoRA using PEFT on AWS SageMaker.

1. Setup: Tokenization and Data Preparation

We’ll start by tokenizing the data, similar to your original code. Mistral 7B will be loaded using the AutoTokenizer from Hugging Face’s transformers.

from transformers import AutoTokenizer
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MultiLabelBinarizer

# Load the Mistral 7B tokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B")

# Tokenization function
def tokenize_sentences(sentences, tokenizer, max_len=512):
    inputs = tokenizer(sentences, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt")
    return inputs['input_ids']

# Tokenize sentences (assuming 'filtered_df' is your dataframe)
sentences = filtered_df['extended_text'].tolist()
input_ids = tokenize_sentences(sentences, tokenizer)

# Prepare labels for multi-label classification
labels_procedures = filtered_df['procedure'].tolist()
mlb = MultiLabelBinarizer()
encoded_labels = mlb.fit_transform(labels_procedures)
labels_procedures = torch.tensor(encoded_labels, dtype=torch.float32)

# Create dataset and dataloader
train_dataset = TensorDataset(input_ids, labels_procedures)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

2. PEFT and LoRA Setup

PEFT enables you to apply LoRA by focusing only on the most critical parameters. The key idea is to update only a small, low-rank subspace in the model’s weight matrices during fine-tuning.

from peft import get_peft_model, LoraConfig, TaskType
from transformers import AutoModelForCausalLM

# Load Mistral 7B model
model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B')

# Define LoRA configuration
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,    # Set to causal language modeling
    r=16,                            # Low-rank dimension
    lora_alpha=32,                   # LoRA scaling factor
    lora_dropout=0.1,                # Dropout for LoRA layers
    target_modules=["q_proj", "v_proj"]  # Apply LoRA on query and value projections in attention layers
)

# Apply LoRA to the model
model = get_peft_model(model, lora_config)

3. Training Loop with Mixed Precision

Using mixed precision (torch.cuda.amp) helps reduce memory usage. Here’s the training loop:

import torch
import torch.nn as nn
from transformers import AdamW, get_linear_schedule_with_warmup

# Move model to device (GPU if available)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=2e-5)
num_training_steps = len(train_loader) * 3  # 3 epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)
loss_fn = nn.BCEWithLogitsLoss()

# Mixed precision training
scaler = torch.cuda.amp.GradScaler()

# Training loop
epochs = 3
for epoch in range(epochs):
    model.train()
    total_loss = 0
    print(f"Starting Epoch {epoch + 1}/{epochs}...")
    
    for batch_idx, batch in enumerate(train_loader):
        batch_input_ids, batch_labels = [x.to(device) for x in batch]
        
        optimizer.zero_grad()

        with torch.cuda.amp.autocast():
            logits = model(batch_input_ids).logits  # Adjust for causal language model
            loss = loss_fn(logits, batch_labels)
        
        # Backpropagation with scaling
        scaler.scale(loss).backward()

        # Step optimizer and update scaler
        scaler.step(optimizer)
        scaler.update()

        # Scheduler step
        scheduler.step()

        total_loss += loss.item()

        # Logging
        if (batch_idx + 1) % 10 == 0:
            print(f"Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}")

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{epochs} completed. Average Loss: {avg_loss:.4f}")

4. Save the Fine-Tuned Model

After fine-tuning, save the model. SageMaker will handle uploading the model artifacts to S3 for deployment.

import os

# Save LoRA fine-tuned model
model_save_path = os.path.join("mistral_lora_fine_tuned")
os.makedirs(model_save_path, exist_ok=True)
model.save_pretrained(model_save_path)

5. Deploying on AWS SageMaker

Deploy the fine-tuned model using SageMaker with Hugging Face integration.

from sagemaker.huggingface import HuggingFaceModel

# Define Hugging Face model
huggingface_model = HuggingFaceModel(
    model_data=f"s3://{bucket}/mistral_lora_fine_tuned.tar.gz",
    role=role,
    transformers_version="4.26",
    pytorch_version="1.13",
    py_version="py39"
)

# Deploy the model
predictor = huggingface_model.deploy(
    initial_instance_count=1,
    instance_type="ml.p4d.24xlarge"  # Adjust based on needs
)

Key Points:

	•	LoRA (Low-Rank Adaptation) helps by updating only a low-rank subset of model weights during fine-tuning, reducing memory and computation.
	•	PEFT library enables efficient tuning of large models like Mistral 7B.
	•	Mixed precision training reduces memory usage and improves speed on GPUs.
	•	SageMaker deployment allows you to deploy the fine-tuned model for inference.

With this setup, you should be able to fine-tune and deploy Mistral 7B on a legal text classification task in a memory-efficient way using AWS SageMaker.
























import torch
import torch.nn as nn
from transformers import RobertaTokenizer, RobertaModel, AdamW
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MultiLabelBinarizer
from torch.utils.tensorboard import SummaryWriter
import os

# Assuming 'filtered_df' contains the sentence and procedure columns
sentences = filtered_df['extended_text'].tolist()  # Replace with your actual sentence column name
labels_procedures = filtered_df['procedure'].tolist()  # Replace with your actual procedure label column

# Convert lists of strings to list of lists (if it's a string representation of lists, use eval)
labels_procedures = [eval(label) if isinstance(label, str) else label for label in labels_procedures]

# MultiLabelBinarizer to one-hot encode the procedure labels
mlb = MultiLabelBinarizer()
encoded_labels = mlb.fit_transform(labels_procedures)

# Convert the encoded labels to a tensor
labels_procedures = torch.tensor(encoded_labels, dtype=torch.float32)

# Tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

def tokenize_sentences(sentences, tokenizer, max_len=512):
    inputs = [tokenizer(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt") for sentence in sentences]
    input_ids = torch.cat([input['input_ids'] for input in inputs], dim=0)  # (batch_size, max_len)
    return input_ids

# Tokenize your input sentences
input_ids = tokenize_sentences(sentences, tokenizer)

# Dataset and DataLoader
train_dataset = TensorDataset(input_ids, labels_procedures)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# Define the Model
class RobertaMultiLabelClassifier(nn.Module):
    def __init__(self, num_labels):
        super(RobertaMultiLabelClassifier, self).__init__()
        self.roberta = RobertaModel.from_pretrained('roberta-base')
        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)
    
    def forward(self, input_ids):
        outputs = self.roberta(input_ids=input_ids)
        pooled_output = outputs.pooler_output  # Get the pooled output
        logits = self.classifier(pooled_output)
        return logits

# Initialize model
num_procedures = len(mlb.classes_)
model = RobertaMultiLabelClassifier(num_labels=num_procedures)

# Optimizer and Loss Function
optimizer = AdamW(model.parameters(), lr=1e-5)
loss_fn = nn.BCEWithLogitsLoss()

# TensorBoard SummaryWriter
log_dir = './logs'
os.makedirs(log_dir, exist_ok=True)
writer = SummaryWriter(log_dir)

# Training Loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

epochs = 5
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        batch_input_ids, batch_labels = [x.to(device) for x in batch]

        # Forward pass
        logits = model(batch_input_ids)
        loss = loss_fn(logits, batch_labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

    # Log training loss to TensorBoard
    writer.add_scalar('Loss/train', avg_loss, epoch)

    # Save checkpoint
    checkpoint_path = os.path.join(log_dir, f'model_epoch_{epoch+1}.pt')
    torch.save(model.state_dict(), checkpoint_path)
    print(f"Checkpoint saved at {checkpoint_path}")

# Close the TensorBoard writer
writer.close()

# Prediction Function
def predict(model, sentence):
    model.eval()
    tokens = tokenizer(sentence, return_tensors="pt", padding='max_length', truncation=True, max_length=512)
    input_ids = tokens['input_ids'].to(device)

    with torch.no_grad():
        logits = model(input_ids)

    # Apply sigmoid to get probabilities
    procedure_probs = torch.sigmoid(logits).squeeze().cpu().numpy()

    # Convert probabilities to binary predictions (threshold can be adjusted)
    procedure_preds = procedure_probs > 0.5

    # Map binary predictions to class names
    procedures = mlb.inverse_transform([procedure_preds])[0]

    return {
        "sentence": sentence,
        "procedures": list(procedures)
    }

# Example usage
example_sentence = "Implement network access control."
prediction = predict(model, example_sentence)
print(prediction)

# Predict on a dataframe
def predict_on_dataframe(model, dataframe):
    predictions = []
    for sentence in dataframe['extended_text']:
        result = predict(model, sentence)
        predictions.append(result)
    return predictions

# Get predictions for the first 200 rows
df_predictions = predict_on_dataframe(model, filtered_df.head(200))
print(df_predictions)














import torch
import torch.nn as nn
from transformers import RobertaTokenizer, RobertaModel, AdamW
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MultiLabelBinarizer

# Assuming 'filtered_df' contains the sentence and procedure columns
# Example columns: 'sentence_column' and 'procedure_labels_column'
sentences = filtered_df['extended_text'].tolist()  # Replace with your actual sentence column name
labels_procedures = filtered_df['procedure'].tolist()  # Replace with your actual procedure label column

# Convert lists of strings to list of lists (if it's a string representation of lists, use eval)
labels_procedures = [eval(label) if isinstance(label, str) else label for label in labels_procedures]

# MultiLabelBinarizer to one-hot encode the procedure labels
mlb = MultiLabelBinarizer()
encoded_labels = mlb.fit_transform(labels_procedures)

# Convert the encoded labels to a tensor
labels_procedures = torch.tensor(encoded_labels, dtype=torch.float32)

# Tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

def tokenize_sentences(sentences, tokenizer, max_len=512):
    inputs = [tokenizer(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt") for sentence in sentences]
    input_ids = torch.cat([input['input_ids'] for input in inputs], dim=0)  # (batch_size, max_len)
    return input_ids

# Tokenize your input sentences
input_ids = tokenize_sentences(sentences, tokenizer)

# Dataset and DataLoader
train_dataset = TensorDataset(input_ids, labels_procedures)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# Define the Model
class RobertaMultiLabelClassifier(nn.Module):
    def __init__(self, num_labels):
        super(RobertaMultiLabelClassifier, self).__init__()
        self.roberta = RobertaModel.from_pretrained('roberta-base')
        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)
    
    def forward(self, input_ids):
        outputs = self.roberta(input_ids=input_ids)
        pooled_output = outputs.pooler_output  # Get the pooled output
        logits = self.classifier(pooled_output)
        return logits

# Initialize model
num_procedures = len(mlb.classes_)
model = RobertaMultiLabelClassifier(num_labels=num_procedures)

# Optimizer and Loss Function
optimizer = AdamW(model.parameters(), lr=1e-5)
loss_fn = nn.BCEWithLogitsLoss()

# Training Loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

epochs = 5
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        batch_input_ids, batch_labels = [x.to(device) for x in batch]

        # Forward pass
        logits = model(batch_input_ids)
        loss = loss_fn(logits, batch_labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

# Prediction Function
def predict(model, sentence):
    model.eval()
    tokens = tokenizer(sentence, return_tensors="pt", padding='max_length', truncation=True, max_length=512)
    input_ids = tokens['input_ids'].to(device)

    with torch.no_grad():
        logits = model(input_ids)

    # Apply sigmoid to get probabilities
    procedure_probs = torch.sigmoid(logits).squeeze().cpu().numpy()

    # Convert probabilities to binary predictions (threshold can be adjusted)
    procedure_preds = procedure_probs > 0.5

    # Map binary predictions to class names
    procedures = mlb.inverse_transform([procedure_preds])[0]

    return {
        "sentence": sentence,
        "procedures": list(procedures)
    }

# Example usage
example_sentence = "Implement network access control."
prediction = predict(model, example_sentence)
print(prediction)

# Predict on a dataframe
def predict_on_dataframe(model, dataframe):
    predictions = []
    for sentence in dataframe['extended_text']:
        result = predict(model, sentence)
        predictions.append(result)
    return predictions

# Get predictions for the first 200 rows
df_predictions = predict_on_dataframe(model, filtered_df.head(200))
print(df_predictions)









import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from transformers import AdamW, RobertaTokenizer
from torch.nn.utils.rnn import pad_sequence

# Assume you have a RoBERTa model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = model.to(device)  # Assuming 'model' is your pre-trained Roberta model

# Example sentence and labels (adjust as needed)
sentences = filtered_df['sentence_column'].tolist()  # Replace 'sentence_column' with your actual column name
labels_procedures = filtered_df['procedure_labels_column'].tolist()  # Replace with actual procedure label column

# Tokenization function with truncation and padding
def tokenize_sentences(sentences, tokenizer, max_len=512):
    inputs = [tokenizer(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors="pt") for sentence in sentences]
    input_ids = torch.cat([input['input_ids'] for input in inputs], dim=0)  # (batch_size, max_len)
    return input_ids

# Tokenize your input sentences
input_ids = tokenize_sentences(sentences, tokenizer)

# Convert labels to tensors
labels_procedures = torch.tensor(labels_procedures, dtype=torch.float32)

# Dataset and DataLoader
train_dataset = TensorDataset(input_ids, labels_procedures)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# Optimizer and Loss Function
optimizer = AdamW(model.parameters(), lr=1e-5)
loss_fn = nn.BCEWithLogitsLoss()

# Training Loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

epochs = 5
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        batch_input_ids, batch_labels = [x.to(device) for x in batch]

        # Forward pass
        logits = model(batch_input_ids).logits  # .logits for transformer models
        loss = loss_fn(logits, batch_labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

 # Maximum length for RoBERTa model
MAX_LEN = 512

# Truncate input sequences to the maximum length and ensure padding if necessary
def preprocess_input(text):
    # Tokenize the input
    inputs = tokenizer(
        text,
        max_length=MAX_LEN,
        padding='max_length',  # Pad to the maximum length
        truncation=True,       # Truncate to the maximum length
        return_tensors='pt'    # Return PyTorch tensors
    )
    return inputs

# Example usage for a batch of input sentences
input_text = ["This is an example sentence.", "Another sentence for testing."]
batch_inputs = [preprocess_input(text) for text in input_text]




1. Install Required Libraries
First, ensure you have the required libraries installed:

bash
Copy code
pip install torch transformers scikit-learn pandas
2. Data Preprocessing
We'll use a filtered_df that contains sentences and corresponding multi-labels for procedures.

python
Copy code
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from transformers import RobertaTokenizer

# Assume `filtered_df` is your dataset with the columns 'extended_text' (sentences) and 'procedure' (multi-labels)

# Example dataframe
# filtered_df = pd.read_csv('your_filtered_df.csv')

# Tokenization
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Convert 'procedure' column into multi-hot vectors (multi-label binarization)
mlb_procedure = MultiLabelBinarizer()
filtered_df['procedure'] = filtered_df['procedure'].apply(lambda x: eval(x) if isinstance(x, str) else x)
multi_hot_procedures = mlb_procedure.fit_transform(filtered_df['procedure'])

# Tokenize sentences
def tokenize_sentence(sentence):
    return tokenizer(sentence, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].squeeze()

# Apply tokenization to the entire dataset
filtered_df['input_ids'] = filtered_df['extended_text'].apply(tokenize_sentence)

# Splitting input_ids and labels for training
input_ids = torch.stack(filtered_df['input_ids'].values)
labels_procedures = torch.tensor(multi_hot_procedures, dtype=torch.float32)
3. Define the Model
We’ll build a RoBERTa model with a classification head for multi-label classification:

python
Copy code
import torch
import torch.nn as nn
from transformers import RobertaModel

class RobertaMultiLabelClassifier(nn.Module):
    def __init__(self, num_procedures):
        super(RobertaMultiLabelClassifier, self).__init__()
        self.roberta = RobertaModel.from_pretrained('roberta-base')
        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_procedures)
    
    def forward(self, input_ids):
        outputs = self.roberta(input_ids=input_ids)
        pooled_output = outputs.pooler_output
        logits = self.classifier(pooled_output)
        return logits

# Initialize the model
num_procedures = len(mlb_procedure.classes_)
model = RobertaMultiLabelClassifier(num_procedures)
4. Training Loop
We'll train the model using Binary Cross Entropy Loss and AdamW optimizer.

python
Copy code
from torch.utils.data import DataLoader, TensorDataset
from transformers import AdamW

# Dataset and DataLoader
train_dataset = TensorDataset(input_ids, labels_procedures)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# Optimizer and Loss Function
optimizer = AdamW(model.parameters(), lr=1e-5)
loss_fn = nn.BCEWithLogitsLoss()

# Training Loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

epochs = 5
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        batch_input_ids, batch_labels = [x.to(device) for x in batch]

        # Forward pass
        logits = model(batch_input_ids)
        loss = loss_fn(logits, batch_labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
5. Prediction Function
Now we’ll write a function to make predictions for a given sentence:

python
Copy code
def predict(model, sentence):
    model.eval()
    tokens = tokenize_sentence(sentence).unsqueeze(0).to(device)  # Add batch dimension
    with torch.no_grad():
        logits = model(tokens)

    # Apply sigmoid to get probabilities
    procedure_probs = torch.sigmoid(logits).squeeze().cpu().numpy()

    # Convert probabilities to binary predictions (threshold can be adjusted)
    procedure_preds = procedure_probs > 0.5

    # Map binary predictions to procedure names
    procedures = mlb_procedure.inverse_transform([procedure_preds])[0]

    return {
        "sentence": sentence,
        "procedures": list(procedures)
    }
6. Evaluation and Model Validation
You can validate the model on a test dataset, but here we use an example prediction:

python
Copy code
# Example usage
sentence = "Implement network access control."
predictions = predict(model, sentence)
print(predictions)
7. Evaluate Model Performance
We’ll use accuracy, precision, recall, and F1-score to evaluate the model's performance.

python
Copy code
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluate_model(model, test_loader):
    model.eval()
    all_labels = []
    all_preds = []
    with torch.no_grad():
        for batch in test_loader:
            batch_input_ids, batch_labels = [x.to(device) for x in batch]

            # Forward pass
            logits = model(batch_input_ids)
            procedure_probs = torch.sigmoid(logits)

            # Convert probabilities to binary predictions
            procedure_preds = procedure_probs > 0.5

            all_labels.append(batch_labels.cpu().numpy())
            all_preds.append(procedure_preds.cpu().numpy())

    # Calculate evaluation metrics
    all_labels = np.vstack(all_labels)
    all_preds = np.vstack(all_preds)
    
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='micro')
    recall = recall_score(all_labels, all_preds, average='micro')
    f1 = f1_score(all_labels, all_preds, average='micro')

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

# Example test evaluation
test_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)  # You should use a real test dataset
evaluate_model(model, test_loader)






















1. Data Preprocessing
python
Copy code
import pandas as pd
import torch
from sklearn.preprocessing import MultiLabelBinarizer
from transformers import RobertaTokenizer
from torch.utils.data import Dataset, DataLoader

# Load the data (filtered_df should contain 'sentence' and 'procedures' columns)
filtered_df = pd.read_csv('filtered_df.csv')

# Preprocess procedures (multi-label binarization)
mlb_procedure = MultiLabelBinarizer()
filtered_df['procedures'] = filtered_df['procedures'].apply(eval)  # Convert string to list
y_procedures = mlb_procedure.fit_transform(filtered_df['procedures'])

# Initialize tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

class SentenceProcedureDataset(Dataset):
    def __init__(self, sentences, labels, tokenizer, max_length=128):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            sentence,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(label, dtype=torch.float)
        }

# Create dataset and data loaders
dataset = SentenceProcedureDataset(filtered_df['sentence'].values, y_procedures, tokenizer)
train_loader = DataLoader(dataset, batch_size=16, shuffle=True)

2. Model Definition
python
Copy code
import torch.nn as nn
from transformers import RobertaModel

class RobertaForMultiLabelClassification(nn.Module):
    def __init__(self, num_procedures):
        super(RobertaForMultiLabelClassification, self).__init__()
        self.roberta = RobertaModel.from_pretrained('roberta-base')
        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_procedures)
        self.sigmoid = nn.Sigmoid()

    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs[1]  # [CLS] token output
        logits = self.classifier(cls_output)
        return logits

# Initialize model
model = RobertaForMultiLabelClassification(num_procedures=y_procedures.shape[1])
3. Training Loop
python
Copy code
import torch.optim as optim

# Initialize optimizer and loss function
optimizer = optim.AdamW(model.parameters(), lr=2e-5)
loss_fn = nn.BCEWithLogitsLoss()

# Training function
def train(model, dataloader, optimizer, loss_fn, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch in dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            # Forward pass
            outputs = model(input_ids, attention_mask)
            loss = loss_fn(outputs, labels)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')

# Train the model
train(model, train_loader, optimizer, loss_fn, epochs=3)
4. Prediction Function
python
Copy code
def tokenize_sentence(sentence):
    encoding = tokenizer.encode_plus(
        sentence,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    return encoding['input_ids'], encoding['attention_mask']

def predict(model, sentence):
    model.eval()
    input_ids, attention_mask = tokenize_sentence(sentence)
    input_ids = input_ids.squeeze(0)  # Remove batch dimension
    attention_mask = attention_mask.squeeze(0)

    with torch.no_grad():
        logits = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))
        probs = torch.sigmoid(logits).squeeze().cpu().numpy()

    # Convert probabilities to binary predictions (threshold can be adjusted)
    preds = probs > 0.5

    # Map predictions to procedure names
    predicted_procedures = mlb_procedure.inverse_transform([preds])[0]

    return list(predicted_procedures)

# Example usage
sentence = "Implement network access control."
predictions = predict(model, sentence)
print(predictions)
5. Evaluation
python
Copy code
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluate(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            logits = model(input_ids, attention_mask)
            probs = torch.sigmoid(logits).cpu().numpy()
            preds = (probs > 0.5).astype(int)

            all_preds.append(preds)
            all_labels.append(labels.cpu().numpy())

    # Convert lists to arrays
    all_preds = np.vstack(all_preds)
    all_labels = np.vstack(all_labels)

    # Calculate metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='micro')
    recall = recall_score(all_labels, all_preds, average='micro')
    f1 = f1_score(all_labels, all_preds, average='micro')

    print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')

# Evaluate the model
evaluate(model, train_loader)
Summary of Steps:
Data Preprocessing:

Sentences are tokenized using the RoBERTa tokenizer.
Procedures are binarized using MultiLabelBinarizer.
Model Building:

A RoBERTa model is used to predict multiple procedures for each sentence using a multi-label classification head.
Training:

The model is trained using BCEWithLogitsLoss and optimized using AdamW optimizer.
Prediction:

Given a new sentence, the model predicts which procedures it maps to.
Evaluation:

Metrics such as accuracy, precision, recall, and F1-score are calculated to evaluate the model's performance.



















import plotly.express as px
import pandas as pd

# Explode the 'procedure' column to separate each procedure into its own row
procedures_df = filtered_df.explode('procedure')

# Group by 'procedure' and count how many sentences (rows) are associated with each procedure
procedure_sentence_count = procedures_df.groupby('procedure').size().reset_index(name='sentence_count')

# Sort by sentence count for better visualization
procedure_sentence_count = procedure_sentence_count.sort_values(by='sentence_count', ascending=False)

# Create a bar chart with Plotly
fig = px.bar(procedure_sentence_count,
             x='procedure', 
             y='sentence_count', 
             hover_name='procedure',  # This will show procedure name on hover
             title='Sentence Count per Procedure')

# Customize layout
fig.update_layout(
    xaxis_title='Procedure',
    yaxis_title='Sentence Count',
    xaxis_tickangle=-45,  # Rotate x-axis labels
    height=600,
    width=1000
)

# Show the plot
fig.show()


import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'procedure' column contains a list of procedures for each row in filtered_df
# First, explode the 'procedure' column to separate each procedure into its own row
procedures_df = filtered_df.explode('procedure')

# Group by the procedure and count occurrences
procedure_counts = procedures_df['procedure'].value_counts()

# Plot the chart
plt.figure(figsize=(10, 8))  # Set the figure size
procedure_counts.plot(kind='bar', color='skyblue')

# Set title and labels
plt.title('Procedure Distribution', fontsize=14)
plt.xlabel('Procedure', fontsize=12)
plt.ylabel('Count', fontsize=12)

# Rotate x labels for better readability
plt.xticks(rotation=90)

# Show the plot
plt.tight_layout()
plt.show()


# Assuming your dataframe is named 'filtered_df' and has a 'Procedure Description' column

# Apply the predict function to all sentences in the 'Procedure Description' column
def apply_prediction(row):
    sentence = row['Procedure Description']  # Modify if needed
    return predict(model, sentence)

# Run prediction for each row in the dataframe and store the results in new columns
filtered_df[['predicted_procedures', 'predicted_objectives', 'predicted_standards', 'predicted_domains']] = (
    filtered_df.apply(lambda row: pd.Series(apply_prediction(row)), axis=1)
)

# Display the dataframe with the predictions
print(filtered_df[['Procedure Description', 'predicted_procedures', 'predicted_objectives', 'predicted_standards', 'predicted_domains']].head())


import boto3

# Variables for the role and S3 bucket
role_arn = "arn:aws:iam::123456789012:role/YourRoleName"  # Replace with your Role ARN
bucket_name = "your-bucket-name"

# Create an STS client
sts_client = boto3.client('sts')

# Assume the role
assumed_role = sts_client.assume_role(
    RoleArn=role_arn,
    RoleSessionName="S3AccessSession"
)

# Extract the temporary credentials from the response
credentials = assumed_role['Credentials']
access_key = credentials['AccessKeyId']
secret_key = credentials['SecretAccessKey']
session_token = credentials['SessionToken']

# Create a new S3 client using the temporary credentials
s3_client = boto3.client(
    's3',
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    aws_session_token=session_token
)

# List the contents of the S3 bucket
response = s3_client.list_objects_v2(Bucket=bucket_name)

# Print the contents of the bucket
if 'Contents' in response:
    for obj in response['Contents']:
        print(obj['Key'])
else:
    print("Bucket is empty or you don't have permission to list its contents.")

To train the HierarchicalRoberta model using the provided code and data, here’s a step-by-step guide to the training algorithm:

Step 1: Load Necessary Libraries

import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import RobertaTokenizer, RobertaConfig
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

Step 2: Load Data

	•	Load the Control Structure:

control_data = pd.read_pickle('control.pkl')


	•	Load Annotated Sentences:

annotated_data = pd.read_pickle('annotated_data.pkl')



Step 3: Verify and Encode Labels

	•	Extract and Encode the Labels:

# Fill NaN or empty strings with a placeholder like 'Unknown'
control_data['domain'].fillna('Unknown', inplace=True)
control_data['domain'].replace('', 'Unknown', inplace=True)

annotated_data['domain'].fillna('Unknown', inplace=True)
annotated_data['domain'].replace('', 'Unknown', inplace=True)


# Extract unique values for encoding
valid_domains = control_data['domain'].unique()
valid_standards = control_data['standard'].unique()
valid_objectives = control_data['objective'].unique()
valid_procedures = control_data['procedure'].unique()

# Initialize Label Encoders
domain_encoder = LabelEncoder().fit(valid_domains)
standard_encoder = LabelEncoder().fit(valid_standards)
objective_encoder = LabelEncoder().fit(valid_objectives)
procedure_encoder = LabelEncoder().fit(valid_procedures)

# Encode the annotated data
annotated_data['domain'] = domain_encoder.transform(annotated_data['domain'])
annotated_data['standard'] = standard_encoder.transform(annotated_data['standard'])
annotated_data['objective'] = objective_encoder.transform(annotated_data['objective'])
annotated_data['procedure'] = procedure_encoder.transform(annotated_data['procedure'])



Step 4: Prepare the Dataset

	•	Create a Custom Dataset Class:

class CustomDataset(Dataset):
    def __init__(self, sentences, labels, tokenizer, max_len):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, index):
        sentence = self.sentences[index]
        label = self.labels[index]

        encoding = self.tokenizer.encode_plus(
            sentence,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


	•	Initialize the Tokenizer:

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')


	•	Prepare Data for Training and Validation:

# Combine labels into a single array
labels = annotated_data[['domain', 'standard', 'objective', 'procedure']].values

# Split into training and validation sets
train_sentences, val_sentences, train_labels, val_labels = train_test_split(
    annotated_data['sentence'].values,
    labels,
    test_size=0.2,
    random_state=42
)

# Create Dataset objects
train_dataset = CustomDataset(
    sentences=train_sentences,
    labels=train_labels,
    tokenizer=tokenizer,
    max_len=128
)

val_dataset = CustomDataset(
    sentences=val_sentences,
    labels=val_labels,
    tokenizer=tokenizer,
    max_len=128
)

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)



Step 5: Initialize the Model

	•	Load Model Configuration:

config = RobertaConfig.from_pretrained('roberta-base')


	•	Initialize the Hierarchical Model:

model = HierarchicalRoberta(config)



Step 6: Set Up the Training Loop

	•	Define Optimizer and Loss:

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)


	•	Training Loop:

def train_epoch(model, data_loader, optimizer, device):
    model = model.train()
    total_loss = 0

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        loss, _, _, _, _ = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(data_loader)

def eval_model(model, data_loader, device):
    model = model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            loss, _, _, _, _ = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            total_loss += loss.item()

    return total_loss / len(data_loader)



Step 7: Train the Model

	•	Train the Model Over Several Epochs:

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

epochs = 3
for epoch in range(epochs):
    print(f'Epoch {epoch + 1}/{epochs}')
    train_loss = train_epoch(model, train_loader, optimizer, device)
    val_loss = eval_model(model, val_loader, device)

    print(f'Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')



Step 8: Save the Trained Model

	•	Save the Model and Tokenizer:

model.save_pretrained('./hierarchical_roberta')
tokenizer.save_pretrained('./hierarchical_roberta')



from transformers import RobertaModel, RobertaConfig, RobertaPreTrainedModel
import torch.nn as nn
import torch

class HierarchicalRoberta(RobertaPreTrainedModel):
    def __init__(self, config):
        super(HierarchicalRoberta, self).__init__(config)
        self.roberta = RobertaModel(config)

        # Define classifiers for each hierarchical level
        self.domain_classifier = nn.Linear(config.hidden_size, len(domain_encoder.classes_))
        self.standard_classifier = nn.Linear(config.hidden_size, len(standard_encoder.classes_))
        self.objective_classifier = nn.Linear(config.hidden_size, len(objective_encoder.classes_))
        self.procedure_classifier = nn.Linear(config.hidden_size, len(procedure_encoder.classes_))

        self.init_weights()

    def forward(self, input_ids, attention_mask, labels=None):
        # Get the RoBERTa output
        outputs = self.roberta(input_ids, attention_mask=attention_mask)
        sequence_output = outputs[0][:, 0, :]  # Use the [CLS] token's output

        # Predict each level sequentially
        domain_logits = self.domain_classifier(sequence_output)
        standard_logits = self.standard_classifier(sequence_output)
        objective_logits = self.objective_classifier(sequence_output)
        procedure_logits = self.procedure_classifier(sequence_output)

        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            domain_loss = loss_fct(domain_logits, labels[:, 0])
            standard_loss = loss_fct(standard_logits, labels[:, 1])
            objective_loss = loss_fct(objective_logits, labels[:, 2])
            procedure_loss = loss_fct(procedure_logits, labels[:, 3])

            total_loss = domain_loss + standard_loss + objective_loss + procedure_loss
            return total_loss, domain_logits, standard_logits, objective_logits, procedure_logits

        return domain_logits, standard_logits, objective_logits, procedure_logits




def predict(model, tokenizer, sentence):
    model.eval()
    
    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    with torch.no_grad():
        domain_logits, standard_logits, objective_logits, procedure_logits = model(input_ids, attention_mask)

    # Convert logits to predicted labels
    domain_pred = torch.argmax(domain_logits, dim=1).item()
    standard_pred = torch.argmax(standard_logits, dim=1).item()
    objective_pred = torch.argmax(objective_logits, dim=1).item()
    procedure_pred = torch.argmax(procedure_logits, dim=1).item()

    # Decode labels
    domain_label = domain_encoder.inverse_transform([domain_pred])[0]
    standard_label = standard_encoder.inverse_transform([standard_pred])[0]
    objective_label = objective_encoder.inverse_transform([objective_pred])[0]
    procedure_label = procedure_encoder.inverse_transform([procedure_pred])[0]

    return {
        'domain': domain_label,
        'standard': standard_label,
        'objective': objective_label,
        'procedure': procedure_label
    }

# Example usage
sentence = "Ensure that all data at rest is encrypted using AES-256."
prediction = predict(model, tokenizer, sentence)
print(prediction)


Sentence: "Encrypt all backup files using AES-256 before transferring to cloud storage."

Expected Domain: Encryption
Expected Standard: Data at Rest Encryption
Expected Objective: Secure Backup Encryption
Expected Procedure: AES-256 Encryption Procedure
Explanation: The sentence explicitly mentions "AES-256," which is a strong indicator that the correct procedure should involve the AES-256 encryption method. The context of encrypting backups for secure transfer is aligned with common encryption procedures, making it likely the model will predict this correctly.

Sentence: "Ensure multi-factor authentication is enforced for all user logins."

Expected Domain: Authentication
Expected Standard: User Access Control
Expected Objective: Multi-Factor Authentication
Expected Procedure: Token-Based MFA Implementation
Explanation: The sentence mentions "multi-factor authentication," a specific technique used to enhance security. The model, having been trained on similar sentences, should predict the procedure related to implementing MFA, such as "Token-Based MFA Implementation."

Sentence: "All passwords should be hashed using the SHA-256 algorithm before storage."

Expected Domain: Encryption
Expected Standard: Password Protection
Expected Objective: Password Hashing
Expected Procedure: SHA-256 Hashing Procedure
Explanation: "SHA-256 algorithm" directly references a specific procedure for hashing passwords, making it highly likely that the model will predict the correct procedure, "SHA-256 Hashing Procedure."

Sentence: "Implement firewall rules to block unauthorized access to the database."

Expected Domain: Network Security
Expected Standard: Access Control
Expected Objective: Database Security
Expected Procedure: Firewall Configuration Procedure
Explanation: The mention of "firewall rules" suggests a specific network security procedure. The model should predict a procedure related to configuring firewall rules, such as "Firewall Configuration Procedure."

Sentence: "Perform regular audits to ensure compliance with data protection regulations."

Expected Domain: Compliance
Expected Standard: Data Protection Compliance
Expected Objective: Regular Audits
Expected Procedure: Audit Procedure Implementation
Explanation: The sentence refers to "regular audits," which are a procedural aspect of compliance. The model should identify the related procedure as "Audit Procedure Implementation."

Why These Predictions are Likely Correct
Clear Keywords: Each sentence contains specific keywords or phrases that are closely tied to a known procedure, such as "AES-256," "multi-factor authentication," or "firewall rules." These keywords are likely to have been encountered during training, making the model's prediction more accurate.

Contextual Information: The context provided by the sentence helps the model infer the correct procedure. For instance, the context of "user logins" in conjunction with "multi-factor authentication" strongly suggests an MFA-related procedure.

Hierarchical Dependencies: The model predicts procedures after first predicting domains, standards, and objectives. This sequential prediction allows it to refine its final prediction based on the hierarchical context provided by the earlier predictions.

Summary of the Steps:

	1.	Load Data: Load the control structure from control.pkl and the annotated sentences from annotated_data.pkl.
	2.	Verify & Encode Labels: Use LabelEncoder to encode labels for domain, standard, objective, and procedure based on the control structure.
	3.	Prepare Dataset: Create a custom PyTorch Dataset to handle tokenization and data formatting.
	4.	Initialize Model: Set up the RoBERTa model for hierarchical classification.
	5.	Train Model: Run the training loop with a defined optimizer and loss function.
	6.	Save Model: After training, save the model and tokenizer for later use.

This step-by-step process allows you to train a hierarchical classification model using RoBERTa, tailored for the task of mapping sentences to their respective control structures.





o evaluate the performance of your hierarchical classification model using metrics like accuracy, precision, recall, F1-score, and a confusion matrix, you can use the following code. This code assumes that you have split your data into training and validation sets and have a trained model ready for evaluation.

1. Import Required Libraries
python
Copy code
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np
import torch

# Assuming you have the following objects:
# model: the trained model
# tokenizer: the tokenizer used for preprocessing
# val_dataset: the validation dataset
# domain_encoder, standard_encoder, objective_encoder, procedure_encoder: label encoders for each hierarchical level

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
2. Define Evaluation Function
python
Copy code
def evaluate(model, val_dataset):
    model.eval()
    
    true_labels = []
    pred_labels = []

    for batch in val_dataset:
        inputs = tokenizer(batch['sentence'], return_tensors='pt', truncation=True, padding=True).to(device)
        labels = batch['labels'].to(device)

        with torch.no_grad():
            domain_logits, standard_logits, objective_logits, procedure_logits = model(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask']
            )

        # Convert logits to predictions
        domain_preds = torch.argmax(domain_logits, dim=1).cpu().numpy()
        standard_preds = torch.argmax(standard_logits, dim=1).cpu().numpy()
        objective_preds = torch.argmax(objective_logits, dim=1).cpu().numpy()
        procedure_preds = torch.argmax(procedure_logits, dim=1).cpu().numpy()

        # Combine predictions and true labels
        pred_labels.append(np.stack([domain_preds, standard_preds, objective_preds, procedure_preds], axis=1))
        true_labels.append(labels.cpu().numpy())

    pred_labels = np.concatenate(pred_labels)
    true_labels = np.concatenate(true_labels)

    return true_labels, pred_labels
3. Calculate Metrics
python
Copy code
def calculate_metrics(true_labels, pred_labels, encoder, level_name):
    accuracy = accuracy_score(true_labels, pred_labels)
    precision = precision_score(true_labels, pred_labels, average='weighted')
    recall = recall_score(true_labels, pred_labels, average='weighted')
    f1 = f1_score(true_labels, pred_labels, average='weighted')
    conf_matrix = confusion_matrix(true_labels, pred_labels)

    print(f"{level_name} Level Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"Confusion Matrix:\n{conf_matrix}\n")
4. Run Evaluation on All Levels
python
Copy code
# Evaluate the model on the validation set
true_labels, pred_labels = evaluate(model, val_dataset)

# For each level, calculate and print the metrics
calculate_metrics(true_labels[:, 0], pred_labels[:, 0], domain_encoder, "Domain")
calculate_metrics(true_labels[:, 1], pred_labels[:, 1], standard_encoder, "Standard")
calculate_metrics(true_labels[:, 2], pred_labels[:, 2], objective_encoder, "Objective")
calculate_metrics(true_labels[:, 3], pred_labels[:, 3], procedure_encoder, "Procedure")
5. Example Usage
After running the code, you will get accuracy, precision, recall, F1-score, and a confusion matrix for each hierarchical level (Domain, Standard, Objective, and Procedure). Here's an example of what the output might look like:

plaintext
Copy code
Domain Level Metrics:
Accuracy: 0.8250
Precision: 0.8205
Recall: 0.8250
F1-Score: 0.8227
Confusion Matrix:
[[50  2  1]
 [ 5 38  7]
 [ 0  3 64]]

Standard Level Metrics:
Accuracy: 0.7800
Precision: 0.7734
Recall: 0.7800
F1-Score: 0.7767
Confusion Matrix:
[[45  8  2]
 [ 6 35  9]
 [ 1  5 58]]

...

Procedure Level Metrics:
Accuracy: 0.7650
Precision: 0.7590
Recall: 0.7650
F1-Score: 0.7619
Confusion Matrix:
[[42  7  3]
 [ 8 32 10]
 [ 3  4 61]]
Explanation:
Accuracy: The proportion of true results (both true positives and true negatives) among the total number of cases examined.
Precision: The proportion of true positive predictions among all positive predictions (how many selected items are relevant).
Recall: The proportion of true positive predictions among all actual positives (how many relevant items are selected).
F1-Score: The harmonic mean of precision and recall, providing a balance between the two.
Confusion Matrix: A table used to describe the performance of a classification model, showing the correct and incorrect predictions.
By analyzing these metrics, you can assess the model's performance and identify areas where it might need improvement.




# Count unique mappings for procedures, objectives, standards, and domains
grouped_df = merged_df.groupby('sentence').agg({
    'procedure': pd.Series.nunique,  # Count unique procedures
    'objective': pd.Series.nunique,  # Count unique objectives
    'standard': pd.Series.nunique,   # Count unique standards
    'domain': pd.Series.nunique      # Count unique domains
}).reset_index()

# Rename columns for clarity
grouped_df.rename(columns={
    'procedure': 'procedure_count',
    'objective': 'objective_count',
    'standard': 'standard_count',
    'domain': 'domain_count'
}, inplace=True)


# Extract distinct procedures
distinct_procedures = filtered_df['procedures'].explode().drop_duplicates().reset_index(drop=True)

# Extract distinct objectives
distinct_objectives = filtered_df['objectives'].explode().drop_duplicates().reset_index(drop=True)

# Extract distinct standards
distinct_standards = filtered_df['standards'].explode().drop_duplicates().reset_index(drop=True)

# Extract distinct domains
distinct_domains = filtered_df['domains'].explode().drop_duplicates().reset_index(drop=True)

# Display the distinct values
print("Distinct Procedures:\n", distinct_procedures)
print("\nDistinct Objectives:\n", distinct_objectives)
print("\nDistinct Standards:\n", distinct_standards)
print("\nDistinct Domains:\n", distinct_domains)






To create a hierarchical multi-class model using RoBERTa that can handle sentences mapped to multiple procedures, objectives, standards, and domains, we’ll first prepare the data and then define and fine-tune a model. This process involves the following steps:

1. Data Preparation

Data Overview:

	•	Control Data: Contains domains, standards, objectives, and procedures.
	•	Sentence Data: Contains sentences that need to be mapped to the control structure.

Data Cleaning and Merging:

The key is to merge these datasets correctly, ensure there are no NaN values, and then structure the data so that it’s ready for model training.

import pandas as pd
import numpy as np

# Assume control_df and sentences_df are your dataframes loaded from ctrl.pkl and sentences.pkl respectively

# Merge the dataframes on a common key if available
merged_df = pd.merge(sentences_df, control_df, how='inner', on='common_key')  # replace 'common_key' with your actual key

# Drop NaNs across necessary columns
filtered_df = merged_df.dropna(subset=['domain', 'standard', 'objective', 'procedure'])

# Handle cases where sentences map to multiple procedures, objectives, etc.
filtered_df['procedures'] = filtered_df['procedures'].apply(lambda x: x.split(',') if isinstance(x, str) else x)
filtered_df['objectives'] = filtered_df['objectives'].apply(lambda x: x.split(',') if isinstance(x, str) else x)
filtered_df['standards'] = filtered_df['standards'].apply(lambda x: x.split(',') if isinstance(x, str) else x)
filtered_df['domains'] = filtered_df['domains'].apply(lambda x: x.split(',') if isinstance(x, str) else x)

# Remove NaN entries from lists
filtered_df = filtered_df.applymap(lambda x: [i for i in x if i] if isinstance(x, list) else x)

# Filter out empty lists to make sure there's no missing data
filtered_df = filtered_df[filtered_df['procedures'].str.len() > 0]

2. Model Definition

Hierarchical Multi-Label Classification with RoBERTa

We’ll define a custom model using the transformers library by extending RobertaPreTrainedModel. The model will predict at each level of the hierarchy (procedure, objective, standard, domain).

import torch
import torch.nn as nn
from transformers import RobertaModel, RobertaPreTrainedModel, RobertaTokenizer

class HierarchicalRobertaMultiLabel(RobertaPreTrainedModel):
    def __init__(self, config):
        super(HierarchicalRobertaMultiLabel, self).__init__(config)
        self.roberta = RobertaModel(config)
        
        # Linear layers for each hierarchical level
        self.procedure_classifier = nn.Linear(config.hidden_size, len(procedure_encoder.classes_))
        self.objective_classifier = nn.Linear(config.hidden_size, len(objective_encoder.classes_))
        self.standard_classifier = nn.Linear(config.hidden_size, len(standard_encoder.classes_))
        self.domain_classifier = nn.Linear(config.hidden_size, len(domain_encoder.classes_))

        self.init_weights()

    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.roberta(input_ids, attention_mask=attention_mask)
        sequence_output = outputs[0][:, 0, :]  # Using CLS token's representation

        procedure_logits = self.procedure_classifier(sequence_output)
        objective_logits = self.objective_classifier(sequence_output)
        standard_logits = self.standard_classifier(sequence_output)
        domain_logits = self.domain_classifier(sequence_output)

        return procedure_logits, objective_logits, standard_logits, domain_logits

3. Fine-Tuning

Data Encoding

Use MultiLabelBinarizer for the multi-label data:

from sklearn.preprocessing import MultiLabelBinarizer

procedure_encoder = MultiLabelBinarizer().fit(filtered_df['procedures'])
objective_encoder = MultiLabelBinarizer().fit(filtered_df['objectives'])
standard_encoder = MultiLabelBinarizer().fit(filtered_df['standards'])
domain_encoder = MultiLabelBinarizer().fit(filtered_df['domains'])

# Encode the labels
y_procedures = procedure_encoder.transform(filtered_df['procedures'])
y_objectives = objective_encoder.transform(filtered_df['objectives'])
y_standards = standard_encoder.transform(filtered_df['standards'])
y_domains = domain_encoder.transform(filtered_df['domains'])

Training Loop

from transformers import RobertaTokenizer, AdamW
from torch.utils.data import DataLoader, TensorDataset
import torch

# Load tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenize the sentences
encodings = tokenizer(list(filtered_df['sentence']), truncation=True, padding=True, max_length=128)

input_ids = torch.tensor(encodings['input_ids'])
attention_mask = torch.tensor(encodings['attention_mask'])
labels_procedures = torch.tensor(y_procedures, dtype=torch.float32)
labels_objectives = torch.tensor(y_objectives, dtype=torch.float32)
labels_standards = torch.tensor(y_standards, dtype=torch.float32)
labels_domains = torch.tensor(y_domains, dtype=torch.float32)

# Create a dataset and dataloader
dataset = TensorDataset(input_ids, attention_mask, labels_procedures, labels_objectives, labels_standards, labels_domains)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# Model and optimizer
model = HierarchicalRobertaMultiLabel.from_pretrained('roberta-base')
optimizer = AdamW(model.parameters(), lr=2e-5)

# Training loop
for epoch in range(3):
    model.train()
    total_loss = 0

    for batch in dataloader:
        b_input_ids, b_attention_mask, b_labels_procedures, b_labels_objectives, b_labels_standards, b_labels_domains = batch

        model.zero_grad()

        #



        # Forward pass
        procedure_logits, objective_logits, standard_logits, domain_logits = model(
            input_ids=b_input_ids,
            attention_mask=b_attention_mask
        )

        # Loss calculation for multi-label classification
        loss_fn = nn.BCEWithLogitsLoss()

        procedure_loss = loss_fn(procedure_logits, b_labels_procedures)
        objective_loss = loss_fn(objective_logits, b_labels_objectives)
        standard_loss = loss_fn(standard_logits, b_labels_standards)
        domain_loss = loss_fn(domain_logits, b_labels_domains)

        # Total loss
        loss = procedure_loss + objective_loss + standard_loss + domain_loss
        total_loss += loss.item()

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    avg_train_loss = total_loss / len(dataloader)
    print(f"Epoch {epoch + 1}, Loss: {avg_train_loss}")







def predict(model, sentence):
    model.eval()
    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids']
    attention_mask = inputs['attention_mask']

    with torch.no_grad():
        procedure_logits, objective_logits, standard_logits, domain_logits = model(input_ids, attention_mask)

    # Convert logits to probabilities
    procedure_probs = torch.sigmoid(procedure_logits).squeeze().tolist()
    objective_probs = torch.sigmoid(objective_logits).squeeze().tolist()
    standard_probs = torch.sigmoid(standard_logits).squeeze().tolist()
    domain_probs = torch.sigmoid(domain_logits).squeeze().tolist()

    # Get the most probable labels (threshold can be adjusted)
    procedure_preds = [procedure_encoder.classes_[i] for i, p in enumerate(procedure_probs) if p > 0.5]
    objective_preds = [objective_encoder.classes_[i] for i, p in enumerate(objective_probs) if p > 0.5]
    standard_preds = [standard_encoder.classes_[i] for i, p in enumerate(standard_probs) if p > 0.5]
    domain_preds = [domain_encoder.classes_[i] for i, p in enumerate(domain_probs) if p > 0.5]

    return {
        "procedures": procedure_preds,
        "objectives": objective_preds,
        "standards": standard_preds,
        "domains": domain_preds
    }

# Example usage
sentence = "Implementing AES-256 encryption for securing data at rest."
predictions = predict(model, sentence)
print(predictions)







from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix

def evaluate_model(model, dataloader):
    model.eval()
    all_preds_procedures = []
    all_preds_objectives = []
    all_preds_standards = []
    all_preds_domains = []
    
    all_labels_procedures = []
    all_labels_objectives = []
    all_labels_standards = []
    all_labels_domains = []
    
    with torch.no_grad():
        for batch in dataloader:
            b_input_ids, b_attention_mask, b_labels_procedures, b_labels_objectives, b_labels_standards, b_labels_domains = batch

            procedure_logits, objective_logits, standard_logits, domain_logits = model(
                input_ids=b_input_ids,
                attention_mask=b_attention_mask
            )

            # Get predicted labels
            procedure_preds = (torch.sigmoid(procedure_logits) > 0.5).float()
            objective_preds = (torch.sigmoid(objective_logits) > 0.5).float()
            standard_preds = (torch.sigmoid(standard_logits) > 0.5).float()
            domain_preds = (torch.sigmoid(domain_logits) > 0.5).float()

            all_preds_procedures.append(procedure_preds.cpu().numpy())
            all_preds_objectives.append(objective_preds.cpu().numpy())
            all_preds_standards.append(standard_preds.cpu().numpy())
            all_preds_domains.append(domain_preds.cpu().numpy())
            
            all_labels_procedures.append(b_labels_procedures.cpu().numpy())
            all_labels_objectives.append(b_labels_objectives.cpu().numpy())
            all_labels_standards.append(b_labels_standards.cpu().numpy())
            all_labels_domains.append(b_labels_domains.cpu().numpy())

    # Flatten the arrays
    y_true_procedures = np.concatenate(all_labels_procedures, axis=0)
    y_pred_procedures = np.concatenate(all_preds_procedures, axis=0)

    y_true_objectives = np.concatenate(all_labels_objectives, axis=0)
    y_pred_objectives = np.concatenate(all_preds_objectives, axis=0)

    y_true_standards = np.concatenate(all_labels_standards, axis=0)
    y_pred_standards = np.concatenate(all_preds_standards, axis=0)

    y_true_domains = np.concatenate(all_labels_domains, axis=0)
    y_pred_domains = np.concatenate(all_preds_domains, axis=0)

    # Metrics calculation
    metrics = {}
    for label, y_true, y_pred in [("procedures", y_true_procedures, y_pred_procedures),
                                  ("objectives", y_true_objectives, y_pred_objectives),
                                  ("standards", y_true_standards, y_pred_standards),
                                  ("domains", y_true_domains, y_pred_domains)]:
        
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='weighted')
        recall = recall_score(y_true, y_pred, average='weighted')
        f1 = f1_score(y_true, y_pred, average='weighted')
        conf_matrix = multilabel_confusion_matrix(y_true, y_pred)

        metrics[label] = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "confusion_matrix": conf_matrix
        }

    return metrics

# Evaluate the model on the validation set
metrics = evaluate_model(model, dataloader)
print(metrics)



Here are some example sentences that might be used with the predict function to generate hierarchical predictions:

1. Data Encryption

sentence = "Implementing AES-256 encryption for securing data at rest."
predictions = predict(model, sentence)
print(predictions)

Possible Output:

{
    "procedures": ["Encryption Procedure"],
    "objectives": ["Data Protection Objective"],
    "standards": ["Encryption Standard"],
    "domains": ["Security Domain"]
}

2. Access Control

sentence = "Configuring multi-factor authentication for all remote access."
predictions = predict(model, sentence)
print(predictions)

Possible Output:

{
    "procedures": ["Authentication Procedure", "Access Control Procedure"],
    "objectives": ["Access Management Objective"],
    "standards": ["Authentication Standard", "Access Control Standard"],
    "domains": ["Security Domain"]
}

3. Data Backup

sentence = "Performing regular backups and storing them in a secure offsite location."
predictions = predict(model, sentence)
print(predictions)

Possible Output:

{
    "procedures": ["Backup Procedure", "Data Recovery Procedure"],
    "objectives": ["Data Availability Objective"],
    "standards": ["Backup Standard", "Disaster Recovery Standard"],
    "domains": ["Operational Continuity Domain"]
}

4. Incident Response

sentence = "Establishing an incident response plan to address security breaches."
predictions = predict(model, sentence)
print(predictions)

Possible Output:

{
    "procedures": ["Incident Response Procedure"],
    "objectives": ["Incident Management Objective"],
    "standards": ["Incident Response Standard"],
    "domains": ["Security Domain"]
}

5. Network Security

sentence = "Implementing a firewall to protect the internal network from unauthorized access."
predictions = predict(model, sentence)
print(predictions)

Possible Output:

{
    "procedures": ["Firewall Configuration Procedure"],
    "objectives": ["Network Security Objective"],
    "standards": ["Network Security Standard"],
    "domains": ["Security Domain"]
}

6. User Training

sentence = "Conducting regular security awareness training for all employees."
predictions = predict(model, sentence)
print(predictions)

Possible Output:

{
    "procedures": ["Training and Awareness Procedure"],
    "objectives": ["Human Resource Security Objective"],
    "standards": ["Security Training Standard"],
    "domains": ["Human Resources Domain"]
}

These examples demonstrate how the model might map sentences to different procedures, objectives, standards, and domains based on the context of the sentence. The actual output will depend on the training data and the model’s performance after fine-tuning.












Step 1: Data Preprocessing
We need to convert the labels into a binary format for multi-label classification and tokenize the sentences using the Roberta tokenizer.

Code:
python
Copy code
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MultiLabelBinarizer
from transformers import RobertaTokenizer, RobertaModel, RobertaPreTrainedModel, AdamW
import torch.nn as nn

# Load tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Assuming `filtered_df` is the DataFrame with the structure you provided.
# The DataFrame has columns: 'sentence', 'procedure', 'objective', 'standard', 'domain'

# Convert the text column to tokens and apply padding/truncation
def tokenize_sentence(sentence):
    return tokenizer(sentence, padding='max_length', truncation=True, max_length=256, return_tensors="pt")['input_ids'].squeeze()

# Tokenizing the sentences
filtered_df['tokenized'] = filtered_df['extended_text'].apply(lambda x: tokenize_sentence(x))

# Use MultiLabelBinarizer to encode multi-label outputs
mlb_procedure = MultiLabelBinarizer()
mlb_objective = MultiLabelBinarizer()
mlb_standard = MultiLabelBinarizer()
mlb_domain = MultiLabelBinarizer()

procedure_labels = mlb_procedure.fit_transform(filtered_df['procedure'])
objective_labels = mlb_objective.fit_transform(filtered_df['objective'])
standard_labels = mlb_standard.fit_transform(filtered_df['standard'])
domain_labels = mlb_domain.fit_transform(filtered_df['domain'])

# Dataset preparation
class HierarchicalDataset(Dataset):
    def __init__(self, sentences, procedures, objectives, standards, domains):
        self.sentences = sentences
        self.procedures = procedures
        self.objectives = objectives
        self.standards = standards
        self.domains = domains

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        return {
            'input_ids': self.sentences[idx],
            'procedures': torch.tensor(self.procedures[idx], dtype=torch.float),
            'objectives': torch.tensor(self.objectives[idx], dtype=torch.float),
            'standards': torch.tensor(self.standards[idx], dtype=torch.float),
            'domains': torch.tensor(self.domains[idx], dtype=torch.float),
        }

# Prepare dataset and DataLoader
dataset = HierarchicalDataset(
    sentences=filtered_df['tokenized'].tolist(),
    procedures=procedure_labels,
    objectives=objective_labels,
    standards=standard_labels,
    domains=domain_labels
)

train_loader = DataLoader(dataset, batch_size=16, shuffle=True)
Step 2: Model Definition
We'll define a model based on Roberta for multi-label classification. Each output (procedures, objectives, standards, domains) will be predicted separately.

Code:
python
Copy code
class HierarchicalRoberta(RobertaPreTrainedModel):
    def __init__(self, config, num_procedures, num_objectives, num_standards, num_domains):
        super(HierarchicalRoberta, self).__init__(config)
        self.roberta = RobertaModel(config)

        # Define classifiers for each hierarchical level
        self.procedure_classifier = nn.Linear(config.hidden_size, num_procedures)
        self.objective_classifier = nn.Linear(config.hidden_size, num_objectives)
        self.standard_classifier = nn.Linear(config.hidden_size, num_standards)
        self.domain_classifier = nn.Linear(config.hidden_size, num_domains)

        self.init_weights()

    def forward(self, input_ids, attention_mask=None, labels=None):
        # Get RoBERTa output
        outputs = self.roberta(input_ids, attention_mask=attention_mask)
        sequence_output = outputs[0][:, 0, :]  # Use [CLS] token output

        # Predict each level sequentially
        procedure_logits = self.procedure_classifier(sequence_output)
        objective_logits = self.objective_classifier(sequence_output)
        standard_logits = self.standard_classifier(sequence_output)
        domain_logits = self.domain_classifier(sequence_output)

        return procedure_logits, objective_logits, standard_logits, domain_logits
Step 3: Training Loop
Code:
python
Copy code
# Model initialization
num_procedures = len(mlb_procedure.classes_)
num_objectives = len(mlb_objective.classes_)
num_standards = len(mlb_standard.classes_)
num_domains = len(mlb_domain.classes_)

model = HierarchicalRoberta.from_pretrained('roberta-base', 
                                            num_procedures=num_procedures, 
                                            num_objectives=num_objectives, 
                                            num_standards=num_standards, 
                                            num_domains=num_domains)

# Training setup
optimizer = AdamW(model.parameters(), lr=5e-5)
criterion = nn.BCEWithLogitsLoss()

# Training loop
def train_model(model, train_loader, optimizer, criterion, num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for batch in train_loader:
            optimizer.zero_grad()

            input_ids = batch['input_ids'].to(torch.int64)
            procedures = batch['procedures']
            objectives = batch['objectives']
            standards = batch['standards']
            domains = batch['domains']

            procedure_logits, objective_logits, standard_logits, domain_logits = model(input_ids)

            loss_procedures = criterion(procedure_logits, procedures)
            loss_objectives = criterion(objective_logits, objectives)
            loss_standards = criterion(standard_logits, standards)
            loss_domains = criterion(domain_logits, domains)

            loss = loss_procedures + loss_objectives + loss_standards + loss_domains
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')

# Train the model
train_model(model, train_loader, optimizer, criterion, num_epochs=3)
Step 4: Inference & Output Format
To predict the labels for a new sentence and return the output in your desired format:

Code:
python
Copy code
def predict(model, sentence):
    model.eval()
    tokens = tokenize_sentence(sentence).unsqueeze(0)  # Add batch dimension
    with torch.no_grad():
        procedure_logits, objective_logits, standard_logits, domain_logits = model(tokens)

    # Apply sigmoid to get probabilities
    procedure_probs = torch.sigmoid(procedure_logits).squeeze().cpu().numpy()
    objective_probs = torch.sigmoid(objective_logits).squeeze().cpu().numpy()
    standard_probs = torch.sigmoid(standard_logits).squeeze().cpu().numpy()
    domain_probs = torch.sigmoid(domain_logits).squeeze().cpu().numpy()

    # Convert probabilities to binary predictions (threshold can be adjusted)
    procedure_preds = procedure_probs > 0.5
    objective_preds = objective_probs > 0.5
    standard_preds = standard_probs > 0.5
    domain_preds = domain_probs > 0.5

    # Map binary predictions to class names
    procedures = mlb_procedure.inverse_transform([procedure_preds])[0]
    objectives = mlb_objective.inverse_transform([objective_preds])[0]
    standards = mlb_standard.inverse_transform([standard_preds])[0]
    domains = mlb_domain.inverse_transform([domain_preds])[0]

    return {
        "procedures": list(procedures),
        "objectives": list(objectives),
        "standards": list(standards),
        "domains": list(domains)
}

# Example usage
sentence = "Implementing AES-256 encryption for securing data at rest."
predictions = predict(model, sentence)
print(predictions)
Step 5: Model Evaluation
To evaluate the model's performance, you can calculate accuracy, precision, recall, and F1-score.

Code:
python
Copy code
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluate_model(model, data_loader):
    model.eval()
    all_procedure_preds, all_objective_preds, all_standard_preds, all_domain_preds = [], [], [], []
    all_procedure_labels, all_objective_labels, all_standard_labels, all_domain_labels = [], [], [], []
    
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(torch.int64)
            procedures = batch['procedures']
            objectives = batch['objectives']
            standards = batch['standards']
            domains = batch['domains']
            
            procedure_logits, objective_logits, standard_logits, domain_logits = model(input_ids)
            
            procedure_preds = (torch.sigmoid(procedure_logits) > 0.5).cpu().numpy()
            objective_preds = (torch.sigmoid(objective_logits) > 0.5).cpu().numpy()
            standard_preds = (torch.sigmoid(standard_logits) > 0.5).cpu().numpy()
            domain_preds = (torch.sigmoid(domain_logits) > 0.5).cpu().numpy()
            
            all_procedure_preds.extend(procedure_preds)
            all_objective_preds.extend(objective_preds)
            all_standard_preds.extend(standard_preds)
            all_domain_preds.extend(domain_preds)
            
            all_procedure_labels.extend(procedures.cpu().numpy())
            all_objective_labels.extend(objectives.cpu().numpy())
            all_standard_labels.extend(standards.cpu().numpy())
            all_domain_labels.extend(domains.cpu().numpy())
    
    print(f"Procedure Accuracy: {accuracy_score(all_procedure_labels, all_procedure_preds)}")
    print(f"Objective Accuracy: {accuracy_score(all_objective_labels, all_objective_preds)}")
    print(f"Standard Accuracy: {accuracy_score(all_standard_labels, all_standard_preds)}")
    print(f"Domain Accuracy: {accuracy_score(all_domain_labels, all_domain_preds)}")

# Example usage
evaluate_model(model, train_loader)
This complete code provides a robust pipeline from preprocessing to model evaluation. You can adjust the number of epochs and learning rate depending on the size and complexity of your data.
